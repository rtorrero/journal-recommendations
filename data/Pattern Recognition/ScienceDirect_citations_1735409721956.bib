@article{JIAN2021108100,
title = {Semantic manifold modularization-based ranking for image recommendation},
journal = {Pattern Recognition},
volume = {120},
pages = {108100},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108100},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002879},
author = {Meng Jian and Jingjing Guo and Chenlin Zhang and Ting Jia and Lifang Wu and Xun Yang and Lina Huo},
keywords = {Manifold propagation, Modularization, Image recommendation, User interest},
abstract = {As the Internet confronts the multimedia explosion, it becomes urgent to investigate personalized recommendation for alleviating information overload and improving users’ experience. Most personalized recommendation approaches pay their attention to collaborative filtering over users’ interactions, which suffers greatly from the highly sparse interactions. In image recommendation, visual correlations among images that users consumed provide a piece of intrinsic evidence to reveal users’ interests. It inspires us to investigate image recommendation over the dense visual graph of images instead of the sparse user interaction graph. In this paper, we propose a semantic manifold modularization-based ranking (MMR) for image recommendation. MMR leverages the dense visual manifold to propagate users’ historical records and infer user-image correlations for image recommendation. Especially, it constrains interest propagation within semantic visual compact groups by manifold modularization to make a tradeoff between users’ personality and graph smoothness in propagation. Experimental results demonstrate that user-consumed visual correlations play actively to capture users’ interests, and the proposed MMR can infer user-image correlations via visual manifold propagation for image recommendation.}
}
@article{SONG2021108084,
title = {Deep robust multilevel semantic hashing for multi-label cross-modal retrieval},
journal = {Pattern Recognition},
volume = {120},
pages = {108084},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108084},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002715},
author = {Ge Song and Xiaoyang Tan and Jun Zhao and Ming Yang},
keywords = {Hashing, Multi-label, Cross-modal retrieval, Deep learning},
abstract = {Hashing based cross-modal retrieval has recently made significant progress. But straightforward embedding data from different modalities involving rich semantics into a joint Hamming space will inevitably produce false codes due to the intrinsic modality discrepancy and noises. We present a novel deep Robust Multilevel Semantic Hashing (RMSH) for more accurate multi-label cross-modal retrieval. It seeks to preserve fine-grained similarity among data with rich semantics,i.e., multi-label, while explicitly require distances between dissimilar points to be larger than a specific value for strong robustness. For this, we give an effective bound of this value based on the information coding-theoretic analysis, and the above goals are embodied into a margin-adaptive triplet loss. Furthermore, we introduce pseudo-codes via fusing multiple hash codes to explore seldom-seen semantics, alleviating the sparsity problem of similarity information. Experiments on three benchmarks show the validity of the derived bounds, and our method achieves state-of-the-art performance.}
}
@article{LUO2021108104,
title = {MVDRNet: Multi-view diabetic retinopathy detection by combining DCNNs and attention mechanisms},
journal = {Pattern Recognition},
volume = {120},
pages = {108104},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108104},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002910},
author = {Xiaoling Luo and Zuhui Pu and Yong Xu and Wai Keung Wong and Jingyong Su and Xiaoyan Dou and Baikang Ye and Jiying Hu and Lisha Mou},
keywords = {Diabetic retinopathy (DR), Deep convolutional neural networks (DCNNs), Multi-view, Attention mechanisms, Classification},
abstract = {Diabetic retinopathy (DR) detection has attracted much attention recently, and the deep learning algorithms have gained traction in this area. At present, DR screening by deep learning algorithms is often based on single-view fundus images, which usually leads to an unsatisfactory accuracy of DR grading due to the incomplete lesion features. In this paper, we proposed a novel diabetic retinopathy detection convolutional network for automatic DR detection by integrating multi-view fundus images. Compared to existing single-view DCNN-based DR detection methods, the proposed method has the following advantages. First, our method fully utilizes the lesion features from the retina with a field-of-view around 120∘−150∘. Second, by introducing the attention mechanisms, more attention will be paid on the influential view and the performance can be improved. Besides, we also assign large weights to important channels in the network for effective feature extraction. Experiments are conducted on our collected multi-view DR dataset contained 15,468 images, in which each eye sample provides four-view images. The experimental results indicate that using multi-view images is suitable for automatic DR detection and our proposed method is superior to other benchmarking methods.}
}
@article{ZHANG2021108189,
title = {A multi-task fully deep convolutional neural network for contactless fingerprint minutiae extraction},
journal = {Pattern Recognition},
volume = {120},
pages = {108189},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108189},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003526},
author = {Zhao Zhang and Shuxin Liu and Manhua Liu},
keywords = {Contactless fingerprint, Minutiae extraction, Deep convolutional neural network, Multi-task learning},
abstract = {With the outbreak and wide spread of novel coronavirus (COVID-19), contactless fingerprint recognition has attracted more attention for personal recognition because it can provide significantly higher user convenience and hygiene than the traditional contact-based fingerprint recognition. However, it is still challenging to achieve a highly accurate recognition due to the low ridge-valley contrast and pose variances of contactless fingerprints. Minutiae points are a kind of ridge flow discontinuities, and robust and accurate extraction is an important step for most automatic fingerprint recognition algorithms. Most of existing methods are based on two stages which locate the minutiae points first and then compute their directions. The two-stage method cannot make full use of location and direction information. In this paper, we propose a multi-task fully deep convolutional neural network for jointly learning the minutiae location detection and its corresponding direction computation which operates directly on the whole gray scale contactless fingerprints. The proposed method consists of offline training and online testing stages. In the training stage, a fully deep convolutional neural network is built for the tasks of minutiae detection and its direction regression, with an attention mechanism to make the direction regression branch concentrate on the minutiae points. A new loss function is proposed to jointly learn the tasks of minutiae detection and its direction regression from the whole fingerprints. In the testing stage, the trained network is applied on the whole contactless fingerprint to generate the minutiae location and direction maps. The proposed multi-task leaning method performs better than the individual single task and it operates directly on the raw gray-scale contactless fingerprints without preprocessing. The results on three contactless fingerprint datasets show the proposed algorithm performs better than other minutiae extraction algorithms and the commercial software.}
}
@article{YANG2021108113,
title = {GGAC: Multi-relational image gated GCN with attention convolutional binary neural tree for identifying disease with chest X-rays},
journal = {Pattern Recognition},
volume = {120},
pages = {108113},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108113},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003009},
author = {Bing Yang and Yan Kang and Lan Zhang and Hao Li},
keywords = {Multi-relational graph, Gated graph convolutional network, Identifying disease, Attention transformer},
abstract = {Using medical images for disease identification is an important application in the medical field. Graph Convolutional Network (GCN) is proposed to model multi-relational image and generate more informative image representations. Recently, the relations between medical images are utilized to identify diseases. This paper proposes a Gated GCN with Attention Convolutional Binary Neural Tree (GGAC) for Multi-Relational Image Identifying Disease. GGAC extracts the discriminative features of the image, strengthen the ability to model medical images, understands images representation deeply and then well captures the multi-modal relation between images. Firstly, an Attention Convolutional Binary Neural Tree based on the attention mechanism is designed to extract the fine-grained features of the images, and use the attention conversion operation on the edge of the tree structure to enhance the network’s acquisition of key image features. Secondly, a Gated GCN is proposed to improve GCN performance by solving the problem of the weight distribution of different neighbors in the same-order neighborhood. Thirdly, a GCN propagation rule is used to transfer messages in multi-relational Graph and then solves the message passing problem of high-dimensional feature data in GCN. Finally, we verify GGAC on a multi-relational graph constructed on the Chest X-rays14. It can be seen from the experiment that overfitting and underfitting can be solved to a certain extent through the extraction and inference of the features of the multi-relational graph, and then GGAC has better performance than the state-of-the-art methods, and keeps good in model complexity.}
}
@article{XU2021108125,
title = {Semi-supervised multi-Layer convolution kernel learning in credit evaluation},
journal = {Pattern Recognition},
volume = {120},
pages = {108125},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108125},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003125},
author = {Lixiang Xu and Lixin Cui and Thomas Weise and Xinlu Li and Zhize Wu and Feiping Nie and Enhong Chen and Yuanyan Tang},
keywords = {Semi-supervised learning, SVM, Convolution kernel function, Random sampling, Multi-layer kernel},
abstract = {In many practical credit evaluation problems, a lot of manpower as well as financial and material resources are required to label samples. Therefore, in the process of labeling, only a small number of samples with category labels can be obtained to train classification models and a large number of customer samples is abandoned without category labels. To solve this problem, we introduce a semi-supervised support vector machine (SVM) technology and combines it with a multi-layer convolution kernel to construct a semi-supervised multi-layer convolution kernel SVM (SSMCK) for category customer credit assessment data sets. We first use a basic solution of the generalized differential operator to generate a base convolution kernel function in the H1 space, and then use the multi-layer strategy of deep learning to construct the multi-layer convolution kernel in the H2 and H3 space (called the family of multi-layer convolution kernel) by using the kernel functions in the H1 space. We further propose a semi-supervised multi-layer convolution kernel SVM algorithm based on the category center estimation and develop two novel SSMCK methods to improve the classification ability: the SSMCK based on multi-kernel learning (SSMCK-MKL) and the SSMCK based on alternative optimization (SSMCK-AO). Finally, experimental verification and analysis is carried out on three customer credit evaluation data sets. The results show that our methods outperforms or are comparable to some the state-of-the-art credit evaluation models.}
}
@article{SEVILLASALCEDO2021108141,
title = {Sparse semi-supervised heterogeneous interbattery bayesian analysis},
journal = {Pattern Recognition},
volume = {120},
pages = {108141},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108141},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003289},
author = {Carlos Sevilla-Salcedo and Vanessa Gómez-Verdejo and Pablo M. Olmos},
keywords = {Bayesian model, Canonical correlation analysis, Principal component analysis, Factor analysis, Feature selection, Semi-supervised, Multi-task},
abstract = {The Bayesian approach to feature extraction, known as factor analysis (FA), has been widely studied in machine learning to obtain a latent representation of the data. An adequate selection of the probabilities and priors of these bayesian models allows the model to better adapt to the data nature (i.e. heterogeneity, sparsity), obtaining a more representative latent space. The objective of this article is to propose a general FA framework capable of modelling any problem. To do so, we start from the Bayesian Inter-Battery Factor Analysis (BIBFA) model, enhancing it with new functionalities to be able to work with heterogeneous data, to include feature selection, and to handle missing values as well as semi-supervised problems. The performance of the proposed model, Sparse Semi-supervised Heterogeneous Interbattery Bayesian Analysis (SSHIBA), has been tested on different scenarios to evaluate each one of its novelties, showing not only a great versatility and an interpretability gain, but also outperforming most of the state-of-the-art algorithms.}
}
@article{FAN2021108169,
title = {Manifold learning with structured subspace for multi-label feature selection},
journal = {Pattern Recognition},
volume = {120},
pages = {108169},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108169},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003563},
author = {Yuling Fan and Jinghua Liu and Peizhong Liu and Yongzhao Du and Weiyao Lan and Shunxiang Wu},
keywords = {Multi-label learning, Feature selection, Manifold learning, Structured subspace, Instance correlations, Label correlations},
abstract = {Nowadays, multi-label learning is ubiquitous in practical applications, in which multi-label data is always confronted with the curse of high-dimensional features. Feature selection has been shown to effectively improve learning performance by selecting discriminative features. Conventional multi-label feature selection only focuses on associating input features with corresponding labels while neglecting the potential structural information, i.e., instance correlations and label correlations. To tackle this problem, we propose manifold learning with structured subspace for multi-label feature selection. Specifically, we first uncover a latent subspace for a more compact and accurate data representation, and take advantage of the subspace to explore the correlations among instances. Then, we explore label correlations in manifold learning to guarantee the global and local structural consistency of labels. Besides, l2,1-norm is introduced into loss function and sparse regularization to facilitate feature selection process. A detail optimization algorithm is presented to solve the objective function of the proposed method. Extensive experiments on real-world data show the superiority of the proposed method under various metrics.}
}
@article{ZENG2021108117,
title = {Correlation-based structural dropout for convolutional neural networks},
journal = {Pattern Recognition},
volume = {120},
pages = {108117},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108117},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003046},
author = {Yuyuan Zeng and Tao Dai and Bin Chen and Shu-Tao Xia and Jian Lu},
keywords = {Over-fitting, Regularization, Dropout, Convolutional neural networks},
abstract = {Convolutional neural networks (CNNs) easily suffer from the over-fitting problem since they are often over-parameterized in the case of small training datasets. The conventional dropout that drops feature units randomly works well for fully connected networks, but fails to regularize CNNs well due to high spatial correlation of the intermediate features, which allows the dropped information to flow through the network, thus leading to the problem of under-dropping. To better regularize CNNs, some structural dropout methods such as SpatialDropout and DropBlock have been proposed by dropping feature units in continuous regions randomly. However, these methods may suffer from the over-dropping problem by discarding the critical discriminative features, thus limiting the performance of CNNs. To address these issues, we propose a novel structural dropout method, Correlation based Dropout (CorrDrop), to regularize CNNs by dropping feature units based on feature correlation. Unlike the previous dropout methods, our CorrDrop can focus on the discriminative information and drops features in a spatial-wise or channel-wise manner. Extensive experiments on different datasets, network architectures, and various tasks (e.g., image classification and object localization) demonstrate the superiority of our method over other methods.}
}
@article{GAO2021108145,
title = {Generalized pyramid co-attention with learnable aggregation net for video question answering},
journal = {Pattern Recognition},
volume = {120},
pages = {108145},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108145},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003320},
author = {Lianli Gao and Tangming Chen and Xiangpeng Li and Pengpeng Zeng and Lei Zhao and Yuan-Fang Li},
keywords = {Video question answering, Diversity learning, Learnable aggregation, Cascaded pyramid transformer co-attention},
abstract = {Video based visual question answering (V-VQA) remains challenging at the intersection of vision and language. In this paper, we propose a novel architecture, namely Generalized Pyramid Co-attention with Learnable Aggregation Net (GPC) to address two central problems: 1) how to deploy co-attention to V-VQA task considering the complex and diverse content of videos; and 2) how to aggregate the frame-level features (or word-level features) without destroying the feature distributions and temporal information. To solve the first problem, we propose a Generalized Pyramid Co-attention structure with a novel diversity learning module to explicitly encourage attention accuracy and diversity. And we first instantiate it into a Multi-path Pyramid Co-attention (MPC) to capture diverse feature. Then we find each attention branch of original co-attention mechanism does not interact with the others, which results in coarse attention maps. So we extend the MPC structure to a Cascaded Pyramid Transformer Co-attention (CPTC) module in which we replace co-attention with transformer co-attention. To solve the second problem, we propose a new learnable aggregation method with a set of evidence gates. It automatically aggregates adaptively-weighted frame-level features (or word-level features) to extract rich video (or question) context semantic information. With evidence gates, it then further chooses the most related signals representing the evidence information to predict the answer. Extensive validations on the two V-VQA datasets, TGIF-QA and TVQA show that both our proposed MPC and CPTC achieve the state-of-the-art performance and CPTC performs better under various settings and metrics. Code and model have been released at:https://github.com/lixiangpengcs/LAD-Net-for-VideoQA.}
}
@article{ROSSI2021108136,
title = {Human trajectory prediction and generation using LSTM models and GANs},
journal = {Pattern Recognition},
volume = {120},
pages = {108136},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108136},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100323X},
author = {Luca Rossi and Marina Paolanti and Roberto Pierdicca and Emanuele Frontoni},
keywords = {Trajectory generation, Trajectory prediction, LSTM, GANs},
abstract = {Human trajectory prediction is an important topic in several application domains, ranging from self-driving cars to environment design and planning, from socially-aware robots to intelligent tracking systems. This complex subject comes with different challenges, such as human-space interaction, human-human interaction, multimodality, and generalizability. Currently, these challenges, especially generalizability, have not been completely explored by state-of-the-art works. This work attempts to fill this gap by proposing and defining new methods and metrics to help understand trajectories. In particular, new deep learning models based on Long Short-Term Memory and Generative Adversarial Network architectures are used in both unimodal and multimodal contexts. These approaches are evaluated with new error metrics, which normalize some biases in standard metrics. Tests have been assessed using newly collected datasets characterized by a higher diversity and lower linearity than those used in state-of-the-art works. The results prove that the proposed models and datasets are comparable to and yield better generalizability than state-of-the-art works. Moreover, we also prove that our datasets better represent multimodal scenarios (allowing for multiple possible behaviors) and that human trajectories are moderately influenced by their spatial region and slightly influenced by their date and time.}
}
@article{HUANG2021108149,
title = {Multi-label feature selection via manifold regularization and dependence maximization},
journal = {Pattern Recognition},
volume = {120},
pages = {108149},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108149},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003368},
author = {Rui Huang and Zhejun Wu},
keywords = {Multi-label learning, Feature selection, Sparse regression, Manifold regularization, Dependence maximization},
abstract = {Feature selection is able to select more discriminative features for classification and plays an important role in multi-label learning to alleviate the effect of the curse of dimensionality. Recently, the multi-label feature selection methods based on the sparse regression model have received increasing attentions. However, most of these methods directly project original data space to label space in the regression model, which is inappropriate because the linear assumption between data space and label space doesn't hold in most cases. In the paper, we propose a feature selection method named multi-label feature selection via manifold regularization and dependence maximization (MRDM). In the regression model of MRDM, the original data space is projected to a low-dimensional manifold space, which not only has the same topological structure with the original data, but also has a strong dependence with the class labels. Then, an objective function involving l2,1-norm regularization is formulated, and an alternating optimization-based iterative algorithm is designed to obtain the sparse coefficients for multi-label feature selection. Extensive experiments on various multi-label data sets demonstrate the superiority of the proposed method compared with some state-of-the-art multi-label feature selection methods.}
}
@article{GU2021108112,
title = {Generalized error path algorithm},
journal = {Pattern Recognition},
volume = {120},
pages = {108112},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108112},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002995},
author = {Bin Gu and Charles X. Ling},
keywords = {Cross validation, Error path, Solution path, Model selection},
abstract = {Model selection with cross validation (CV) is very popular in machine learning. However, CV with grid and other common search strategies cannot guarantee to find the model with minimum CV error, which is often the ultimate goal of model selection. Recently, various solution path algorithms have been proposed for several important learning algorithms including support vector classification, Lasso, and so on. However, they still do not guarantee to find the model with minimum CV error. In this paper, we first show that the solution paths produced by various algorithms have the property of piecewise linearity. Then, we prove that a large class of error (or loss) functions are piecewise constant, linear, or quadratic w.r.t. the regularization parameter, based on the solution path. Finally, we propose a new generalized error path algorithm (GEP), and prove that it will find the model with minimum CV error in a finite number of steps for the entire range of the regularization parameter. The experimental results on a variety of datasets not only confirm our theoretical findings, but also show that the best model with our GEP has better generalization error on the test data, compared to the grid search, manual search, and random search.}
}
@article{LIU2021108121,
title = {Modulating scalable Gaussian processes for expressive statistical learning},
journal = {Pattern Recognition},
volume = {120},
pages = {108121},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108121},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003083},
author = {Haitao Liu and Yew-Soon Ong and Xiaomo Jiang and Xiaofang Wang},
keywords = {Gaussian process, Modulation, Scalability, Heteroscedastic noise, Multi-modality, Non-stationarity},
abstract = {For a learning task, Gaussian process (GP) is interested in learning the statistical relationship between inputs and outputs, since it offers not only the prediction mean but also the associated variability. The vanilla GP however is hard to learn complicated distribution with the property of, e.g., heteroscedastic noise, multi-modality and non-stationarity, from massive data due to the Gaussian marginal and the cubic complexity. To this end, this article studies new scalable GP paradigms including the non-stationary heteroscedastic GP, the mixture of GPs and the latent GP, which introduce additional latent variables to modulate the outputs or inputs in order to learn richer, non-Gaussian statistical representation. Particularly, we resort to different variational inference strategies to arrive at analytical or tighter evidence lower bounds (ELBOs) of the marginal likelihood for efficient and effective model training. Extensive numerical experiments against state-of-the-art GP and neural network (NN) counterparts on various tasks verify the superiority of these scalable modulated GPs, especially the scalable latent GP, for learning diverse data distributions.}
}
@article{YANG2021108192,
title = {GAMI-Net: An explainable neural network based on generalized additive models with structured interactions},
journal = {Pattern Recognition},
volume = {120},
pages = {108192},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108192},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003484},
author = {Zebin Yang and Aijun Zhang and Agus Sudjianto},
keywords = {Explainable neural network, Generalized additive model, Pairwise interaction, Interpretability constraints},
abstract = {The lack of interpretability is an inevitable problem when using neural network models in real applications. In this paper, an explainable neural network based on generalized additive models with structured interactions (GAMI-Net) is proposed to pursue a good balance between prediction accuracy and model interpretability. GAMI-Net is a disentangled feedforward network with multiple additive subnetworks; each subnetwork consists of multiple hidden layers and is designed for capturing one main effect or one pairwise interaction. Three interpretability aspects are further considered, including a) sparsity, to select the most significant effects for parsimonious representations; b) heredity, a pairwise interaction could only be included when at least one of its parent main effects exists; and c) marginal clarity, to make main effects and pairwise interactions mutually distinguishable. An adaptive training algorithm is developed, where main effects are first trained and then pairwise interactions are fitted to the residuals. Numerical experiments on both synthetic functions and real-world datasets show that the proposed model enjoys superior interpretability and it maintains competitive prediction accuracy in comparison to the explainable boosting machine and other classic machine learning models.}
}
@article{ZHANG2021108137,
title = {Multi-label feature selection considering label supplementation},
journal = {Pattern Recognition},
volume = {120},
pages = {108137},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108137},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003241},
author = {Ping Zhang and Guixia Liu and Wanfu Gao and Jiazhi Song},
keywords = {Multi-label learning, Multi-label feature selection, Information theory, Label relationships},
abstract = {Multi-label feature selection is an efficient technique to alleviate the high dimensionality for multi-label learning. Existing multi-label feature selection methods based on information theory either deal with labels individually or treat all label relationships as redundancy. However, two important and being ignored issues are the different effects of label relationships and the dynamic changes of label relationships in measuring different candidate features. To address these issues, we first distinguish three types of label relationships: label independence, label redundancy and label supplementation. Second, we consider the changes of label relationships based on different features. By analyzing the differences and the changes of label relationships, two new methods named LSMFS and MLSMFS are proposed, which extracts all supplementary information and the maximum supplementary information of features for each label from other labels, respectively. Finally, experiments on fifteen benchmark multi-label data sets demonstrate the effectiveness of the proposed methods against nine other methods.}
}
@article{FAN2021108129,
title = {Dynamic and reliable subtask tracker with general schatten p-norm regularization},
journal = {Pattern Recognition},
volume = {120},
pages = {108129},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108129},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003162},
author = {Baojie Fan and Yang Cong and Jiandong Tian and Yandong Tang},
keywords = {Reliable multi-subtask tracking, Weighted schatten -norm, Hyper-graph regularization, Decision-evaluation strategy},
abstract = {Some multi-task trackers adopt an inaccurate shrink strategy to treat different rank components equally. Thus, their flexibility is vulnerable to some tracking challenges. To resolve this problem, we propose a spatial-aware reliable multi-subtask tracker via weighted Schatten p-norm regularization (SLRT-W), which dynamically chooses the suitable and reliable subset of the whole subtasks for tracking. Its major merits not only assign the flexible weights to different subtask rank components depending on their tracking contribution, but also preserve consistent spatial layout structure and correspondence of layered multi-subtask. Specifically, multiple layered subtasks correspond to different target subregions, they are cooperative and complement. A weighted Schatten p-norm is introduced to adaptively shrink different multi-subtask rank components, and emphasize important components as reliable ones. Then, a structured hyper-graph regularized term simultaneously exploits the intrinsic geometry correspondence among multiple layers of subtasks, and spatial layout structure inside each layer. We devise an alternatively generalized iterated shrinkage method to optimize the multi-subtask Schatten p-norm minimization. Finally, a robust decision-evaluation strategy is developed to choose the reliable multi-subtask tracking combination. Encouraging results on some challenging benchmarks demonstrate the proposed tracker performs favorably in robustness and accuracy, against some state-of-the-art trackers.}
}
@article{XUAN2021108103,
title = {Reducing magnetic resonance image spacing by learning without ground-truth},
journal = {Pattern Recognition},
volume = {120},
pages = {108103},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108103},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002909},
author = {Kai Xuan and Liping Si and Lichi Zhang and Zhong Xue and Yining Jiao and Weiwu Yao and Dinggang Shen and Dijia Wu and Qian Wang},
keywords = {Generative adversarial network, Magnetic resonance imaging, Super-resolution, Variational auto-encoder},
abstract = {High-quality magnetic resonance (MR) image, i.e., with near isotropic voxel spacing, is desirable in various scenarios of medical image analysis. However, many MR images are acquired using good in-plane resolution but large spacing between slices in clinical practice. In this work, we propose a novel deep-learning-based super-resolution algorithm to generate high-resolution (HR) MR images of small slice spacing from low-resolution (LR) inputs of large slice spacing. Notice that real HR images are needed in most existing deep-learning-based methods to supervise the training, but in clinical scenarios, usually they will not be acquired. Therefore, our unique goal herein is to design and train the super-resolution network without real HR ground-truth. Specifically, two-staged training is used in our method. In the first stage, HR images of reduced slice spacing are synthesized from real LR images using variational auto-encoder (VAE). Although these synthesized HR images of reduced slice spacing are as realistic as possible, they may still suffer from unexpected morphing induced by VAE, implying that the synthesized HR images cannot be paired with the real LR images in terms of anatomical structure details. In the second stage, we degrade the synthesized HR images to generate corresponding LR-HR image pairs and train a super-resolution network based on these synthesized pairs. The underlying mechanism is that such a super-resolution network is less vulnerable to anatomical variability. Experiments on knee MR images successfully demonstrate the effectiveness of our proposed solution to reduce the slice spacing for better rendering.}
}
@article{ILIC2021108144,
title = {Explainable boosted linear regression for time series forecasting},
journal = {Pattern Recognition},
volume = {120},
pages = {108144},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108144},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003319},
author = {Igor Ilic and Berk Görgülü and Mucahit Cevik and Mustafa Gökçe Baydoğan},
keywords = {Time series regression, Probabilistic forecasting, Decision trees, Linear regression, ARIMA},
abstract = {Time series forecasting involves collecting and analyzing past observations to develop a model to extrapolate such observations into the future. Forecasting of future events is important in many fields to support decision making as it contributes to reducing the future uncertainty. We propose explainable boosted linear regression (EBLR) algorithm for time series forecasting, which is an iterative method that starts with a base model, and explains the model’s errors through regression trees. At each iteration, the path leading to highest error is added as a new variable to the base model. In this regard, our approach can be considered as an improvement over general time series models since it enables incorporating nonlinear features by residual explanation. More importantly, use of the single rule that contributes to the error most enables access to interpretable results. The proposed approach extends to probabilistic forecasting through generating prediction intervals based on the empirical error distribution. We conduct a detailed numerical study with EBLR and compare against various other approaches. We observe that EBLR substantially improves the base model performance through extracted features, and provide a comparable performance to other well established approaches. The interpretability of the model predictions and high predictive accuracy of EBLR makes it a promising method for time series forecasting.}
}
@article{JIAO2021108107,
title = {DDAT: Dual domain adaptive translation for low-resolution face verification in the wild},
journal = {Pattern Recognition},
volume = {120},
pages = {108107},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108107},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002946},
author = {Qianfen Jiao and Rui Li and Wenming Cao and Jian Zhong and Si Wu and Hau-San Wong},
keywords = {Low-resolution face verification, Domain adaptation, Image translation, GAN},
abstract = {Low-resolution (LR) face verification has received much attention because of its wide applicability in real scenarios, especially in long-distance surveillance. However, the poor quality and scarcity of training data make the accuracy far from satisfactory. In this paper, we propose an end-to-end LR face translation and verification framework to improve the generation quality of face images and face verification accuracy simultaneously. We design a dual domain adaptive structure to generate high-quality images. On one hand, the structure can reduce the domain gap between training data and test data. On the other hand, the structure preserves identity consistency and low-level attributes. Meanwhile, in order to make the whole model more robust, we treat the generated images of the target domain as an extension of the training data. We conduct extensive comparative experiments on multiple benchmark data sets. Experimental results verify that our method achieves improved results in high-quality face generation and LR face verification. In particular, our model DDAT reduces FID to 18.63 and 39.55 on the source and the target domain from 254.7 and 206.19 of the up-sampling results, respectively. Our method outperforms competing approaches by more than 10 percentage points in terms of face verification accuracy on multiple surveillance benchmarks.}
}
@article{WANG2021108075,
title = {Knowledge-aware deep framework for collaborative skin lesion segmentation and melanoma recognition},
journal = {Pattern Recognition},
volume = {120},
pages = {108075},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108075},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002624},
author = {Xiaohong Wang and Xudong Jiang and Henghui Ding and Yuqian Zhao and Jun Liu},
keywords = {Melanoma diagnosis, Knowledge-aware deep framework, Lesion-based pooling and shape extraction, Diagnosis guided feature fusion, Recursive mutual learning},
abstract = {Deep learning techniques have shown their superior performance in dermatologist clinical inspection. Nevertheless, melanoma diagnosis is still a challenging task due to the difficulty of incorporating the useful dermatologist clinical knowledge into the learning process. In this paper, we propose a novel knowledge-aware deep framework that incorporates some clinical knowledge into collaborative learning of two important melanoma diagnosis tasks, i.e., skin lesion segmentation and melanoma recognition. Specifically, to exploit the knowledge of morphological expressions of the lesion region and also the periphery region for melanoma identification, a lesion-based pooling and shape extraction (LPSE) scheme is designed, which transfers the structure information obtained from skin lesion segmentation into melanoma recognition. Meanwhile, to pass the skin lesion diagnosis knowledge from melanoma recognition to skin lesion segmentation, an effective diagnosis guided feature fusion (DGFF) strategy is designed. Moreover, we propose a recursive mutual learning mechanism that further promotes the inter-task cooperation, and thus iteratively improves the joint learning capability of the model for both skin lesion segmentation and melanoma recognition. Experimental results on two publicly available skin lesion datasets show the effectiveness of the proposed method for melanoma analysis.}
}
@article{DONG2021108188,
title = {Knowledge memorization and generation for action recognition in still images},
journal = {Pattern Recognition},
volume = {120},
pages = {108188},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108188},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003757},
author = {Jian Dong and Wankou Yang and Yazhou Yao and Fatih Porikli},
keywords = {Action recognition, Deep learning, Knowledge-transfer},
abstract = {Human action recognition in visual data is one of the most fundamental challenges in computer vision. Existing approaches for this primary goal have been based on video data, often incorporating both color and dynamic flow information. Nevertheless, the majority of the visual data constitute still images, and for this reason, being able to recognize actions in still image is an ultimate objective of visual understanding with an extended list of applications. In this paper, we present a novel method that transfers the knowledge learned from action videos onto images to allow recognition of the principal action depicted in still image. Our intuition is that a generative model for knowledge transfer can be learned by taking advantage of the available action videos in the training stage to bridge images to videos. Based on this, we propose two complementary knowledge-transfer models utilizing fully connected networks to deliver the knowledge extracted from color and motion flow sequences to still images. We introduce a weighted reconstruction and classification loss to steer the generation procedure of the networks. In addition, we describe and analyze the influence of different data augmentation techniques, initialization strategies, and weighting coefficients for improving the performance. We observe that: both the transferred knowledge from color sequences and motion flow sequences can improve the performance of still image based human action recognition; the latter one which provides complementary dynamic information improves the performance a lot. We evaluate our models on two publicly available video based human action recognition datasets: UCF101 and HMDB51. To further validate the generalization ability of the proposed solution, we test the learned models from UCF101 dataset on two still image based human action recognition benchmarks: Willow7 Actions and the Sports. Our results demonstrate that the proposed method outperforms the baseline approaches with more than 2% accuracy, 3% accuracy, 3% accuracy and 5% mAP on UCF101, HMDB51, Sports and Willow 7 Actions datasets, respectively.}
}
@article{BANDARA2021108148,
title = {Improving the accuracy of global forecasting models using time series data augmentation},
journal = {Pattern Recognition},
volume = {120},
pages = {108148},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108148},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003356},
author = {Kasun Bandara and Hansika Hewamalage and Yuan-Hao Liu and Yanfei Kang and Christoph Bergmeir},
keywords = {Time series forecasting, Global forecasting models, Data augmentation, Transfer learning, RNN},
abstract = {Forecasting models that are trained across sets of many time series, known as Global Forecasting Models (GFM), have shown recently promising results in forecasting competitions and real-world applications, outperforming many state-of-the-art univariate forecasting techniques. In most cases, GFMs are implemented using deep neural networks, and in particular Recurrent Neural Networks (RNN), which require a sufficient amount of time series to estimate their numerous model parameters. However, many time series databases have only a limited number of time series. In this study, we propose a novel, data augmentation based forecasting framework that is capable of improving the baseline accuracy of the GFM models in less data-abundant settings. We use three time series augmentation techniques: GRATIS, moving block bootstrap (MBB), and dynamic time warping barycentric averaging (DBA) to synthetically generate a collection of time series. The knowledge acquired from these augmented time series is then transferred to the original dataset using two different approaches: the pooled approach and the transfer learning approach. When building GFMs, in the pooled approach, we train a model on the augmented time series alongside the original time series dataset, whereas in the transfer learning approach, we adapt a pre-trained model to the new dataset. In our evaluation on competition and real-world time series datasets, our proposed variants can significantly improve the baseline accuracy of GFM models and outperform state-of-the-art univariate forecasting methods.}
}
@article{2021108268,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {120},
pages = {108268},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(21)00448-9},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004489}
}
@article{GAO2021108124,
title = {Meta-learning based relation and representation learning networks for single-image deraining},
journal = {Pattern Recognition},
volume = {120},
pages = {108124},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108124},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003113},
author = {Xinjian Gao and Yang Wang and Jun Cheng and Mingliang Xu and Meng Wang},
keywords = {Meta-learning, Relation network, Single-image deraining, Representation learning},
abstract = {Single-image deraining is a kind of computer vision task that aims to restore the image that be degraded by rain streaks, which motivates existing methods to either directly translate the rainy image to its clean one, or indirectly learn the rain residual based on the prior information. However, both methodologies harm the generalization ability due to the limited diversity of the training samples, comparing with the endless varieties of the real-world rainy images. Such fact inspires us to take the merit of meta-learning and propose a meta-learning based representation learning network to learn the transferable embeddings of the rainy/clean images, while their discrepancies are characterized by the relation vector, which is generated by the subsequent meta-learning based relation learning network. These networks are leveraged into the meta-learning based deraining network (MLDN) to enhance the generalization ability by removing the latent relation vector from the transferable embedding of the rainy image and generate high-quality deraining result. Superior performance is achieved by MLDN, which has averaged 4% better than the state-of-the-arts.}
}
@article{LIU2021108140,
title = {Certainty driven consistency loss on multi-teacher networks for semi-supervised learning},
journal = {Pattern Recognition},
volume = {120},
pages = {108140},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108140},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003277},
author = {Lu Liu and Robby T. Tan},
keywords = {Semi-supervised learning, Certainty-driven consistency loss, Uncertainty estimation, Decoupled student-teacher, Reliable targets, Noisy labels},
abstract = {One of the successful approaches in semi-supervised learning is based on the consistency regularization. Typically, a student model is trained to be consistent with teacher prediction for the inputs under different perturbations. To be successful, the prediction targets given by teacher should have good quality, otherwise the student can be misled by teacher. Unfortunately, existing methods do not assess the quality of the teacher targets. In this paper, we propose a novel Certainty-driven Consistency Loss (CCL) that exploits the predictive uncertainty in the consistency loss to let the student dynamically learn from reliable targets. Specifically, we propose two approaches, i.e. Filtering CCL and Temperature CCL to either filter out uncertain predictions or pay less attention on them in the consistency regularization. We further introduce a novel decoupled framework to encourage model difference. Experimental results on SVHN, CIFAR-10, and CIFAR-100 demonstrate the advantages of our method over a few existing methods.}
}
@article{MU2021108168,
title = {Progressive global perception and local polishing network for lung infection segmentation of COVID-19 CT images},
journal = {Pattern Recognition},
volume = {120},
pages = {108168},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108168},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003551},
author = {Nan Mu and Hongyu Wang and Yu Zhang and Jingfeng Jiang and Jinshan Tang},
keywords = {Coronavirus disease 2019 (COVID-19), Global perception, Local polishing, Feature recursive aggregation, Multiple supervision},
abstract = {In this paper, a progressive global perception and local polishing (PCPLP) network is proposed to automatically segment the COVID-19-caused pneumonia infections in computed tomography (CT) images. The proposed PCPLP follows an encoder-decoder architecture. Particularly, the encoder is implemented as a computationally efficient fully convolutional network (FCN). In this study, a multi-scale multi-level feature recursive aggregation (mmFRA) network is used to integrate multi-scale features (viz. global guidance features and local refinement features) with multi-level features (viz. high-level semantic features, middle-level comprehensive features, and low-level detailed features). Because of this innovative aggregation of features, an edge-preserving segmentation map can be produced in a boundary-aware multiple supervision (BMS) way. Furthermore, both global perception and local perception are devised. On the one hand, a global perception module (GPM) providing a holistic estimation of potential lung infection regions is employed to capture more complementary coarse-structure information from different pyramid levels by enlarging the receptive fields without substantially increasing the computational burden. On the other hand, a local polishing module (LPM), which provides a fine prediction of the segmentation regions, is applied to explicitly heighten the fine-detail information and reduce the dilution effect of boundary knowledge. Comprehensive experimental evaluations demonstrate the effectiveness of the proposed PCPLP in boosting the learning ability to identify the lung infected regions with clear contours accurately. Our model is superior remarkably to the state-of-the-art segmentation models both quantitatively and qualitatively on a real CT dataset of COVID-19.}
}
@article{LI2021108116,
title = {Hierarchical Object Relationship Constrained Monocular Depth Estimation.},
journal = {Pattern Recognition},
volume = {120},
pages = {108116},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108116},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003034},
author = {Shuai Li and Jiaying Shi and Wenfeng Song and Aimin Hao and Hong Qin},
keywords = {Monocular Depth Estimation, Semantic Constraints, Hierarchical Object Relationship, Global and Local Context},
abstract = {Monocular depth estimation has been gaining growing momentum in recent years. Despite significant advances of this task, due to the inherent difficulty of reliably capturing contextual cues from RGB images, it remains challenging to accurately predict depth in scenes with complicated and cluttered spatial arrangement of objects. Instead of naively utilizing the primary features in the single RGB image, in this paper we propose a hierarchical object relationship constrained network for monocular depth estimation, which could enable accurate and smooth depth prediction from monocular RGB image. The key idea of our method is to exploit object-centric hierarchical relationship as contextual constraints to compensate for the regularity of spatial depth changing. In particular, we design a semantics-guided CNN network to encode the original image into a global context feature map and encode the objects’ relationship into a local relationship feature map simultaneously, so that we can leverage such effective and consolidated coding scheme over scenario samples to guide the depth prediction in a more accurate way. Benefiting from the local-to-global context constraints, our method can well respect the global depth changing and preserve the local depth details at the same time. In addition, our approach could make full use of the hierarchical semantic relationship across inner-object components and neighboring objects to define depth changing constraints. We conduct extensive experiments and make comprehensive evaluations on widely-used public datasets, and the experiments confirm that our method outperforms most state-of-the-art depth estimation methods in preserving the local details in depth.}
}
@article{YANG2021108164,
title = {Training object detectors from few weakly-labeled and many unlabeled images},
journal = {Pattern Recognition},
volume = {120},
pages = {108164},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108164},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003514},
author = {Zhaohui Yang and Miaojing Shi and Chao Xu and Vittorio Ferrari and Yannis Avrithis},
keywords = {Object detection, Weakly-supervised learning, Semi-supervised learning, Unlabelled set},
abstract = {Weakly-supervised object detection attempts to limit the amount of supervision by dispensing the need for bounding boxes, but still assumes image-level labels on the entire training set. In this work, we study the problem of training an object detector from one or few images with image-level labels and a larger set of completely unlabeled images. This is an extreme case of semi-supervised learning where the labeled data are not enough to bootstrap the learning of a detector. Our solution is to train a weakly-supervised student detector model from image-level pseudo-labels generated on the unlabeled set by a teacher classifier model, bootstrapped by region-level similarities to labeled images. Building upon the recent representative weakly-supervised pipeline PCL [1], our method can use more unlabeled images to achieve performance competitive or superior to many recent weakly-supervised detection solutions. Code will be made available at https://github.com/zhaohui-yang/NSOD.}
}
@article{SINGH2021108111,
title = {MetaMed: Few-shot medical image classification using gradient-based meta-learning},
journal = {Pattern Recognition},
volume = {120},
pages = {108111},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108111},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002983},
author = {Rishav Singh and Vandana Bharti and Vishal Purohit and Abhinav Kumar and Amit Kumar Singh and Sanjay Kumar Singh},
keywords = {Few-shot learning, Meta-learning, Multi-shot learning, Medical image classification, Image augmentation, Histopathological image classification},
abstract = {The occurrence of long-tailed distributions and unavailability of high-quality annotated images is a common phenomenon in medical datasets. The use of conventional Deep Learning techniques to obtain an unbiased model with high generalization accuracy for such datasets is a challenging task. Thus, we formulated a few-shot learning problem and presented a meta-learning-based “MetaMed” approach. The model presented here can adapt to rare disease classes with the availability of few images, and less compute. MetaMed is validated on three publicly accessible medical datasets – Pap smear, BreakHis, and ISIC 2018. We used advanced image augmentation techniques like CutOut, MixUp, and CutMix to overcome the problem of over-fitting. Our approach has shown promising results on all the three datasets with an accuracy of more than 70%. Inclusion of advanced augmentation techniques regularizes the model and increases the generalization capability by  2–5%. Comparative analysis of MetaMed against transfer learning demonstrated that MetaMed classifies images with a higher confidence score and on average outperforms transfer learning for 3, 5, and 10-shot tasks for both 2-way and 3-way classification.}
}
@article{ZHAO2021107998,
title = {A nested U-shape network with multi-scale upsample attention for robust retinal vascular segmentation},
journal = {Pattern Recognition},
volume = {120},
pages = {107998},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.107998},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321001850},
author = {Ruohan Zhao and Qin Li and Jianrong Wu and Jane You},
keywords = {Vascular segmentation, Retinal imaging, Dense U-Net, Multi-scale attention, Deep learning},
abstract = {This paper presents a new nested U-shape attention network (NUA-Net) with improved robustness of lesions for effective vascular segmentation in retinal imaging. Unlike most of the current deep learning approaches which rely on vanilla upsample module to recover distinguishable features for segmentation, our attention-based multi-scale network extends the U-shape segmentation network by introducing a novel multi-scale upsample attention (MSUA) module to enhance vessel features in a hierarchical structure. The new approach connects encoder-decoder branches through a nested skip-connection pyramid architecture to extract discriminating retinal features from the rich local details. Experimental evaluations on five publicly available databases DRIVE, STARE, CHASE_DB, IOSTAR and HRF show the NUA-Net achieves 0.8043–0.8511 (Sensitivity), 0.9741–0.99 (Specificity) and 0.9646–0.9794 (Accuracy) respectively. The benchmark by cross-testing and separate-testing presents a state-of-the-art performance and better vessel preservation compared with other approaches.}
}
@article{CAO2021108172,
title = {Effective action recognition with embedded key point shifts},
journal = {Pattern Recognition},
volume = {120},
pages = {108172},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108172},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003599},
author = {Haozhi Cao and Yuecong Xu and Jianfei Yang and Kezhi Mao and Jianxiong Yin and Simon See},
keywords = {Action recognition, Temporal feature, Key point shifts},
abstract = {Temporal feature extraction is an essential technique in video-based action recognition. Key points have been utilized in skeleton-based action recognition methods but they require costly key point annotation. In this paper, we propose a novel temporal feature extraction module, named Key Point Shifts Embedding Module (KPSEM), to adaptively extract channel-wise key point shifts across video frames without key point annotation. Key points are adaptively extracted as feature points with maximum feature values at split regions and key point shifts are the spatial displacements of corresponding key points. The key point shifts are encoded as the overall temporal features via linear embedding layers in a multi-set manner. Our method achieves competitive performance through embedding key point shifts with trivial computational cost, achieving the state-of-the-art performance of 78.81% on Mini-Kinetics and competitive performance on UCF101, Something-Something-v1 and HMDB51 datasets.}
}
@article{ZHOU2021108128,
title = {Point cloud denoising using non-local collaborative projections},
journal = {Pattern Recognition},
volume = {120},
pages = {108128},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108128},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003150},
author = {Yiyao Zhou and Rui Chen and Yiqiang Zhao and Xiding Ai and Guoqing Zhou},
keywords = {Point cloud denoising, Adaptive curvature threshold, Structure-aware descriptor, Projective height vector, Improved weighted nuclear norm minimization},
abstract = {Point cloud is important for object detection and recognition. The main challenge of point cloud denoising is to preserve the geometric structures. Several state-of-the-art point cloud denoising methods focus only on analyzing local geometric information, which is sensitive to noise and outliers. In this paper, we propose a novel point cloud denoising algorithm based on the characteristics of non-local self-similarity. First, we present an adaptive curvature threshold to select the edge points and tune their corresponding normals, which can preserve the sharp details. Then we propose a structure-aware descriptor called projective height vector to capture the local height variations by normal height projection and the most similar non-local projective height vectors are grouped into a height matrix to enhance the structure representation. Moreover, the proposed structure descriptor is invariant with rigid transformation. Finally, an improved weighted nuclear norm minimization is proposed to optimize the height matrix and reconstruct a high-quality point cloud. Rather than treating each singular value independently, each component in our proposed weight definition connects with the most important components to preserve the major structural information. Experiments on synthetic and scanned point cloud datasets demonstrate that our algorithm outperforms state-of-the-art methods in terms of reconstruction accuracy and structure preservation.}
}
@article{ZHAO2021108120,
title = {Real-time and light-weighted unsupervised video object segmentation network},
journal = {Pattern Recognition},
volume = {120},
pages = {108120},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108120},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003071},
author = {Zongji Zhao and Sanyuan Zhao and Jianbing Shen},
keywords = {Unsupervised video object segmentation, Salient object detection},
abstract = {Video object segmentation is one of the most practical computer vision tasks, especially in the unsupervised case, which has no manually labeled segmentation mask at the beginning of a video sequence. In this paper, we propose a new real-time unsupervised video object segmentation network. Based on the encoder-decoder framework, we present a Dynamic ASPP module and a RNN-Conv module. The former adds a dynamic selection mechanism into the Astrous Spatial Pyramid Pooling structure, and then the dilated convolutional kernels adaptively select appropriate features according to the scales by the channel attention mechanism. Compared with directly concatenating the dilated convolutional features, dynamically selecting feature maps reduces the amount of parameters and makes the module more efficient. The RNN-Conv module incorporates the RNN units with external convolutional blocks, aggregating the temporal features of a video sequence with the spatial information extracted by the convolutional network. We stack this module to extract deeper spatiotemporal features than the traditional RNN network. This module helps to avoid the gradient disappearance and explosion during network training. We test our network on the popular video object segmentation datasets. The experiment results demonstrate the effectiveness of our model.11Our code is available at https://github.com/Sanyuan-Zhao/Real-Time-and-Light-Weighted-UVOS}
}
@article{RIBA2021108132,
title = {Learning graph edit distance by graph neural networks},
journal = {Pattern Recognition},
volume = {120},
pages = {108132},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108132},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003198},
author = {Pau Riba and Andreas Fischer and Josep Lladós and Alicia Fornés},
keywords = {Graph neural networks, Graph edit distance, Geometric deep learning, Keyword spotting, Document image analysis},
abstract = {The emergence of geometric deep learning as a novel framework to deal with graph-based representations has faded away traditional approaches in favor of completely new methodologies. In this paper, we propose a new framework able to combine the advances on deep metric learning with traditional approximations of the graph edit distance. Hence, we propose an efficient graph distance based on the novel field of geometric deep learning. Our method employs a message passing neural network to capture the graph structure, and thus, leveraging this information for its use on a distance computation. The performance of the proposed graph distance is validated on two different scenarios. On the one hand, in a graph retrieval of handwritten words i.e. keyword spotting, showing its superior performance when compared with (approximate) graph edit distance benchmarks. On the other hand, demonstrating competitive results for graph similarity learning when compared with the current state-of-the-art on a recent benchmark dataset.}
}
@article{ACKERMAN2021108152,
title = {Weighted clustering: Towards solving the user’s dilemma},
journal = {Pattern Recognition},
volume = {120},
pages = {108152},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108152},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003393},
author = {Margareta Ackerman and Shai Ben-David and Simina Brânzei and David Loker},
keywords = {Clustering, Theory, Properties},
abstract = {This paper makes a major step towards addressing a long-standing challenge in cluster analysis, known as the user’s dilemma, which is the problem of selecting an appropriate clustering algorithm for a specific task. A formal approach for addressing this challenge relies on the identification of succinct, user-friendly properties that capture formal differences amongst clustering techniques. While helpful for gaining insight into the nature of clustering paradigms, there is a theory-practice gap that has so far limited the utility of this approach: Formal properties typically highlight advantages of classical linkage-based algorithms, while practical experience shows that center-based methods are preferable for many applications. We present simple new properties that delineate core differences between common clustering paradigms and overcome this theory-practice gap. The properties we present give a formal understanding of the advantages of center-based approaches for some applications and insight into when different clustering paradigms should be used. These properties address how sensitive algorithms are to changes in element frequencies, which we capture in a generalized setting where every element is associated with a real-valued weight. To complement extensive formal analysis, we discuss how these properties can be applied in practice.}
}
@article{FARAZI2021108106,
title = {Accuracy vs. complexity: A trade-off in visual question answering models},
journal = {Pattern Recognition},
volume = {120},
pages = {108106},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108106},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002934},
author = {Moshiur Farazi and Salman Khan and Nick Barnes},
keywords = {Visual question answering, Visual feature extraction, Language features, Multi-modal fusion, Speed-accuracy trade-off},
abstract = {Visual Question Answering (VQA) has emerged as a Visual Turing Test to validate the reasoning ability of AI agents. The pivot to existing VQA models is the joint embedding that is learned by combining the visual features from an image and the semantic features from a given question. Consequently, a large body of literature has focused on developing complex joint embedding strategies coupled with visual attention mechanisms to effectively capture the interplay between these two modalities. However, modelling the visual and semantic features in a high dimensional (joint embedding) space is computationally expensive, and more complex models often result in trivial improvements in the VQA accuracy. In this work, we systematically study the trade-off between the model complexity and the performance on the VQA task. VQA models have a diverse architecture comprising of pre-processing, feature extraction, multimodal fusion, attention and final classification stages. We specifically focus on the effect of “multi-modal fusion” in VQA models that is typically the most expensive step in a VQA pipeline. Our thorough experimental evaluation leads us to three proposals, one optimized for minimal complexity, one for balanced complexity-accuracy and the last one for state-of-the-art VQA performance.}
}
@article{WANG2021108143,
title = {Temporal consistent portrait video segmentation},
journal = {Pattern Recognition},
volume = {120},
pages = {108143},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108143},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003307},
author = {Yifan Wang and Wenbo Zhang and Lijun Wang and Fenghua Yang and Huchuan Lu},
keywords = {Portrait video segmentation, Meta-learning, Feature reconstruction, Feature aggregation block},
abstract = {We explore a new video segmentation task, named portrait video segmentation (PVS), which aims to automatically segment the dominant person throughout a given portrait video. To achieve accurate and temporal-coherent segmentation results, a feature reconstruction based PVS method is developed under the meta-learning framework. Due to the dramatic pose variation and severe occlusion in portrait videos, feature reconstruction using existing optical flow models usually suffers from severe ghosting effects in reconstructed features. We mitigate this issue by presenting a soft correspondence network (SCN), which learns to facilitate feature reconstruction in an unsupervised fashion by softly assigning each pixel displacement probabilities between portrait frames. Based on the proposed SCN, a novel portrait segmentation network (PSN) is further designed, which explores the reconstructed features through feature aggregation blocks (FABs), yielding more reliable segmentation results. To capture temporal and target-specific cues, the parameters of FABs are determined by a meta-updater network which is trained offline in the meta-level. In addition, we introduce a new PVS dataset with high-quality segmentation annotations. Experimental results clearly demonstrate the effectiveness of the proposed PVS method.}
}
@article{KOROTIN2021108175,
title = {Mixability of integral losses: A key to efficient online aggregation of functional and probabilistic forecasts},
journal = {Pattern Recognition},
volume = {120},
pages = {108175},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108175},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003629},
author = {Alexander Korotin and Vladimir V’yugin and Evgeny Burnaev},
keywords = {Integral loss functions, Mixability, Exponential concavity, Prediction with expert advice, Functional forecasting, Probabilistic forecasting},
abstract = {In this paper we extend the setting of the online prediction with expert advice to function-valued forecasts. At each step of the online game several experts predict a function, and the learner has to efficiently aggregate these functional forecasts into a single forecast. We adapt basic mixable (and exponentially concave) loss functions to compare functional predictions and prove that these adaptations are also mixable (exp-concave). We call this phenomenon mixability (exp-concavity) of integral loss functions. As an application of our main result, we prove that various loss functions used for probabilistic forecasting are mixable (exp-concave). The considered losses include Sliced Continuous Ranked Probability Score, Energy-Based Distance, Optimal Transport Costs & Sliced Wasserstein-2 distance, Beta-2 & Kullback-Leibler divergences, Characteristic function and Maximum Mean Discrepancies.}
}
@article{GONG2021108171,
title = {A two-level framework for place recognition with 3D LiDAR based on spatial relation graph},
journal = {Pattern Recognition},
volume = {120},
pages = {108171},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108171},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003587},
author = {Yansong Gong and Fengchi Sun and Jing Yuan and Wenbin Zhu and Qinxuan Sun},
keywords = {Place recognition, 3D LiDAR, Spatial relation graph, Two-level framework},
abstract = {In the field of robotics, due to the complexity of real environments, place recognition using the 3D LiDAR is always a challenging problem. The spatial relations of internal structures underlying the LiDAR data from different places are distinguishable, which can be used to describe the environment. In this paper, we utilize the spatial relations of internal structures and propose a two-level framework for 3D LiDAR place recognition based on the spatial relation graph (SRG). At first, the proposed framework segments the point cloud into multiple clusters, then the features of the clusters and the spatial relation descriptors (SRDs) between the clusters are extracted, and the point cloud is represented by the SRG, which uses the clusters as the nodes and their spatial relations as the edges. After that, we propose a two-level matching model in which two different models are fused for accurately and efficiently matching the SRGs, including the upper-level searching model (U-LSM) and lower-level matching model (L-LMM). In the U-LSM, an incremental bag-of-words model is used to search for candidate SRGs through the distribution of the SRDs in the SRG. In the L-LMM, we utilize the improved spectral method to calculate similarities between the current SRG and the candidates. The experimental results demonstrate that our framework achieves good precision, recall and viewpoint robustness on both public benchmarks and self-built campus dataset.}
}
@article{ZHANG2021108147,
title = {A unified weight learning and low-rank regression model for robust complex error modeling},
journal = {Pattern Recognition},
volume = {120},
pages = {108147},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108147},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003344},
author = {Miaohua Zhang and Yongsheng Gao and Jun Zhou},
keywords = {Regression, Weight learning, Low-rank approximation, Generalized correntropy, Robust learning},
abstract = {One of the most important problems in regression-based error model is modeling the complex representation error caused by various corruptions and environment changes in images. For example, in robust face recognition, images are often affected by varying types and levels of corruptions, such as random pixel corruptions, block occlusions, or disguises. However, existing works are not robust enough to solve this problem due to they cannot model the complex corrupted errors very well. In this paper, we address this problem by a unified sparse weight learning and low-rank approximation regression model, which enables the random noises and contiguous occlusions in images to be treated simultaneously. For the random noise, we define a generalized correntropy (GC) function to match the error distribution. For the structured error caused by occlusions or disguises, we propose a GC function based rank approximation to measure the rank of error matrices. Since the proposed objective function is non-convex, an effective iterative optimization algorithm is developed to achieve the optimal weight learning and low-rank approximation. Extensive experimental results on three public face databases show that the proposed model can fit the error distribution and structure very well, thus obtain better recognition accuracies in comparison with the existing methods.}
}
@article{WANG2021108123,
title = {Statistical mechanical analysis for unweighted and weighted stock market networks},
journal = {Pattern Recognition},
volume = {120},
pages = {108123},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108123},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003101},
author = {Jianjia Wang and Xingchen Guo and Weimin Li and Xing Wu and Zhihong Zhang and Edwin R. Hancock},
keywords = {Stock market networks, Thermodynamic characterisations, Statistical mechanics},
abstract = {Financial markets are time-evolving complex systems containing different financial entities, such as banks, corporations and institutions that interact through transactions and respond to external economic and political events. They can be conveniently represented as a network structure. In this paper, we analyse the unweighted and weighted market networks from a statistical mechanical perspective. In particular, we propose a novel thermodynamic analogy to characterise the dynamic structural properties of time-evolving networks. The intricate pattern of edge connections in the network is modelled by using a heat bath analogy in which particles occupy the energy states according to the Boltzmann distribution. According to this analogy the occupation of the energy states is determined by the temperature of the heat bath, and the spectrum of energy states of the network is determined by the number of nodes and edges. For unweighted networks, the binary representation of the elements in the adjacency matrix can be modelled as a statistical ensemble, using the corresponding partition function to compute thermodynamic network characterisations. For weighted networks, on the other hand, the derived thermodynamic quantities together with their distribution of fluctuations identify the salient structure in the network evolution. We conduct experiments on time-evolving stock exchanges using data for the S&P500 Index Stock Exchanges over the past decade. The thermodynamic characterisations provide an excellent framework to identify epochs in which there is significant variance in network structure during financial crises induced by economic and political events.}
}
@article{FEI2021108139,
title = {Doubly supervised parameter transfer classifier for diagnosis of breast cancer with imbalanced ultrasound imaging modalities},
journal = {Pattern Recognition},
volume = {120},
pages = {108139},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108139},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003265},
author = {Xiaoyan Fei and Shichong Zhou and Xiangmin Han and Jun Wang and Shihui Ying and Cai Chang and Weijun Zhou and Jun Shi},
keywords = {Doubly supervised parameter transfer classifier, Support vector machine plus, Hilbert-Schmidt independence criterion, B-mode ultrasound, Breast cancer},
abstract = {The bimodal ultrasound, namely B-mode ultrasound (BUS) and elastography ultrasound (EUS), provide complementary information to improve the diagnostic accuracy of breast cancers. However, in clinical practice, it is easier to acquire the labeled BUS images than the paired bimodal ultrasound data with shared labels due to the lack of EUS devices, especially in many rural hospitals. Thus, the single-modal BUS-based computer-aided diagnosis (CAD) generally has wide applications. Transfer learning (TL) can promote a BUS-based CAD model by transferring additional knowledge from EUS modality. To make full use of labeled paired bimodal data and the additional single-modal BUS images for knowledge transfer, a novel doubly supervised parameter transfer classifier (DSPTC) is proposed to well handle the TL between imbalanced modalities with the guidance of label information. Specifically, the proposed DSPTC consists of two loss functions corresponding to the paired bimodal ultrasound data with shared labels and the unpaired images with different labels, respectively. The former uses the loss function in the specially designed TL paradigm of support vector machine plus, while the latter adopts the Hilbert-Schmidt Independence Criterion (HSIC) for knowledge transfer between the unpaired images, which consist of the single-modal BUS images and the EUS images from the paired bimodal data. Consequently, the doubly supervised knowledge transfer is implemented by way of parameter transfer in a unified optimization framework. Two experiments are designed to evaluate the proposed DSPTC for the ultrasound-based diagnosis of breast cancers. The experimental results indicate that DSPTC outperforms all the compared algorithms, suggesting its wide potential applications.}
}
@article{BAI2021108102,
title = {Explainable deep learning for efficient and robust pattern recognition: A survey of recent developments},
journal = {Pattern Recognition},
volume = {120},
pages = {108102},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108102},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002892},
author = {Xiao Bai and Xiang Wang and Xianglong Liu and Qiang Liu and Jingkuan Song and Nicu Sebe and Been Kim},
keywords = {Explainable deep learning, Network compression and acceleration, Adversarial robustness, Stability in deep learning},
abstract = {Deep learning has recently achieved great success in many visual recognition tasks. However, the deep neural networks (DNNs) are often perceived as black-boxes, making their decision less understandable to humans and prohibiting their usage in safety-critical applications. This guest editorial introduces the thirty papers accepted for the Special Issue on Explainable Deep Learning for Efficient and Robust Pattern Recognition. They are grouped into three main categories: explainable deep learning methods, efficient deep learning via model compression and acceleration, as well as robustness and stability in deep learning. For each of the three topics, a survey of the representative works and latest developments is presented, followed by the brief introduction of the accepted papers belonging to this topic. The special issue should be of high relevance to the reader interested in explainable deep learning methods for efficient and robust pattern recognition applications and it helps promoting the future research directions in this field.}
}
@article{AVERSANO2021108135,
title = {Deep neural networks ensemble to detect COVID-19 from CT scans},
journal = {Pattern Recognition},
volume = {120},
pages = {108135},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108135},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003228},
author = {Lerina Aversano and Mario Luca Bernardi and Marta Cimitile and Riccardo Pecori},
keywords = {Deep learning, CT Scan images, COVID-19, Coronavirus},
abstract = {Research on Coronavirus Disease 2019 (COVID-19) detection methods has increased in the last months as more accurate automated toolkits are required. Recent studies show that CT scan images contain useful information to detect the COVID-19 disease. However, the scarcity of large and well balanced datasets limits the possibility of using detection approaches in real diagnostic contexts as they are unable to generalize. Indeed, the performance of these models quickly becomes inadequate when applied to samples captured in different contexts (e.g., different equipment or populations) from those used in the training phase. In this paper, a novel ensemble-based approach for more accurate COVID-19 disease detection using CT scan images is proposed. This work exploits transfer learning using pre-trained deep networks (e.g., VGG, Xception, and ResNet) evolved with a genetic algorithm, combined into an ensemble architecture for the classification of clustered images of lung lobes. The study is validated on a new dataset obtained as an integration of existing ones. The results of the experimental evaluation show that the ensemble classifier ensures effective performance, also exhibiting better generalization capabilities.}
}
@article{LAO2021108127,
title = {FoCL: Feature-oriented continual learning for generative models},
journal = {Pattern Recognition},
volume = {120},
pages = {108127},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108127},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003149},
author = {Qicheng Lao and Mehrzad Mortazavi and Marzieh Tahaei and Francis Dutil and Thomas Fevens and Mohammad Havaei},
keywords = {Catastrophic forgetting, Continual learning, Generative models, Feature matching, Generative replay, Pseudo-rehearsal},
abstract = {In this paper, we propose a general framework in continual learning for generative models: Feature-oriented Continual Learning (FoCL). Unlike previous works that aim to solve the catastrophic forgetting problem by introducing regularization in the parameter space or image space, FoCL imposes regularization in the feature space. We show in our experiments that FoCL has faster adaptation to distributional changes in sequentially arriving tasks, and achieves state-of-the-art performance for generative models in task incremental learning. We discuss choices of combined regularization spaces towards different use case scenarios for boosted performance, e.g., tasks that have high variability in the background. Finally, we introduce a forgetfulness measure that fairly evaluates the degree to which a model suffers from forgetting. Interestingly, the analysis of our proposed forgetfulness score also implies that FoCL tends to have a mitigated forgetting for future tasks.}
}
@article{WANG2021108082,
title = {Non-uniform motion deblurring with blurry component divided guidance},
journal = {Pattern Recognition},
volume = {120},
pages = {108082},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108082},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002697},
author = {Pei Wang and Wei Sun and Qingsen Yan and Axi Niu and Rui Li and Yu Zhu and Jinqiu Sun and Yanning Zhang},
keywords = {Non-uiniform deblurring, Component divided, Attention mechanism},
abstract = {Blind image deblurring is a fundamental and challenging computer vision problem, which aims to recover both the blur kernel and the latent sharp image from only a blurry observation. Despite the superiority of deep learning methods in image deblurring have displayed, there still exists a major challenge with various non-uniform motion blur. Previous methods simply take all the image features as the input to the decoder, which handles different degrees (e.g. large blur, small blur) simultaneously, leading to challenges for sharp image generation. To tackle the above problems, we present a deep two-branch network to deal with blurry images via a component divided module, which divides an image into two components based on the representation of blurry degree. Specifically, two component attentive blocks are employed to learn attention maps to exploit useful deblurring feature representations on both large and small blurry regions. Then, the blur-aware features are fed into two-branch reconstruction decoders respectively. In addition, a new feature fusion mechanism, orientation-based feature fusion, is proposed to merge sharp features of the two branches. Both qualitative and quantitative experimental results show that our method performs favorably against the state-of-the-art approaches.}
}
@article{ZHANG2021108155,
title = {Part-guided graph convolution networks for person re-identification},
journal = {Pattern Recognition},
volume = {120},
pages = {108155},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108155},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003423},
author = {Zhong Zhang and Haijia Zhang and Shuang Liu and Yuan Xie and Tariq S. Durrani},
keywords = {Person re-identification, Graph convolution network},
abstract = {Recently, part-based deep models have achieved promising performance in person re-identification (Re-ID), yet these models ignore the inter-local relationship of the corresponding parts among pedestrian images and the intra-local relationship between adjacent parts in one pedestrian image. As a result, the feature representations are hard to learn the information from the same parts of other pedestrian images and are lack of the contextual information of pedestrian. In this paper, we propose a novel deep graph model named Part-Guided Graph Convolution Network (PGCN) for person Re-ID, which could simultaneously learn the inter-local relationship and the intra-local relationship for feature representations. Specifically, we construct the inter-local graph using the local features extracted from the same parts of pedestrian images and build the adjacency matrix using the similarity so as to mine the inter-local relationship. Meanwhile, we construct the intra-local graph using the local features extracted from different body parts in one pedestrian image, and propose the fractional dynamic mechanism (FDM) to accurately describe the correlations between adjacent parts in the optimization process. Finally, after the graph convolutional operation, the inter-local relationship and the intra-local relationship are injected into the feature representations of pedestrian images. Extensive experiments are conducted on Market-1501, CUHK03, DukeMTMC-reID and MSMT17, and the experimental results show the proposed PGCN exceeds state-of-the-art methods by an overwhelming margin.}
}
@article{BICICI2021108151,
title = {Conditional information gain networks as sparse mixture of experts},
journal = {Pattern Recognition},
volume = {120},
pages = {108151},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108151},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003381},
author = {Ufuk Can Bicici and Lale Akarun},
keywords = {Machine learning, Deep learning, Conditional deep learning},
abstract = {Deep neural network models owe their representational power and high performance in classification tasks to the high number of learnable parameters. Running deep neural network models in limited-resource environments is a problematic task. Models employing conditional computing aim to reduce the computational burden while retaining model performance on par with more complex neural network models. This paper, proposes a new model, Conditional Information Gain Networks as Sparse Mixture of Experts (sMoE-CIGNs). A CIGN model is a neural tree that allows conditionally skipping parts of the tree based on routing mechanisms inserted into the architecture. These routing mechanisms are based on differentiable Information Gain objectives. CIGN groups semantically similar samples in the leaves, enabling simpler classifiers to focus on differentiating between similar classes. This lets the CIGN model attain high classification performances with lighter models. We further improve the basic CIGN model by proposing a sparse mixture of experts model for difficult to classify samples that may get routed to suboptimal branches. If a sample has routing confidence higher than a specific threshold, the sample may be routed towards multiple child nodes. The classification decision can then be taken as a mixture of these expert decisions. We learn the optimal routing thresholds by Bayesian Optimization over a validation set by minimizing a weighted loss, including the classification accuracy and the number of multiplication and accumulations (MAC). We show the effectiveness of the CIGN models enhanced with the Sparse Mixture of Experts approach with extensive tests on MNIST, Fashion MNIST, CIFAR 100 and UCI-USPS datasets, as well as comparisons with methods from the literature. sMoE-CIGN models can retain high generalization performance, on par with a thick unconditional model while keeping the operation burden at the same level with a much thinner model.11This article is an extended version of reference “Bicici U.C., Keskin C., Akarun L., Conditional information gain networks”, which appeared in Proceedings of the 24th International Conference on Pattern Recognition (ICPR) (2018).}
}
@article{MENSI2021108115,
title = {Enhanced anomaly scores for isolation forests},
journal = {Pattern Recognition},
volume = {120},
pages = {108115},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108115},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003022},
author = {Antonella Mensi and Manuele Bicego},
keywords = {Anomaly detection, Isolation forest, Anomaly score, Outliers},
abstract = {Isolation Forest represents a variant of Random Forest largely and successfully employed for outlier detection. The main idea is that outliers are likely to get isolated in a tree after few splits. The anomaly score is therefore a function inversely related to the leaf depth. This paper proposes enhanced anomaly scores of the Isolation Forest by making two different contributions. The first consists in weighing the path traversed by an object to obtain a more informative anomaly score. The second contribution employs a different aggregation function to combine the tree scores. We thoroughly evaluate the proposed methodology by testing it on sixteen datasets.}
}
@article{ALAOUIMHAMDI2021108131,
title = {3D object recognition through a size function resulting from an invariant topological feature},
journal = {Pattern Recognition},
volume = {120},
pages = {108131},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108131},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003186},
author = {Mohammed Ayoub {Alaoui Mhamdi} and Djemel Ziou},
keywords = { Object description, Object recognition, Object categorization, Shape classification},
abstract = {In this paper, a critical points based descriptor for 3D objects recognition is presented. It is based on the topological invariant provided by the critical points of the 3D object. The critical points and the links between them are represented by a size function resulting from a measure function that captures the surface displacement along the 3D object, and that encompasses invariance to affine transformations, articulations and torsions. In order to tackle the problems of partial matching of the 3D objects, a well-suited metric learning method is used to weight the matchings according to their relevance. The proposed method’s performance was validated by different collections of 3D objects. The obtained scores are favorably comparable to the related work.}
}
@article{GHARAEE2021108174,
title = {Graph representation learning for road type classification},
journal = {Pattern Recognition},
volume = {120},
pages = {108174},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108174},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003617},
author = {Zahra Gharaee and Shreyas Kowshik and Oliver Stromann and Michael Felsberg},
keywords = {Road network graphs, Graph representation learning, Line graph transformation, Neighborhood aggregation, Topological neighborhood},
abstract = {We present a novel learning-based approach to graph representations of road networks employing state-of-the-art graph convolutional neural networks. Our approach is applied to realistic road networks of 17 cities from Open Street Map. While edge features are crucial to generate descriptive graph representations of road networks, graph convolutional networks usually rely on node features only. We show that the highly representative edge features can still be integrated into such networks by applying a line graph transformation. We also propose a method for neighborhood sampling based on a topological neighborhood composed of both local and global neighbors. We compare the performance of learning representations using different types of neighborhood aggregation functions in transductive and inductive tasks and in supervised and unsupervised learning. Furthermore, we propose a novel aggregation approach, Graph Attention Isomorphism Network, GAIN1. Our results show that GAIN outperforms state-of-the-art methods on the road type classification problem.}
}
@article{ZHENG2021108153,
title = {Knowledge base graph embedding module design for Visual question answering model},
journal = {Pattern Recognition},
volume = {120},
pages = {108153},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108153},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100340X},
author = {Wenfeng Zheng and Lirong Yin and Xiaobing Chen and Zhiyang Ma and Shan Liu and Bo Yang},
keywords = {Faster R-CNN, DBpedia spotlight, knowledge base, VQA},
abstract = {In this paper, a knowledge base graph embedding module is constructed to extend the versatility of knowledge-based VQA (Visual Question Answering) models. The knowledge base graph embedding module constructed in this paper extracts core entities from images and text, and maps them as knowledge base entities, then extracts the sub-graphs closely related to the core entities, and converts the sub-graphs into low-dimensional vectors to realize sub-graph embedding. In order to achieve good subgraph embedding, we first extracted two experimental knowledge bases with rich semantics from DBpedia: DBV and DBA. Based on these two knowledge bases, this paper selects several excellent models in knowledge base embedding as test models, including SE (structured embedding),SME(semantic matching energy function), and TransE model to produce link prediction. The results show that there is a clear correspondence between the entities of the DBV, which can achieve excellent node embedding. And the TransE model can achieve a good knowledge base embedding, so we built the knowledge base graph embedding module based on TransE. And then we construct a VQA model (KBSN) based on the knowledge base graph embedding. Experimental results on VQA2.0 and KB-VQA data sets prove that the knowledge base graph embedding module improves the accuracy.}
}
@article{ZHONG2021108154,
title = {Selection of diverse features with a diverse regularization},
journal = {Pattern Recognition},
volume = {120},
pages = {108154},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108154},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003411},
author = {Weichan Zhong and Xiaojun Chen and Qingyao Wu and Min Yang and Joshua Zhexue Huang},
keywords = {Feature selection, Supervised feature selection, Diverse feature, Regularization},
abstract = {Many embedded feature selection methods ignore the correlation among the important features. To reduce correlation, some models introduce constraints to impose sparsity on features, some try to exploit the similarity and group features without changing the objective function. In this paper, we propose diverse feature selection (DFS), which simultaneously performs feature clustering and selection. Given a dataset with known class labels, we separate the features into a set of feature clusters where the features in the same cluster have a higher correlation with each other than with the features in different clusters. A diverse regularization (DR) is proposed to reduce the linear and nonlinear correlations among important features. Using this regularization, DFS can select features that are both informative and diverse. The experimental results on seven image datasets, five gene datasets as well as four other datasets demonstrate the superior performance of DFS.}
}
@article{KIM2021108118,
title = {Spatial reasoning for few-shot object detection},
journal = {Pattern Recognition},
volume = {120},
pages = {108118},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108118},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003058},
author = {Geonuk Kim and Hong-Gyu Jung and Seong-Whan Lee},
keywords = {Few-shot learning, Object detection, Transfer learning, Visual reasoning, Data augmentation},
abstract = {Although modern object detectors rely heavily on a significant amount of training data, humans can easily detect novel objects using a few training examples. The mechanism of the human visual system is to interpret spatial relationships among various objects and this process enables us to exploit contextual information by considering the co-occurrence of objects. Thus, we propose a spatial reasoning framework that detects novel objects with only a few training examples in a context. We infer geometric relatedness between novel and base RoIs (Region-of-Interests) to enhance the feature representation of novel categories using an object detector well trained on base categories. We employ a graph convolutional network as the RoIs and their relatedness are defined as nodes and edges, respectively. Furthermore, we present spatial data augmentation to overcome the few-shot environment where all objects and bounding boxes in an image are resized randomly. Using the PASCAL VOC and MS COCO datasets, we demonstrate that the proposed method significantly outperforms the state-of-the-art methods and verify its efficacy through extensive ablation studies.}
}
@article{MESSOUDI2021108101,
title = {Copula-based conformal prediction for multi-target regression},
journal = {Pattern Recognition},
volume = {120},
pages = {108101},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108101},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002880},
author = {Soundouss Messoudi and Sébastien Destercke and Sylvain Rousseau},
keywords = {Inductive conformal prediction, Copula functions, Multi-target regression, Deep neural networks, Random forests},
abstract = {There are relatively few works dealing with conformal prediction for multi-task learning issues, and this is particularly true for multi-target regression. This paper focuses on the problem of providing valid (i.e., frequency calibrated) multi-variate predictions. To do so, we propose to use copula functions for inductive conformal prediction, and illustrate our proposal by applying it to deep neural networks and random forests. We show that the proposed method ensures efficiency and validity for multi-target regression problems on various data sets.}
}
@article{WANG2021108158,
title = {Automated delineation of corneal layers on OCT images using a boundary-guided CNN},
journal = {Pattern Recognition},
volume = {120},
pages = {108158},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108158},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003459},
author = {Lei Wang and Meixiao Shen and Qian Chang and Ce Shi and Yang Chen and Yuheng Zhou and Yanchun Zhang and Jiantao Pu and Hao Chen},
keywords = {Corneal layers, OCT images, Segmentation, Convolutional neural networks},
abstract = {Accurate segmentation of corneal layers depicted on optical coherence tomography (OCT) images is very helpful for quantitatively assessing and diagnosing corneal diseases (e.g., keratoconus and dry eye). In this study, we presented a novel boundary-guided convolutional neural network (CNN) architecture (BG-CNN) to simultaneously extract different corneal layers and delineate their boundaries. The developed BG-CNN architecture used three convolutional blocks to construct two network modules on the basis of the classical U-Net network. We trained and validated the network on a dataset consisting of 1,712 OCT images acquired on 121 subjects using a 10-fold cross-validation method. Our experiments showed an average dice similarity coefficient (DSC) of 0.9691, an intersection over union (IOU) of 0.9411, and a Hausdorff distance (HD) of 7.4423 pixels. Compared with several other classical networks, namely U-Net, Attention U-Net, Asymmetric U-Net, BiO-Net, CE-Net, CPFnte, M-Net, and Deeplabv3, on the same dataset, the developed network demonstrated a promising performance, suggesting its unique strength in segmenting corneal layers depicted on OCT images.}
}
@article{JAMPOUR2021107851,
title = {CapsNet regularization and its conjugation with ResNet for signature identification},
journal = {Pattern Recognition},
volume = {120},
pages = {107851},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.107851},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321000388},
author = {Mahdi Jampour and Saeid Abbaasi and Malihe Javidi},
keywords = {Regularized capsule neural network, Residual neural network, CapsNet regularization, CapsNet and ResNet conjugation, Signature recognition},
abstract = {We propose a new regularization term for CapsNet that significantly improves the generalization power of the original method from small training data while requiring much fewer parameters, making it suitable for large input images. We also propose a very efficient DNN architecture that integrates CapsNet with ResNet to obtain the advantages of the two architectures. CapsNet allows a powerful understanding of the objects’ components and their positions, while ResNet provides efficient feature extraction and description. Our approach is general, and we demonstrate it on the problem of signature identification from images. To show our approach superiority, we provide several evaluations with different protocols. We also show that our approach outperforms the state-of-the-art on this problem with thorough experiments on three publicly available datasets CEDAR, MCYT, and UTSig.}
}
@article{TIAN2021108186,
title = {Discretization-aware architecture search},
journal = {Pattern Recognition},
volume = {120},
pages = {108186},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108186},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003733},
author = {Yunjie Tian and Chang Liu and Lingxi Xie and Jianbin jiao and Qixiang Ye},
keywords = {Neural architecture search, Weight-sharing, Discretization-aware, Imbalanced network configuration},
abstract = {The search cost of neural architecture search (NAS) has been largely reduced by differentiable architecture search and weight-sharing methods. Such methods optimize a super-network with all possible edges and operations, and determine the optimal sub-network by discretization, i.e., pruning off operations/edges of small weights. However, the discretization process performed on either operations or edges incurs significant inaccuracy and thus the quality of the architecture is not guaranteed. In this paper, we propose discretization-aware architecture search (DA2S), and target at pushing the super-network towards the configuration of desired topology. DA2S is implemented with an entropy-based loss term, which can be regularized to differentiable architecture search in a plug-and-play fashion. The regularization is controlled by elaborated continuation functions, so that discretization is adaptive to the dynamic change of edges and operations. Experiments on standard image classification benchmarks demonstrate the effectiveness of our approach, in particular, under imbalanced network configurations that were not studied before. Code is available at github.com/sunsmarterjie/DAAS.}
}
@article{GORGULU2021108097,
title = {Randomized trees for time series representation and similarity},
journal = {Pattern Recognition},
volume = {120},
pages = {108097},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108097},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002843},
author = {Berk Görgülü and Mustafa Gökçe Baydoğan},
keywords = {Time series, Representation learning, Random trees, Classification},
abstract = {Most of the temporal data mining tasks require representations to capture important characteristics of time series. Representation learning is challenging when time series differ in distributional characteristics and/or show irregularities such as varying lengths and missing observations. Moreover, when time series are multivariate, interactions between variables should be modeled efficiently. This study proposes a unified, flexible time series representation learning framework for both univariate and multivariate time series called Rand-TS. Rand-TS models density characteristics of each time series as a time-varying Gaussian distribution using random decision trees and embeds density information into a sparse vector. Rand-TS can work with time series of various lengths and missing observations, furthermore, it allows using customized features. We illustrate the classification performance of Rand-TS on 113 univariate, 19 multivariate along with 15 univariate time series with varying lengths from UCR database. The results show that in addition to its flexibility, Rand-TS provides competitive classification performance.}
}
@article{KIM2021108126,
title = {Normalized class coherence change-based kNN for classification of imbalanced data},
journal = {Pattern Recognition},
volume = {120},
pages = {108126},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108126},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003137},
author = {Kyoungok Kim},
keywords = {NN, Nearest neighbor classification, Imbalanced data, Class coherence},
abstract = {kNN is a widely used machine learning algorithm in many different domains because of its fairly good performance in actual cases and its simplicity. This study aims to enhance the performance of kNN for imbalanced datasets, a topic that has been relatively ignored in kNN research. The proposed kNN algorithm, called normalized class coherence change-based k-nearest neighbor (NCC-NN) algorithm, determines the label of a test sample by computing the normalized class coherence changes at class and sample levels for every possible class and assigning the sample to the class with the maximum value. It considers the tendency that the minority classes usually show the lower-class coherence than the majority class. NCC-kNN also utilizes the adaptive k for the class coherence, which is calculated in a weighted manner to reduce the sensitivity to the selection of k. NCC-kNN was applied to 20 benchmark datasets with varying class imbalance and coherence, and its performance was compared with that of five kNN algorithms, SMOTE and MetaCost with standard kNN as a base classifier. The proposed NCC-kNN outperformed the other kNN algorithms in classification of imbalanced data, especially for imbalanced data with low positive class coherence.}
}
@article{JIA2021108122,
title = {A survey on dorsal hand vein biometrics},
journal = {Pattern Recognition},
volume = {120},
pages = {108122},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108122},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003095},
author = {Wei Jia and Wei Xia and Bob Zhang and Yang Zhao and Lunke Fei and Wenxiong Kang and Di Huang and Guodong Guo},
keywords = {Biometrics, Dorsal hand vein, Survey},
abstract = {Biometrics technology is one of the most important and effective solutions for personal authentication. In recent years, as one of the emerging biometrics technologies, dorsal hand vein (DHV) biometrics has received a lot of attention. In fact, DHV biometrics has been studied for more than 30 years, during which different problems related to DHV recognition have been addressed. In this paper, we conduct a comprehensive survey on the state-of-the-art in DHV biometrics. Nearly all important aspects of DHV biometrics have been summarized, including the developmental history, data acquisition, databases, preprocessing algorithms, feature extraction and matching algorithms, information fusion schemes and commercial products. We also discuss the challenges and future directions in DHV biometrics research.}
}
@article{XU2021108142,
title = {Learnable low-rank latent dictionary for subspace clustering},
journal = {Pattern Recognition},
volume = {120},
pages = {108142},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108142},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003290},
author = {Yesong Xu and Shuo Chen and Jun Li and Lei Luo and Jian Yang},
keywords = {Subspace clustering, Low-rank, Feature extraction, Block diagonal representation},
abstract = {Recently, Self-Expressive-based Subspace Clustering (SESC) has been widely applied in pattern clustering and machine learning as it aims to learn a representation that can faithfully reflect the correlation between data points. However, most existing SESC methods directly use the original data as the dictionary, which miss the intrinsic structure (e.g., low-rank and nonlinear) of the real-word data. To address this problem, we propose a novel Projection Low-Rank Subspace Clustering (PLRSC) method by integrating feature extraction and subspace clustering into a unified framework. In particular, PLRSC learns a projection transformation to extract the low-dimensional features and utilizes a low-rank regularizer to ensure the informative and important structures of the extracted features. The extracted low-rank features effectively enhance the self-expressive property of the dictionary. Furthermore, we extend PLRSC to a nonlinear version (i.e., NPLRSC) by integrating a nonlinear activator into the projection transformation. NPLRSC cannot only effectively extract features but also guarantee the data structure of the extracted features. The corresponding optimization problem is solved by the Alternating Direction Method (ADM), and we also prove that the algorithm converges to a stationary point. Experimental results on the real-world datasets validate the superior of our model over the existing subspace clustering methods.}
}
@article{ZHENG2021108138,
title = {Top-rank convolutional neural network and its application to medical image-based diagnosis},
journal = {Pattern Recognition},
volume = {120},
pages = {108138},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108138},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003253},
author = {Yan Zheng and Yuchen Zheng and Daiki Suehiro and Seiichi Uchida},
keywords = {Top-rank learning, Representation learning, Medical diagnosis},
abstract = {Top-rank learning identifies a real-valued ranking function that will provide more absolute top samples. These are highly reliable positive samples that are ranked higher than the highest-ranked negative samples. Therefore, top-rank learning is useful for tasks that require reliable decisions. Additionally, it inherits the merits of the ranking functions, such as robustness to the unbalanced condition. However, conventional top-rank learning tasks are formulated as linear or kernel-based problems and are thus limited in coping with complicated tasks. In this study, we propose a Top-rank convolutional neural network (TopRank CNN) to realize top-rank learning with representation learning for complicated tasks. Given that the original objective function of top-rank learning suffers from overfitting, we employ the p-norm relaxation of the original loss function in the proposed method. We prove the usefulness of TopRank CNN experimentally with medical diagnosis tasks that require reliable decisions and robustness to the unbalanced condition.}
}
@article{KOZIARSKI2021108114,
title = {Potential Anchoring for imbalanced data classification},
journal = {Pattern Recognition},
volume = {120},
pages = {108114},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108114},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003010},
author = {Michał Koziarski},
keywords = {Machine learning, Classification, Imbalanced data, Oversampling, Undersampling, Radial basis functions},
abstract = {Data imbalance remains one of the factors negatively affecting the performance of contemporary machine learning algorithms. One of the most common approaches to reducing the negative impact of data imbalance is preprocessing the original dataset with data-level strategies. In this paper we propose a unified framework for imbalanced data over- and undersampling. The proposed approach utilizes radial basis functions to preserve the original shape of the underlying class distributions during the resampling process. This is done by optimizing the positions of generated synthetic observations with respect to the proposed potential resemblance loss. The final Potential Anchoring algorithm combines over- and undersampling within the proposed framework. The results of the experiments conducted on 60 imbalanced datasets show outperformance of Potential Anchoring over state-of-the-art resampling algorithms, including previously proposed methods that utilize radial basis functions to model class potential. Furthermore, the results of the analysis based on the proposed data complexity index show that Potential Anchoring is particularly well suited for handling naturally complex (i.e. not affected by the presence of noise) datasets.}
}
@article{SUN2021108130,
title = {Community-based k-shell decomposition for identifying influential spreaders},
journal = {Pattern Recognition},
volume = {120},
pages = {108130},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108130},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003174},
author = {Peng Gang Sun and Qiguang Miao and Steffen Staab},
keywords = {Influential spreader, Community-based -shell decomposition, Linear threshold model},
abstract = {How to identify the most influential nodes in a network for the maximization of influence spread is a great challenge. Known methods like k-shell decomposition determine core nodes who individually might be the most influential spreaders for the spreading originating in a single origin. However, these techniques are not suitable for determining multiple origins that together lead to the most effective spreading. The reason is that core nodes are often found to be located closely to each other, which results in large overlapping regions rather than spreading far across the network. In this paper, we propose a new algorithm, called community-based k-shell decomposition, by which a network can be viewed as multiple hierarchically ordered structures each branching off from the innermost shell to the periphery shell. To alleviate the overlap problem, our algorithm pursues a greedy strategy that preferably selects core nodes from different communities in the network, thus maximizing the joint influence of multiple origins. We systematically evaluate our algorithm against competing algorithms on multiple networks with varying network characteristics, and find that our algorithm outperforms other algorithms on networks that exhibit community structures, and the stronger communities, the better performance.}
}