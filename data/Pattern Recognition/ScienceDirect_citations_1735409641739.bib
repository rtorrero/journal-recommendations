@article{ZHANG2022108490,
title = {TradeBot: Bandit learning for hyper-parameters optimization of high frequency trading strategy},
journal = {Pattern Recognition},
volume = {124},
pages = {108490},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108490},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100666X},
author = {Weipeng Zhang and Lu Wang and Liang Xie and Ke Feng and Xiang Liu},
keywords = {High-Frequency trading, Hyper-parameter optimization, Multi-armed bandit learning, Inverse reinforcement learning},
abstract = {Quantitative trading takes advantage of mathematical functions for automatically making stock or futures trading decisions. Specifically, various trading strategies that proposed by human-experts are associated with weight hyper-parameters to determine the probability of selecting a specific strategy according to market conditions. Prior work manually adjusting the weight hyper-parameters is error-prone, because the essential advantage of quantitative trading, i.e., automation, is lost. In this paper, we propose a dynamic parameter tuning algorithm, i.e., TradeBot, based on bandit learning for quantitative trading. We consider sequentially selecting hyper-parameters of rules for trading as a bandit game, where a set of hyper-parameters of trading rule is considered as an action. A novel reward-agnostic Upper Confidence Bound bandit method is proposed to solve the automatically trading problem with a reward function estimated by inverse reinforcement learning. Experimental results on China Commodity Futures Market Data show state-of-the-art performance. To our best knowledge, this is one of the first work deployed in the online trading system via reinforcement learning, in published literature.}
}
@article{YUAN2022108495,
title = {Adaptive Gabor convolutional networks},
journal = {Pattern Recognition},
volume = {124},
pages = {108495},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108495},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006713},
author = {Ye Yuan and Li-Na Wang and Guoqiang Zhong and Wei Gao and Wencong Jiao and Junyu Dong and Biao Shen and Dongdong Xia and Wei Xiang},
keywords = {Gabor filters, Deep convolutional neural networks, Invariant information, Gabor convolutional filters, Image classification},
abstract = {Despite the great breakthroughs that deep convolutional neural networks (DCNNs) have achieved on image representation learning in recent years, they lack the ability to extract invariant information from images. On the other hand, several traditional feature extractors like Gabor filters are widely used for invariant information learning from images. In this paper, we propose a new class of DCNNs named adaptive Gabor convolutional networks (AGCNs). In the AGCNs, the convolutional kernels are adaptively multiplied by Gabor filters to construct the Gabor convolutional filters (GCFs), while the parameters in the Gabor functions (i.e., scale and orientation) are learned alongside those in the convolutional kernels. In addition, the GCFs can be regenerated after updating the Gabor filters and convolutional kernels. We evaluate the performance of the proposed AGCNs on image classification using five benchmark image datasets, i.e., MNIST and its rotated version, SVHN, CIFAR-10, CINIC-10, and DogsVSCats. Experimental results show that the AGCNs are robust to spatial transformations and have achieved higher accuracy compared with the DCNNs and other state-of-the-art deep networks. Moreover, the GCFs can be easily embedded into any classical DCNN models (e.g., ResNet) and require fewer parameters than the corresponding DCNNs.}
}
@article{HAN2022108519,
title = {A unified perspective of classification-based loss and distance-based loss for cross-view gait recognition},
journal = {Pattern Recognition},
volume = {125},
pages = {108519},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108519},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006956},
author = {Feng Han and Xuejian Li and Jian Zhao and Furao Shen},
keywords = {Biometrics, Gait recognition, Computer vision, Metric learning, Angular softmax loss function, Triplet loss function},
abstract = {Gait can be used to recognize people in an uncooperative and noninvasive manner and it is hard to imitate or counterfeit, which makes it suitable for video surveillance. The current solutions for gait recognition are still not robust to handle the conditions when the view angles of the gallery and query are different. We improve the performance of cross-view gait recognition from the perspective of metric learning. Specifically, we propose to use angular softmax loss to impose an angular margin for extracting separable features. At the same time, we use triplet loss to make the extracted features more discriminative. Additionally, we add a batch-normalization layer after extracting gait features to effectively optimize two different losses. We evaluate our approach on two widely-used gait dataset: CASIA-B dataset and TUM GAID dataset. The experiment results show that our approach outperforms the prior state-of-the-art approaches, which shows the effectiveness of our approach.}
}
@article{SUN2022108467,
title = {Learning to rectify for robust learning with noisy labels},
journal = {Pattern Recognition},
volume = {124},
pages = {108467},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108467},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006439},
author = {Haoliang Sun and Chenhui Guo and Qi Wei and Zhongyi Han and Yilong Yin},
keywords = {Label noise, Meta-learning, Probabilistic model, Robust learning},
abstract = {Label noise significantly degrades the generalization ability of deep models in applications. Effective strategies and approaches (e.g., re-weighting or loss correction) are designed to alleviate the negative impact of label noise when training a neural network. Those existing works usually rely on the pre-specified architecture and manually tuning the additional hyper-parameters. In this paper, we propose warped probabilistic inference (WarPI) to achieve adaptively rectifying the training procedure for the classification network within the meta-learning scenario. In contrast to the deterministic models, WarPI is formulated as a hierarchical probabilistic model by learning an amortization meta-network, which can resolve sample ambiguity and be therefore more robust to serious label noise. Unlike the existing approximated weighting function of directly generating weight values from losses, our meta-network is learned to estimate a rectifying vector from the input of the logits and labels, which has the capability of leveraging sufficient information lying in them. The procedure provides an effective way to rectify the learning procedure for the classification network, demonstrating a significant improvement of the generalization ability. Besides, modeling the rectifying vector as a latent variable and learning the meta-network can be seamlessly integrated into the SGD optimization of the classification network. We evaluate WarPI on four benchmarks of robust learning with noisy labels and achieve the new state-of-the-art under variant noise types. Extensive study and analysis also demonstrate the effectiveness of our model.}
}
@article{LI2022108455,
title = {Text-instance graph: Exploring the relational semantics for text-based visual question answering},
journal = {Pattern Recognition},
volume = {124},
pages = {108455},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108455},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006312},
author = {Xiangpeng Li and Bo Wu and Jingkuan Song and Lianli Gao and Pengpeng Zeng and Chuang Gan},
keywords = {Text-based visual question answering, Spatial overlapping, Text-Instance graph, Copy mechanism},
abstract = {It is time to stop neglecting the text around your world. In VQA, the surrounding text helps humans to understand complete visual scenes and reason question semantics efficiently. Here, we address the challenging Text-based Visual Question Answering (TextVQA) problem, which requires a model to answer the VQA questions with text reading ability. Existing TextVQA methods mainly focus on the latent relationships between detected object instances and scene texts with the given question, but ignore spatial location relationships and complex relational semantics between visual object instances and OCR texts (e.g. the A of B on C). To deal with these challenges, we propose a novel Text-Instance Graph (TIG) network for TextVQA. The TIG builds an OCR-OBJ graph for overlapping relationships modeling, where each node of graph is updated by utilizing relative objects or OCR texts. To deal with the question with complex logic, we propose a dynamic OCR-OBJ graph network to extend the perception space of graph nodes, which grasps the information of non-directly adjacent node features. Considering a scene about “the brand of the computer on the table”, the model would build correlations between “brand” and “table” using “the computer” node as the intermediate node. Extensive experiments on three benchmarks demonstrate the effectiveness and superiority of the proposed method. In addition, our TIG achieves 0.505 ANLS on ST-VQA challenge leaderboard and sets a new state-of-the-art.}
}
@article{SALLOUM2022108378,
title = {cPCA++: An efficient method for contrastive feature learning},
journal = {Pattern Recognition},
volume = {124},
pages = {108378},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108378},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005586},
author = {Ronald Salloum and C.-C. Jay Kuo},
keywords = {PCA, Contrastive PCA, Feature learning, Dimensionality reduction},
abstract = {In this work, we propose a new data visualization and clustering technique for discovering discriminative structures in high-dimensional data. This technique, referred to as cPCA++, is motivated by the fact that the interesting features of a “target” dataset may be obscured by high variance components during traditional PCA. By analyzing what is referred to as a “background” dataset (i.e., one that exhibits the high variance principal components but not the interesting structures), our technique is capable of efficiently highlighting the structures that are unique to the “target” dataset. Similar to another recently proposed algorithm called “contrastive PCA” (cPCA), the proposed cPCA++ method identifies important dataset-specific patterns that are not detected by traditional PCA in a wide variety of settings. However, unlike cPCA, the proposed cPCA++ method does not require a parameter sweep, and as a result, it is significantly more efficient. Several experiments were conducted in order to compare the proposed method to state-of-the-art methods. These experiments show that the proposed method achieves performance that is similar to or better than that of the other methods, while being more efficient.}
}
@article{LUO2022108427,
title = {Segmentation information with attention integration for classification of breast tumor in ultrasound image},
journal = {Pattern Recognition},
volume = {124},
pages = {108427},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108427},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006038},
author = {Yaozhong Luo and Qinghua Huang and Xuelong Li},
keywords = {Computer-aided diagnosis, Breast ultrasound, Deep convolution neural network, Feature combination},
abstract = {Breast cancer is one of the most common forms of cancer among women worldwide. The development of computer-aided diagnosis (CAD) technology based on ultrasound imaging to promote the diagnosis of breast lesions has attracted the attention of researchers and deep learning is a popular and effective method. However, most of the deep learning based CAD methods neglect the relationship between two vision tasks tumor region segmentation and classification. In this paper, taking into account some prior knowledges of medicine, we propose a novel segmentation-to-classification scheme by adding the segmentation-based attention (SBA) information to the deep convolution network (DCNN) for breast tumors classification. A segmentation network is trained to generate tumor segmentation enhancement images. Then two parallel networks extract features for the original images and segmentation enhanced images and one channel attention based feature aggregation network is to automatically integrate the features extracted from two feature networks to improve the performance of recognizing malignant tumors in the breast ultrasound images. To validate our method, experiments have been conducted on breast ultrasound datasets. The classification results of our method have been compared with those obtained by eleven existing approaches. The experimental results show that the proposed method achieves the highest Accuracy (90.78%), Sensitivity (91.18%), Specificity (90.44%), F1-score (91.46%), and AUC (0.9549).}
}
@article{FENG2022108503,
title = {Identifying players in broadcast videos using graph convolutional network},
journal = {Pattern Recognition},
volume = {124},
pages = {108503},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108503},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006798},
author = {Tao Feng and Kaifan Ji and Ang Bian and Chang Liu and Jianzhou Zhang},
keywords = {Graph representation learning, Graph embedding, Pre-trained model, Player identification},
abstract = {The person representation problem is a critical bottleneck in the player identification task. However, the current approaches for player identification utilizing the entire image features only are not sufficient to preserve identities due to the reliance on visible visual representations. In this paper, we propose a novel player representation method using a graph-powered pose representation to resolve this bottleneck problem. Our framework consists of three modules: (i.) a novel pose-guided representation module that is able to capture the pose changes dynamically and their associated effects; (ii.) a pose-guided graph embedding module using both the image deep features and the pose structure information for a better player representation inference; (iii.) an identification module as a player classifier. Experiment results on the real-world sport game scenarios demonstrate that our method achieves state-of-the-art identification performance, together with a better player representation.}
}
@article{CHEN2022108491,
title = {Relevance attack on detectors},
journal = {Pattern Recognition},
volume = {124},
pages = {108491},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108491},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006671},
author = {Sizhe Chen and Fan He and Xiaolin Huang and Kun Zhang},
keywords = {Adversarial attack, Attack transferability, Black-box attack, Relevance map, Interpreters, Object detection},
abstract = {This paper focuses on high-transferable adversarial attacks on detectors, which are hard to attack in a black-box manner, because of their multiple-output characteristics and the diversity across architectures. To pursue a high attack transferability, one plausible way is to find a common property across detectors, which facilitates the discovery of common weaknesses. We are the first to suggest that the relevance map from interpreters for detectors is such a property. Based on it, we design a Relevance Attack on Detectors (RAD), which achieves a state-of-the-art transferability, exceeding existing results by above 20%. On MS COCO, the detection mAPs for all 8 black-box architectures are more than halved and the segmentation mAPs are also significantly influenced. Given the great transferability of RAD, we generate the first adversarial dataset for object detection and instance segmentation, i.e., Adversarial Objects in COntext (AOCO), which helps to quickly evaluate and improve the robustness of detectors.}
}
@article{YANG2022108439,
title = {3D pose estimation and future motion prediction from 2D images},
journal = {Pattern Recognition},
volume = {124},
pages = {108439},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108439},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006154},
author = {Ji Yang and Youdong Ma and Xinxin Zuo and Sen Wang and Minglun Gong and Li Cheng},
keywords = {Pose estimation, Motion prediction, Multitask learning},
abstract = {This paper considers to jointly tackle the highly correlated tasks of estimating 3D human body poses and predicting future 3D motions from RGB image sequences. Based on Lie algebra pose representation, a novel self-projection mechanism is proposed that naturally preserves human motion kinematics. This is further facilitated by a sequence-to-sequence multi-task architecture based on an encoder-decoder topology, which enables us to tap into the common ground shared by both tasks. Finally, a global refinement module is proposed to boost the performance of our framework. The effectiveness of our approach, called PoseMoNet, is demonstrated by ablation tests and empirical evaluations on Human3.6M and HumanEva-I benchmark, where competitive performance is obtained comparing to the state-of-the-arts.}
}
@article{ZHANG2022108463,
title = {A new framework of designing iterative techniques for image deblurring},
journal = {Pattern Recognition},
volume = {124},
pages = {108463},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108463},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006397},
author = {Min Zhang and Geoffrey S. Young and Yanmei Tie and Xianfeng Gu and Xiaoyin Xu},
keywords = {Continuous forward model update, GMRES, Inverse problem, Image deblurring, Iterative algorithms, Landweber method, Least square method, Van Cittert method},
abstract = {In this work we present a framework of designing iterative techniques for image deblurring in inverse problem. The new framework is based on two observations about existing methods. We used Landweber method as the basis to develop and present the new framework but note that the framework is applicable to other iterative techniques. First, we observed that the iterative steps of Landweber method consist of a constant term, which is a low-pass filtered version of the already blurry observation. We proposed a modification to use the observed image directly. Second, we observed that Landweber method uses an estimate of the true image as the starting point. This estimate, however, does not get updated over iterations. We proposed a modification that updates this estimate as the iterative process progresses. We integrated the two modifications into one framework of iteratively deblurring images. Finally, we tested the new method and compared its performance with several existing techniques, including Landweber method, Van Cittert method, GMRES (generalized minimal residual method), and LSQR (least square), to demonstrate its superior performance in image deblurring.}
}
@article{SHEN2022108451,
title = {Exploiting appearance transfer and multi-scale context for efficient person image generation},
journal = {Pattern Recognition},
volume = {124},
pages = {108451},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108451},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006270},
author = {Chengkang Shen and Peiyan Wang and Wei Tang},
keywords = {Person image generation, Appearance transfer, Multi-scale context, Efficient image generation},
abstract = {Pose guided person image generation means to generate a photo-realistic person image conditioned on an input person image and a desired pose. This task requires spatial manipulation of the source image according to the target pose. However, convolutional neural networks (CNNs) are inherently limited to geometric transformations due to the fixed geometric structures in their building modules, i.e., convolution, pooling and unpooling, which cannot handle large motion and occlusions caused by large pose transform. This paper introduces a novel two-stream context-aware appearance transfer network to address these challenges. It is a three-stage architecture consisting of a source stream and a target stream. Each stage features an appearance transfer module, a multi-scale context module and two-stream feature fusion modules. The appearance transfer module handles large motion by finding the dense correspondence between the two-stream feature maps and then transferring the appearance information from the source stream to the target stream. The multi-scale context module handles occlusion via contextual modeling, which is achieved by atrous convolutions of different sampling rates. Both quantitative and qualitative results indicate the proposed network can effectively handle challenging cases of large pose transform while retaining the appearance details. Compared with state-of-the-art approaches, it achieves comparable or superior performance using much fewer parameters while being significantly faster.}
}
@article{LI2022108506,
title = {Joint image denoising with gradient direction and edge-preserving regularization},
journal = {Pattern Recognition},
volume = {125},
pages = {108506},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108506},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006828},
author = {Pengliang Li and Junli Liang and Miaohua Zhang and Wen Fan and Guoyang Yu},
keywords = {Joint image denoising, Gradient direction, Majorization minimization, Nonlinear optimization, Nonconvex optimization},
abstract = {Joint image denoising algorithms use the structures of the guidance image as a prior to restore the noisy target image. While the provided guidance images are helpful to improve the denoising performance, the denoised edges are most likely to be blurred especially when the edges of the guidance image are weak or inexistent. To address this weakness, this paper proposes a new gradient-direction-based joint image denoising method in which the absolute cosine value of the angle between two gradient vectors of the guidance image and those of the image to recover is employed as the parallel measurement to ensure that the gradient directions of the denoised image are approximately the same as or opposite to those of the guidance image. Besides, a new edge-preserving regularization term is developed to alleviate the effects of the unreliable prior information from guidance image. To simplify the resultant complex nonconvex and nonlinear fractional model, the logarithm function is employed to convert the multiplication operation into addition operation. Then, we construct the surrogate function for the logarithmic term of l2-norm, and separate the variables to transform the objective function into convex one with high numerical stability while retaining high efficiency. Finally, the optimal solutions can be obtained by directly minimizing the convex functions. Experimental results on public datasets and from nine benchmark methods consistently demonstrate the effectiveness of the proposed method both visually and quantitatively.}
}
@article{XUE2022108494,
title = {Detection and rectification of arbitrary shaped scene texts by using text keypoints and links},
journal = {Pattern Recognition},
volume = {124},
pages = {108494},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108494},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006701},
author = {Chuhui Xue and Shijian Lu and Steven Hoi},
keywords = {Scene text detection, Scene text recognition, Deep learning, Neural network},
abstract = {Detection and recognition of scene texts of arbitrary shapes remain a grand challenge due to the super-rich text shape variation in text line orientations, lengths, curvatures, etc. This paper presents a mask-guided multi-task network that detects and rectifies scene texts of arbitrary shapes reliably. Three types of keypoints are detected which specify the centre line and so the shape of text instances accurately. In addition, four types of keypoint links are detected of which the horizontal links associate the detected keypoints of each text instance and the vertical links predict a pair of landmark points (for each keypoint) along the upper and lower text boundary, respectively. Scene texts can be located and rectified by linking up the associated landmark points (giving localization polygon boxes) and transforming the polygon boxes via thin plate spline, respectively. Extensive experiments over several public datasets show that the use of text keypoints is tolerant to the variation in text orientations, lengths, and curvatures, and it achieves competitive scene text detection and rectification performance as compared with state-of-the-art methods.}
}
@article{ZHANG2022108518,
title = {Safe incomplete label distribution learning},
journal = {Pattern Recognition},
volume = {125},
pages = {108518},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108518},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006944},
author = {Jing Zhang and Hong Tao and Tingjin Luo and Chenping Hou},
keywords = {Label distribution learning, Safeness, Incomplete supervised learning},
abstract = {Label Distribution Learning (LDL) is a popular scenario for solving label ambiguity problems by learning the relative importance of each label to a particular instance. Nevertheless, the label is often incomplete due to the difficulty in annotating label distribution. In this mixing label case with complete and incomplete labels, it is often expected that the learning method can achieve better performance than the baseline method merely utilizing complete labeled data. However, the usage of incomplete labeled data may degrade the performance in real applications. Therefore, it is vital to design a safe incomplete LDL method, which will not deteriorate the performance when exploiting incomplete labeled data. To tackle this important but rarely studied problem, we propose a Safe Incomplete LDL method (SILDL), which learns a classifier that can prevent incomplete labeled instances from worsening the performance. Concretely, we learn predictions from multiple incomplete supervised learners and design an efficient solving algorithm by formulating it as a convex quadratic program. Theoretically, we prove that SILDL can obtain the maximal performance gain against the best one of the multiple baseline methods with mild conditions. Extensive experimental results validate the safeness of the proposed approach and show improvements in performance.}
}
@article{YAN2022108466,
title = {Deep reinforcement learning with credit assignment for combinatorial optimization},
journal = {Pattern Recognition},
volume = {124},
pages = {108466},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108466},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006427},
author = {Dong Yan and Jiayi Weng and Shiyu Huang and Chongxuan Li and Yichi Zhou and Hang Su and Jun Zhu},
keywords = {Combinatorial optimization, Reinforcement learning, Credit assignment},
abstract = {Recent advances in Deep Reinforcement Learning (DRL) demonstrates the potential for solving Combinatorial Optimization (CO) problems. DRL shows advantages over traditional methods both on scalability and computation efficiency. However, the DRL problems transformed from CO problems usually have a huge state space, and the main challenge of solving them has changed from high computation complexity to high sample complexity. Credit assignment determines the contribution of each internal decision to the final success or failure, and it has been shown to be effective in reducing the sample complexity of the training process. In this paper, we resort to a model-based reinforcement learning method to assign credits for model-free DRL methods. Since heuristic methods plays an important role on state-of-the-art solutions for CO problems, we propose using a model to represent those heuristic knowledge and derive the credit assignment from the model. This model-based credit assignment can facilitate the model-free DRL to perform a more effective exploration, and the data collected by the model-free DRL refines the model continuously as the training progresses. Extensive experiments on various CO problems with different settings show that our framework outperforms previous state-of-the-art methods on performance and training efficiency.}
}
@article{ALPAR2022108426,
title = {Signature barcodes for online verification},
journal = {Pattern Recognition},
volume = {124},
pages = {108426},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108426},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006026},
author = {Orcan Alpar},
keywords = {Barcode, Biometrics, Online signature verification, Scalogram, Frequency, SVM},
abstract = {As a sub-branch of behavioral biometrics, online signature verification systems deal with unique signing characteristics, which could be better differentiated by extraction of habitual singing styles instead of geometric features in case of perfect forgery. Even if the signatures are geometrically identical, speed and frequency components of the signing process might significantly vary. Therefore, a novel framework is introduced as a new signature verification protocol for touchscreen devices using barcodes containing the dominant frequency component of the speed signals. A special interface is designed as signature tracker to extract the displacement data sampled from the signing process. The speed signals are interpolated from the displacement data and the frequency components of the signals are computed by scalograms analysis governed by continuous wavelet transformations (CWT). The signature barcodes are generated as 4-scale scalograms and classified by support vector machines (SVM). Among several compatible wavelets, Gaussian derivative wavelet is selected for generating scalograms and the results of the process are calculated as 2.25% FAR, 2.75% FRR and 2.81%EER for our dataset. The framework is also tested with SVC2004 data that we achieved 0% FAR, 9.33% FRR and 8%EER, also with SUSIG-Visual, SUSIG-Blind, MOBISIG databases and we reached between 1.22%-3.62% average EERs, which are competitive among the relevant results. Given the promising outcomes, the signature barcoding is very reliable method which could be executed by a simple touchscreen interface collecting the barcodes for storing and benchmarking when needed.}
}
@article{SUN2022108502,
title = {Two-stage aware attentional Siamese network for visual tracking},
journal = {Pattern Recognition},
volume = {124},
pages = {108502},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108502},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006786},
author = {Xinglong Sun and Guangliang Han and Lihong Guo and Hang Yang and Xiaotian Wu and Qingqing Li},
keywords = {Visual tracking, Siamese network, Feature learning, Attention network},
abstract = {Siamese networks have achieved great success in visual tracking with the advantages of speed and accuracy. However, how to track an object precisely and robustly still remains challenging. One reason is that multiple types of features are required to achieve good precision and robustness, which are unattainable by a single training phase. Moreover, Siamese networks usually struggle with online adaption problem. In this paper, we present a novel two-stage aware attentional Siamese network for tracking (Ta-ASiam). Concretely, we first propose a position-aware and an appearance-aware training strategy to optimize different layers of Siamese network. By introducing diverse training patterns, two types of required features can be captured simultaneously. Then, following the rule of feature distribution, an effective feature selection module is constructed by combining both channel and spatial attention networks to adapt to rapid appearance changes of the object. Extensive experiments on various latest benchmarks have well demonstrated the effectiveness of our method, which significantly outperforms state-of-the-art trackers.}
}
@article{ZHOU2022108450,
title = {Supervised dimensionality reduction technology of generalized discriminant component analysis and its kernelization forms},
journal = {Pattern Recognition},
volume = {124},
pages = {108450},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108450},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006269},
author = {Ruixu Zhou and Wensheng Gao and Dengwei Ding and Weidong Liu},
keywords = {Dimensionality reduction, Subspace projection, Generalized discriminant component analysis, Pattern recognition},
abstract = {Supervised subspace projection technology is a major method for dimensionality reduction in pattern recognition. At present, most supervised subspace projection algorithms are derived from the multi-dimensional extended version of Fisher linear discriminant analysis (FDA), also known as Multi-dimensional Fisher discriminant analysis (MD-FDA). However, MD-FDA needs to be improved further because the projection vectors in the noise-subspace cannot be sorted and the ill-condition of the within-class scatter matrix may cause severe numerical instabilities. Generalized discriminant component analysis (GDCA), the generalization of MD-FDA, together with its kernelization forms are proposed and correspondingly rigorous mathematical proofs are detailed in this paper. By virtue of 5 validation data sets derived from UCI Machine Learning Repository and our laboratory, the theoretical validity and technical advantages of GDCA as well as its kernelization forms are verified, and the effectiveness of the newly proposed method is demonstrated in comparison with 36 kinds of state-of-the-art dimensionality reduction algorithms.}
}
@article{SI2022108462,
title = {Spatial-driven features based on image dependencies for person re-identification},
journal = {Pattern Recognition},
volume = {124},
pages = {108462},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108462},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006385},
author = {Tongzhen Si and Fazhi He and Haoran Wu and Yansong Duan},
keywords = {Person re-identification, Spatial dependencies, Recurrent neural network, Deep learning},
abstract = {Person re-identification (Re-ID) aims to search for the same pedestrian in different cameras, which is a crucial research direction in pattern recognition. Recent deep learning methods have advanced the development of Re-ID. However, the existing approaches easily result in performance degradation in the case of larger scene data because they do not adequately consider the spatial dependencies of both the inter-image and the intra-image. The paper proposes a novel Spatial-Driven Network (SDN) to learn particularly discriminative features with abundant semantic information from both the inter-image and the intra-image dependencies for person Re-ID. Firstly, we design a global-correlation attention module to capture the inter-image dependencies among a series of different pedestrian images. Secondly, we present a local-correlation attention module to compute the intra-image dependencies from any pair of pixels within each pedestrian image. Furthermore, we propose a specific network integration mechanism, which carefully combines the above two complementary modules to match well the solution of the spatial dependency problem. We implement numerous experiments to assess the proposed SDN on mainstream person Re-ID databases. The results demonstrate that the proposed SDN outperforms most of the state-of-the-art methods in typical key criteria.}
}
@article{BOUTROS2022108473,
title = {Self-restrained triplet loss for accurate masked face recognition},
journal = {Pattern Recognition},
volume = {124},
pages = {108473},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108473},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100649X},
author = {Fadi Boutros and Naser Damer and Florian Kirchbuchner and Arjan Kuijper},
keywords = {COVID-19, Biometric recognition, Identity verification, Masked face recognition},
abstract = {Using the face as a biometric identity trait is motivated by the contactless nature of the capture process and the high accuracy of the recognition algorithms. After the current COVID-19 pandemic, wearing a face mask has been imposed in public places to keep the pandemic under control. However, face occlusion due to wearing a mask presents an emerging challenge for face recognition systems. In this paper, we present a solution to improve masked face recognition performance. Specifically, we propose the Embedding Unmasking Model (EUM) operated on top of existing face recognition models. We also propose a novel loss function, the Self-restrained Triplet (SRT), which enabled the EUM to produce embeddings similar to these of unmasked faces of the same identities. The achieved evaluation results on three face recognition models, two real masked datasets, and two synthetically generated masked face datasets proved that our proposed approach significantly improves the performance in most experimental settings.}
}
@article{SEDGHI2022108454,
title = {Sketches by MoSSaRT: Representative selection from manifolds with gross sparse corruptions},
journal = {Pattern Recognition},
volume = {124},
pages = {108454},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108454},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006300},
author = {Mahlagha Sedghi and Michael Georgiopoulos and George K. Atia},
keywords = {Representative selection, Gross sparse corruption, Manifold learning, Reproducing kernel Hilbert spaces},
abstract = {Conventional sampling techniques fall short of selecting representatives that encode the underlying conformation of non-linear manifolds. The problem is exacerbated if the data is contaminated with gross sparse corruptions. In this paper, we present a data selection approach, dubbed MoSSaRT, which draws robust and descriptive sketches of grossly corrupted manifold structures. Built upon an explicit randomized transformation, we obtain a judiciously designed representation of the data relations, which facilitates a versatile selection approach accounting for robustness to gross corruption, descriptiveness and novelty of the chosen representatives, simultaneously. Our model lends itself to a convex formulation with an efficient parallelizable algorithm, which coupled with our randomized matrix structures gives rise to a highly scalable implementation. Theoretical analysis guarantees probabilistic convergence of the approximate function to the desired objective function and reveals insightful geometrical characterization of the chosen representatives. Finally, MoSSaRT substantially outperforms the state-of-the-art algorithms as demonstrated by experiments conducted on both real and synthetic data.}
}
@article{LI2022108521,
title = {Unsupervised person re-identification with multi-label learning guided self-paced clustering},
journal = {Pattern Recognition},
volume = {125},
pages = {108521},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108521},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000024},
author = {Qing Li and Xiaojiang Peng and Yu Qiao and Qi Hao},
keywords = {MLC, Multi-scale network, Multi-label learning, Self-paced clustering, Unsupervised person Re-ID},
abstract = {Although unsupervised person re-identification (Re-ID) has drawn increasing research attention recently, it remains challenging to learn discriminative features without annotations across disjoint camera views. In this paper, we address the unsupervised person Re-ID with a conceptually novel yet simple framework, termed as Multi-label Learning guided self-paced Clustering (MLC). MLC mainly learns discriminative features with three crucial modules, namely a multi-scale network, a multi-label learning module, and a self-paced clustering module. Specifically, the multi-scale network generates multi-granularity person features in both global and local views. The multi-label learning module leverages a memory feature bank and assigns each image with a multi-label vector based on the similarities between the image and feature bank. After multi-label training for several epochs, the self-paced clustering joins in training and assigns a pseudo label for each image. The benefits of our MLC come from three aspects: i) the multi-scale person features for better similarity measurement, ii) the multi-label assignment based on the whole dataset ensures that every image can be trained, and iii) the self-paced clustering removes some noisy samples for better feature learning. Extensive experiments on three popular large-scale Re-ID benchmarks demonstrate that our MLC outperforms previous state-of-the-art methods and significantly improves the performance of unsupervised person Re-ID.}
}
@article{2022108573,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {125},
pages = {108573},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(22)00054-1},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000541}
}
@article{LIU2022108438,
title = {Human object interaction detection using two-direction spatial enhancement and exclusive object prior},
journal = {Pattern Recognition},
volume = {124},
pages = {108438},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108438},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006142},
author = {Lu Liu and Robby T. Tan},
keywords = {Human-object interaction detection, Two-direction spatial enhancement, Exclusive object prior, Mis-grouped human-object pairs, Non-interactive suppression},
abstract = {Human-Object Interaction (HOI) detection aims to detect visual relations between humans and objects in images. One significant problem of HOI detection is that non-interactive human-object pair can be easily mis-grouped and misclassified as an action, especially when the humans are close and performing similar actions in the scene. To address the mis-grouping problem, we propose a spatial enhancement approach to enforce fine-level spatial constraints in two directions between human body parts and object parts. At inference, we propose a human-object regrouping approach for object-exclusive actions by considering the object-exclusive property of the interactive object, where the target object should not be shared by more than one human. By suppressing non-interactive pairs, our approach can decrease the false positives. Experiments on V-COCO and HICO-DET datasets demonstrate our approach is more robust compared to the existing methods under the presence of multiple humans and objects in the scene.}
}
@article{ZHANG2022108505,
title = {Mask encoding: A general instance mask representation for object segmentation},
journal = {Pattern Recognition},
volume = {124},
pages = {108505},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108505},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006816},
author = {Rufeng Zhang and Tao Kong and Xinlong Wang and Mingyu You},
keywords = {Mask encoding, Instance segmentation, Video instance segmentation},
abstract = {Instance segmentation is one of the most challenging tasks in computer vision, which requires separating each instance in pixels. To date, a low-resolution binary mask is the dominant paradigm for representation of instance mask. For example, the size of the predicted mask in Mask R-CNN is usually 28×28. Generally, a low-resolution mask can not capture the object details well, while a high-resolution mask dramatically increases the training complexity. In this work, we propose a flexible and effective approach to encode the high-resolution structured mask to the compact representation which shares the advantages of high-quality and low-complexity. The proposed mask representation can be easily integrated into two-stage pipelines such as Mask R-CNN, improving mask AP by 0.9% on the COCO dataset, 1.4% on the LVIS dataset, and 2.1% on the Cityscapes dataset. Moreover, a novel single shot instance segmentation framework can be constructed by extending the existing one-stage detector with a mask branch for this instance representation. Our model shows its superiority over the explicit contour-based pipelines in accuracy with similar computational complexity. We also evaluate our method for video instance segmentation, achieving promising results on YouTube-VIS dataset. Code is available at: https://git.io/AdelaiDet}
}
@article{WANG2022108517,
title = {Entropy regularization for unsupervised clustering with adaptive neighbors},
journal = {Pattern Recognition},
volume = {125},
pages = {108517},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108517},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006932},
author = {Jingyu Wang and Zhenyu Ma and Feiping Nie and Xuelong Li},
keywords = {Unsupervised clustering, Similarity matrix, Entropy regularization, Trivial similarity distribution, Laplacian rank constraint, Adaptive neighbors},
abstract = {Graph-based clustering has been considered as an effective kind of method in unsupervised manner to partition various items into several groups, such as Spectral Clustering (SC). However, there are three species of drawbacks in SC: (1) The effects of clustering is sensitive to the affinity matrix that is fixed by original data. (2) The input affinity matrix is simply based on distance measurement, which lacks of clear physical meaning under probabilistic prediction. (3) Additional discretization procedures still need to be operated. To cope with these issues, we propose a new clustering model, which refers to Entropy Regularization for unsupervised Clustering with Adaptive Neighbors (ERCAN), to dynamically and simultaneously update affinity matrix and clustering results. Firstly, the maximized entropy regularization term is introduced in probability model to avoid trivial similarity distributions. Additionally, we newly introduce the Laplacian rank constraint with ℓ0-norm to construct adaptive neighbors for sparsity and strength segmentation ability without extra discretization process. Finally, we present a novel monotonic function optimization method, which reveals the consistence between graph sparsity and neighbor assignment, to address the ℓ0-norm constraint in alternative optimization process. Comprehensive experiments show the superiority of our method with promising results.}
}
@article{MA2022108465,
title = {Contrastive attention network with dense field estimation for face completion},
journal = {Pattern Recognition},
volume = {124},
pages = {108465},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108465},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006415},
author = {Xin Ma and Xiaoqiang Zhou and Huaibo Huang and Gengyun Jia and Zhenhua Chai and Xiaolin Wei},
keywords = {Face completion, Unsupervised learning, Attention mechanism, 3D Face analysis},
abstract = {Most modern face completion approaches adopt an autoencoder or its variants to restore missing regions in face images. Encoders are often utilized to learn powerful representations that play an important role in meeting the challenges of sophisticated learning tasks. Specifically, various kinds of masks are often presented in face images in the wild, forming complex patterns, especially in this hard period of COVID-19. It’s difficult for encoders to capture such powerful representations under this complex situation. To address this challenge, we propose a self-supervised Siamese inference network to improve the generalization and robustness of encoders. It can encode contextual semantics from full-resolution images and obtain more discriminative representations. To deal with geometric variations of face images, a dense correspondence field is integrated into the network. We further propose a multi-scale decoder with a novel dual attention fusion module (DAF), which can combine the restored and known regions in an adaptive manner. This multi-scale architecture is beneficial for the decoder to utilize discriminative representations learned from encoders into images. Extensive experiments clearly demonstrate that the proposed approach not only achieves more appealing results compared with state-of-the-art methods but also improves the performance of masked face recognition dramatically.}
}
@article{LI2022108489,
title = {Guided neighborhood affine subspace embedding for feature matching},
journal = {Pattern Recognition},
volume = {124},
pages = {108489},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108489},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006658},
author = {Zizhuo Li and Yong Ma and Xiaoguang Mei and Jun Huang and Jiayi Ma},
keywords = {Feature matching, Image correspondence, Neighborhood affine subspace, Multi-scale, Outlier, Mismatch removal},
abstract = {Feature matching, which refers to determining reliable correspondences between two sets of feature points, is a fundamental component of numerous visual tasks. This paper proposes a novel method, termed as guided neighborhood affine subspace embedding (NASE), to eliminate false matches from the given tentative feature matches. Its essential philosophy is to preserve the underlying intrinsic manifold of potential true matches. Specifically, we aim to approximate the manifold of an inlier with an affine subspace fitted on its neighbors by imposing a motion-consistency constraint. Considering that the “corresponding manifold” of inliers may be biased by gross outliers, we introduce a density-based seed point selection strategy for neighborhood refinement. Based on the above two strategies, we further formulate the general feature matching problem into a mathematical optimization model and deduce a closed-form solution with linearithmic time complexity (i.e., O(NlogN)) for mismatch removal. Additionally, we devise a multi-scale strategy for neighborhood construction, making our method more robust to various degradations. Extensive experiments on general feature matching, fundamental matrix estimation, and loop closure detection demonstrate the clear superiority of NASE over the state-of-the-arts.}
}
@article{JIANG2022108501,
title = {Super-resolution semantic segmentation with relation calibrating network},
journal = {Pattern Recognition},
volume = {124},
pages = {108501},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108501},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006774},
author = {Jie Jiang and Jing Liu and Jun Fu and Weining Wang and Hanqing Lu},
keywords = {Image semantic segmentation, Super-resolution semantic segmentation, Relation calibrating},
abstract = {To achieve high-resolution segmentation results, typical semantic segmentation models often require high-resolution inputs. However, high-resolution inputs inevitably bring high cost on computation, which limits its application seriously in realistic scenarios. To address the problem, we propose to predict a high-resolution semantic segmentation result with a degraded low-resolution image as input, which is called super-resolution semantic segmentation in this paper. We further propose a Relation Calibrating Network (RCNet) for this task. Specifically, we propose two modules, namely Relation Upsampling Module (RUM) and Feature Calibrating Module (FCM). In RUM, the input feature map generates the relation map of pixels in low-resolution, which is then gradually upsampled to high-resolution. Meanwhile, FCM takes the input feature map and the relation map from RUM as inputs, gradually calibrating the feature. Finally, the last FCM outputs the high-resolution segmentation results. We conduct extensive experiments to verify the effectiveness of our method. Specially, we achieve a comparable segmentation result (from 70.01% to 70.90%) with only 1/4 of the computational cost (from 1107.57 to 255.72 GFLOPs) based on FCN on Cityscapes dataset.}
}
@article{ZHOU2022108449,
title = {Deep collaborative multi-task network: A human decision process inspired model for hierarchical image classification},
journal = {Pattern Recognition},
volume = {124},
pages = {108449},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108449},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006257},
author = {Yu Zhou and Xiaoni Li and Yucan Zhou and Yu Wang and Qinghua Hu and Weiping Wang},
keywords = {Hierarchical image classification, Deep multi-task network, Collaborative learning, Decision uncertainty evaluation},
abstract = {Hierarchical classification is significant for big data, where the original task is divided into several sub-tasks to provide multi-granularity predictions based on a tree-shape label structure. Obviously, these sub-tasks are highly correlated: results of the coarser-grained sub-tasks can reduce the candidates for the fine-grained sub-tasks, while results of the fine-grained sub-tasks provide attributes describing the coarser-grained classes. A human can integrate feedbacks from all the related sub-tasks instead of considering each sub-task independently. Therefore, we propose a deep collaborative multi-task network for hierarchical image classification. Specifically, we first extract the relationship matrix between every two sub-tasks defined by the hierarchical label structure. Then, the information of each sub-task is broadcasted to all the related sub-tasks through the relationship matrix. Finally, to combine this information, a novel fusion function based on the task evaluation and the decision uncertainty is designed. Extensive experimental results demonstrate that our model can achieve state-of-the-art performance.}
}
@article{WEI2022108420,
title = {A cascaded nested network for 3T brain MR image segmentation guided by 7T labeling},
journal = {Pattern Recognition},
volume = {124},
pages = {108420},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108420},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005963},
author = {Jie Wei and Zhengwang Wu and Li Wang and Toan Duc Bui and Liangqiong Qu and Pew-Thian Yap and Yong Xia and Gang Li and Dinggang Shen},
keywords = {Brain segmentation, Cascaded nested network, Deep learning, Magnetic resonance imaging},
abstract = {Accurate segmentation of the brain into gray matter, white matter, and cerebrospinal fluid using magnetic resonance (MR) imaging is critical for visualization and quantification of brain anatomy. Compared to 3T MR images, 7T MR images exhibit higher tissue contrast that is contributive to accurate tissue delineation for training segmentation models. In this paper, we propose a cascaded nested network (CaNes-Net) for segmentation of 3T brain MR images, trained by tissue labels delineated from the corresponding 7T images. We first train a nested network (Nes-Net) for a rough segmentation. The second Nes-Net uses tissue-specific geodesic distance maps as contextual information to refine the segmentation. This process is iterated to build CaNes-Net with a cascade of Nes-Net modules to gradually refine the segmentation. To alleviate the misalignment between 3T and corresponding 7T MR images, we incorporate a correlation coefficient map to allow well-aligned voxels to play a more important role in supervising the training process. We compared CaNes-Net with SPM and FSL tools, as well as four deep learning models on 18 adult subjects and the ADNI dataset. Our results indicate that CaNes-Net reduces segmentation errors caused by the misalignment and improves segmentation accuracy substantially over the competing methods.}
}
@article{JIANG2022108433,
title = {Combining embedding-based and symbol-based methods for entity alignment},
journal = {Pattern Recognition},
volume = {124},
pages = {108433},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108433},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006099},
author = {Tingting Jiang and Chenyang Bu and Yi Zhu and Xindong Wu},
keywords = {Entity alignment, Knowledge graph embedding, String Similarity},
abstract = {The objective of entity alignment is to judge whether entities refer to the same object in the real world. Methods for entity alignment can be grossly divided into two groups: conventional symbol-based entity alignment methods and embedding-based entity alignment methods. Both groups of methods have advantages and disadvantages (which are detailed in Section 1). Therefore, combining the advantages of both methods might be a promising strategy. However, to the best of our knowledge, only the RTEA algorithm that was proposed in our previous conference paper (Proceeding of Pacific Rim International Conference on Artificial Intelligence, pp. 162–175, 2019) utilizes this strategy for entity alignment. This manuscript is an extended version of that conference paper, in which an improved algorithm, namely, ESEA (combining embedding-based and symbol-based methods for entity alignment), is proposed based on the following steps. First, a novel method for combining embedding models with symbol-based models is proposed. Entities with high vector similarities are obtained through a hybrid embedding model, and the final aligned entity pairs are calculated via symbol-based methods. Second, a series of symbol-based methods, instead of only the edit distance method in the original version, are combined with embedding-based methods for relation alignment. Third, we combine symbol-based and embedding-based methods in a more complicated framework with the objective of better exploiting the advantages of both methods. The experimental results on real-world datasets demonstrate that the proposed method outperformed several state-of-the-art embedding-based entity alignment approaches and outperformed our previous RTEA method.}
}
@article{ZHOU2022108425,
title = {MTCNet: Multi-task collaboration network for rotation-invariance face detection},
journal = {Pattern Recognition},
volume = {124},
pages = {108425},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108425},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006014},
author = {Lifang Zhou and Hui Zhao and Jiaxu Leng},
keywords = {Rotation-invariant face detection, Face alignment, Multi-task learning},
abstract = {Detecting rotated faces is a challenging task with images from uncontrolled environments. The use of deep convolutional neural networks have greatly improved detection performance, but these methods still do not fully exploit face structure information. This leaves faces with more extreme rotation angles undetectable. In this paper, we present a novel Multi-Task Collaboration Network (MTCNet) for rotation-invariance face detection that fully uses facial landmarks to improve the detection performance by means of collaboration between face detection and face alignment. Differing from previous methods that predict rotation angles in a single step, MTCNet employs a cascaded architecture with three stages to predict faces with gradually decreasing rotation-in-plane ranges in a coarse-to-fine process. Accurate facial landmarks further facilitate face detection. We also introduce a new training loss by integrating the geometric angle into the penalization process, which is much more reasonable than measuring the differences of training samples roughly. Our approach also explores contextual information to distinguish challenging faces from unconstrained scenarios. Extensive experimental results were conducted to demonstrate the effectiveness of MTCNet on both the multiple orientation and rotation datasets. Empirical studies show that MTCNet achieves results competitive with state-of-the-art face detectors while being time-efficient.}
}
@article{LIU2022108520,
title = {Symmetry-Driven hyper feature GCN for skeleton-based gait recognition},
journal = {Pattern Recognition},
volume = {125},
pages = {108520},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108520},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000012},
author = {Xiaokai Liu and Zhaoyang You and Yuxiang He and Sheng Bi and Jie Wang},
keywords = {Dynamics of skeleton, Gait recognition, Graph convolutional networks, Symmetric interaction pattern, Hyper feature},
abstract = {Gait recognition, as an attractive task in biometrics, remains challenging due to significant intra-class changes of clothing and pose variations across different cameras. Recent approaches mainly focus on silhouette-based gait mode, which is easy to model in Convolutional Neural Networks (CNNs). Compared with silhouettes, the dynamics of skeletons essentially convey more robust information, which is invariant to view and clothing changes. Conventional approaches for modeling skeletons usually rely on hand-crafted features or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we address the skeleton-based gait recognition task with a novel Symmetry-Driven Hyper Feature Graph Convolutional Network (SDHF-GCN), which goes beyond the limitations of previous approaches by automatically learning multiple dynamic patterns and hierarchical semantic features in a unified Graph Convolutional Network (GCN). This model involves three dynamic patterns: natural connection, temporal correlation and symmetric interaction, which enriches the description of dynamic patterns by exploiting symmetry perceptual principles. Furthermore, a hyper feature network is proposed to aggregate the hierarchical semantic features, including dynamic features at the high level, structured features at the intermediate level, and static features at the low level, which complement each other to enhance the discriminative ability. By integrating different patterns in the hierarchical structure, the model is able to generate versatile and discriminative representations, thus improving the recognition rate. On the CASIA-B and OUMVLP-Pose datasets, the proposed SDHF-GCN renders substantial improvements over mainstream methods, especially in the coat-wearing scenario, with superior robustness to covariate factors.}
}
@article{FAN2022108437,
title = {Adaptive region-aware feature enhancement for object detection},
journal = {Pattern Recognition},
volume = {124},
pages = {108437},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108437},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006130},
author = {Zhongjie Fan and Qiong Liu},
keywords = {Object detection, Feature enhancement, Adaptive region-aware FPN, Adaptive region-aware RoI feature fusion},
abstract = {Increasing object detectors reveal the importance of feature representation in improving detection performance. Currently, feature enhancement mainly focuses on Feature Pyramid Network (FPN) as well as Region-of-Interest (RoI) feature fusion in two-stage object detectors. Based on this, we propose Adaptive Region-aware Feature Enhancement method including Adaptive Region-aware FPN (AR-FPN) and Adaptive Region-aware RoI Feature Fusion (AR-RFF) modules. Specifically, AR-FPN aims to capture position-sensitive map for each level to enhance the pixel-wise interest degree and make the differences among levels more distinctive. AR-RFF focuses on obtaining distinguishable RoI features by introducing adaptive region information and eliminating scale inconsistency between the refined and original features. Extensive experiments show that our method acquires 1.7% AP higher at least and strong generalization capability compared to others.}
}
@article{RUIZPARRADO2022108513,
title = {A bibliometric analysis of off-line handwritten document analysis literature (1990–2020)},
journal = {Pattern Recognition},
volume = {125},
pages = {108513},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108513},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006890},
author = {Victoria Ruiz-Parrado and Ruben Heradio and Ernesto Aranda-Escolastico and Ángel Sánchez and José F. Vélez},
keywords = {Automatic document analysis, Off-line handwriting recognition, Writer identification, Signature verification, Bibliometrics, Science mapping},
abstract = {Providing computers with the ability to process handwriting is both important and challenging, since many difficulties (e.g., different writing styles, alphabets, languages, etc.) need to be overcome for addressing a variety of problems (text recognition, signature verification, writer identification, word spotting, etc.). This paper reviews the growing literature on off-line handwritten document analysis over the last thirty years. A sample of 5389 articles is examined using bibliometric techniques. Using bibliometric techniques, this paper identifies (i) the most influential articles in the area, (ii) the most productive authors and their collaboration networks, (iii) the countries and institutions that have led research on the topic, (iv) the journals and conferences that have published most papers, and (v) the most relevant research topics (and their related tasks and methodologies) and their evolution over the years.}
}
@article{2022108532,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {124},
pages = {108532},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(22)00013-9},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000139}
}
@article{YI2022108504,
title = {Weakly-supervised semantic segmentation with superpixel guided local and global consistency},
journal = {Pattern Recognition},
volume = {124},
pages = {108504},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108504},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006804},
author = {Sheng Yi and Huimin Ma and Xiang Wang and Tianyu Hu and Xi Li and Yu Wang},
keywords = {Weakly supervised condition, Semantic segmentation, Pixel-level affinity, Superpixel},
abstract = {Weakly supervised semantic segmentation task aims to learn a segmentation model with only image-level annotations. Existing methods generally refine the initial seeds to obtain pseudo labels for training a fully supervised model. In recent years, some affinity-based methods perform well in this task. However, most of these methods only focus on the localization information from class activation map, while ignoring rule-based appearance information. In this paper, we find that the superpixel guidance is helpful for mining semantic affinities between pixels because pixels belonging to the same superpixel often have the same class label. As such, we propose a Superpixel Guided Weakly Segmentation framework, which alternately learns two modules to fuse superpixel information and localization information. The semantic segmentation results are more consistent with the image’s local and global consistency through our framework. Experiments show that the proposed method achieves state-of-the-art performance, with mIoU at 70.5% on the PASCAL VOC 2012 test set and mIoU at 34.4% on the MS-COCO 2014 val set.}
}
@article{CHEN2022108418,
title = {Sparse attention block: Aggregating contextual information for object detection},
journal = {Pattern Recognition},
volume = {124},
pages = {108418},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108418},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100594X},
author = {Chunlin Chen and Jun Yu and Qiang Ling},
keywords = {Object detection, Self-attention, Convolution neural network},
abstract = {It is well recognized that the contextual information of surrounding objects is beneficial for object detection. Such contextual information can often be obtained from long-range dependencies. This paper proposes a sparse attention block to capture long-range dependencies in an efficient way. Unlike the conventional non-local block, which generates a dense attention map to characterize the dependency between any two positions of the input feature map, our sparse attention block samples the most representative positions for contextual information aggregation. After searching for local peaks in a heat map of the given input feature map, it adaptively selects a sparse set of positions to represent the relationship between query and key elements. With the obtained sparse positions, our sparse attention block can well model long-range dependencies, and greatly improve the object detection performance at the additional cost of <2% GPU memory and computation of the conventional non-local block. This sparse attention block can be easily plugged into various object detection frameworks, such as Faster R-CNN, RetinaNet and Mask R-CNN. Experiments on COCO benchmark confirm that our sparse attention block can boost the detection accuracy with significant gains ranging from 1.4% to 1.9% and negligible overhead of computation and memory usage.}
}
@article{FRITTOLI2022108488,
title = {Deep open-set recognition for silicon wafer production monitoring},
journal = {Pattern Recognition},
volume = {124},
pages = {108488},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108488},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006646},
author = {Luca Frittoli and Diego Carrera and Beatrice Rossi and Pasqualina Fragneto and Giacomo Boracchi},
keywords = {Pattern classification, Open-set recognition, Sparse convolutions, Quality inspection, Wafer monitoring},
abstract = {The chips contained in any electronic device are manufactured over circular silicon wafers, which are monitored by inspection machines at different production stages. Inspection machines detect and locate any defect within the wafer and return a Wafer Defect Map (WDM), i.e., a list of the coordinates where defects lie, which can be considered a huge, sparse, and binary image. In normal conditions, wafers exhibit a small number of randomly distributed defects, while defects grouped in specific patterns might indicate known or novel categories of failures in the production line. Needless to say, a primary concern of semiconductor industries is to identify these patterns and intervene as soon as possible to restore normal production conditions. Here we address WDM monitoring as an open-set recognition problem, where the aim is to classify WDM in known categories and promptly detect novel patterns. In particular, we propose a comprehensive pipeline for wafer monitoring based on a Submanifold Sparse Convolutional Network, a deep architecture designed to process sparse data at an arbitrary resolution, which is trained on the known classes. To detect novelties, we define an outlier detector based on a Gaussian Mixture Model fitted on the latent representation of the classifier. Our experiments on a real dataset of WDMs show that directly processing full-resolution WDMs by Submanifold Sparse Convolutions yields superior classification performance on known classes than traditional Convolutional Neural Networks, which require a preliminary binning to reduce the size of the binary images representing WDMs. Moreover, our solution outperforms state-of-the-art open-set recognition solutions in novelty detection.}
}
@article{XIONG2022108436,
title = {Source data-free domain adaptation for a faster R-CNN},
journal = {Pattern Recognition},
volume = {124},
pages = {108436},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108436},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006129},
author = {Lin Xiong and Mao Ye and Dan Zhang and Yan Gan and Yiguang Liu},
keywords = {Source data-free, Object detection, Domain adaptation, Transfer learning},
abstract = {The existing domain adaptive object detection methods often need to carry a large number of source domain samples for domain adaptation, which is not realistic due to GPU limitations, privacy and physical memory in practical applications. To solve this problem, we propose a source data-free domain adaptive object detection method. Only unlabeled target domain data is used to optimize the source domain model so that it can work better in the target domain. Our method takes Faster R-CNN as baseline. Specifically, we first construct global class prototypes which will be updated in batch iteratively. Then based on the global class prototypes, more accurate pseudo-labels are generated for training the target model. In this way, the source and target domains are also implicitly aligned. Our contributions are 1) a prototype guided domain adaptation method which uses prototypes to mine the semantic category information without accessing the source dataset; 2) a scheme of iteratively updating global class prototype which can handle the class and sample imbalances in the training procedure and 3) a more accurate pseudo-label generation method combining semantic information and image information. On multiple public domain adaptive scenarios, our method achieves the state-of-the-art results in terms of accuracy compared with the Faster R-CNN model and some domain adaptive methods with source datasets.}
}
@article{WANG2022108512,
title = {User-based network embedding for opinion spammer detection},
journal = {Pattern Recognition},
volume = {125},
pages = {108512},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108512},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006889},
author = {Ziyang Wang and Wei Wei and Xian-Ling Mao and Guibing Guo and Pan Zhou and Sheng Jiang},
keywords = {Spam detection, Collective spammer, Network embedding, Signed network},
abstract = {Due to the huge commercial interests behind online reviews, a tremendous amount of spammers manufacture spam reviews for product reputation manipulation. To further enhance the influence of spam reviews, spammers often collaboratively post spam reviews within a short period of time, the activities of whom are called collective opinion spam campaign. The goals and members of the spam campaign activities change frequently, and some spammers also imitate normal purchases to conceal the identity, which makes the spammer detection challenging. In this paper, we propose an unsupervised network embedding-based approach to jointly exploiting different types of relations, e.g., direct common behavior relation, and indirect co-reviewed relation to effectively represent the relevances of users for detecting the collective opinion spammers. The average improvements of our method over the state-of-the-art solutions on dataset AmazonCn and YelpHotel are [14.09%,12.04%] and [16.25%,12.78%] in terms of AP and AUC, respectively.}
}
@article{LI2022108472,
title = {Semi-supervised robust training with generalized perturbed neighborhood},
journal = {Pattern Recognition},
volume = {124},
pages = {108472},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108472},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006488},
author = {Yiming Li and Baoyuan Wu and Yan Feng and Yanbo Fan and Yong Jiang and Zhifeng Li and Shu-Tao Xia},
keywords = {Adversarial Defense, Adversarial Learning, Semi-supervised Learning, AI Security, Deep Learning, Classification},
abstract = {Adversarial examples have been shown to be a severe threat to deep neural networks (DNNs). One of the most effective adversarial defense methods is adversarial training (AT) through minimizing the adversarial risk Radv, which encourages both the benign example x and its adversarially perturbed neighborhoods within the ℓp-ball to be predicted as the ground-truth label. In this paper, we propose a novel defense method, the robust training (RT), by jointly minimizing two separated risks (i.e., Rstand and Rrob), which are with respect to the benign example and its neighborhoods, respectively. The motivation is to explicitly and jointly enhance the accuracy and the adversarial robustness. We prove that Radv is upper-bounded by Rstand+Rrob, which implies that RT has similar effect as AT. Intuitively, minimizing the standard risk enforces the benign example to be correctly predicted, while the robust risk minimization encourages the predictions of the neighbor examples to be consistent with the prediction of the benign example. Besides, since Rrob is independent of the ground-truth label, RT is naturally extended to the semi-supervised mode (i.e., SRT), to further enhance its effectiveness. Moreover, we extend the ℓp-bounded neighborhood to a general case, which covers different types of perturbations, such as the pixel-wise (i.e., x+δ) or the spatial perturbation (i.e., Ax+b). Extensive experiments on benchmark datasets not only verify the superiority of the proposed SRT to state-of-the-art methods for defending pixel-wise or spatial perturbations separately but also demonstrate its robustness to both perturbations simultaneously. Our work may shed the light on the understanding of universal model robustness and the potential of unlabeled samples. The code for reproducing main results is available at https://github.com/THUYimingLi/Semi-supervised_Robust_Training.}
}
@article{LI2022108516,
title = {ADR-MVSNet: A cascade network for 3D point cloud reconstruction with pixel occlusion},
journal = {Pattern Recognition},
volume = {125},
pages = {108516},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108516},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006920},
author = {Ying Li and Zhijie Zhao and Jiahao Fan and Wenyue Li},
keywords = {3D point cloud reconstruction, Multiview stereo, Deep neural network, Cost volume},
abstract = {3D point cloud reconstruction is an urgent task in computer vision for environment perception. Nevertheless, the reconstructed scene is inaccurate and incomplete, because the visibility of pixels is not taken into account by existing methods. In this paper, a cascaded network with a multiple cost volume aggregation module named ADR-MVSNet is proposed. Three improvements are presented in ADR-MVSNet. First, to improve the reconstruction accuracy and reduce the time complexity, an adaptive depth reduction module, which adaptively adjusts the depth range of the pixel through the confidence interval, is proposed. Second, to more accurately estimate the depth of occluded pixels in multiview images, a multiple cost volume aggregation module, in which Gini impurity is introduced to measure the confidence of pixel depth prediction, is proposed. Third, a multiscale photometric consistency filter module is proposed, which considers the information in multiple confidence maps at the same time and filters out outliers accurately to remove pixels with low confidence. Therefore, the accuracy of point cloud reconstruction is improved. The experimental results on the DTU and Tanks and Temple datasets demonstrate that ADR-MVSNet achieves highly accurate and highly complete reconstruction compared with state-of-the-art benchmarks.}
}
@article{PENG2022108464,
title = {Characterizing ordinal network of time series based on complexity-entropy curve},
journal = {Pattern Recognition},
volume = {124},
pages = {108464},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108464},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006403},
author = {Kun Peng and Pengjian Shang},
keywords = {Ordinal network, Signal processing, Symbolic patterns, Tsallis -entropy, Novelty detection},
abstract = {Characterizing signal dynamics with network approaches have attracted significant attention in nonlinear time series analysis. Among these approaches, ordinal networks have received great interest for their simplicity and computational efficiency. But most studies mainly use the topological structure of ordinal network to characterize time series while the underlying information in the transition probabilities remain insufficiently concerned. In this paper, the authors introduce an ordinal network-based complexity-entropy curve to fill this gap. The numerical results show that this curve has a great discriminating power for signals with different dynamics, outperforming the recently proposed global node entropy. In the empirical application on stock indices, these curves distinguish stock market with different market development and further identify the impact of the 2008 global financial crisis on stock market dynamics. In the analysis of geomagnetic activity, these curves detect the dynamical change in Earths magnetic field caused by the geomagnetic storm.}
}
@article{MYGDALIS2022108527,
title = {Hyperspherical class prototypes for adversarial robustness},
journal = {Pattern Recognition},
volume = {125},
pages = {108527},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108527},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000085},
author = {Vasileios Mygdalis and Ioannis Pitas},
keywords = {Adversarial defense, Adversarial robustness, Hypersphere prototype loss, HCP loss},
abstract = {This work addresses the problem of adversarial robustness in deep neural network classification from an optimal class boundary estimation perspective. It is argued that increased model robustness to adversarial attacks can be achieved when the feature learning process is monitored by geometrically-inspired optimization criteria. To this end, we propose to learn hyperspherical class prototypes in the neural feature embedding space, along with training the network parameters. Three concurrent optimization functions for the intermediate hidden layer training data activations are devised, requiring items of the same class to be enclosed by the corresponding class prototype boundaries, to have minimum distance from their class prototype vector (i.e., hypersphere center) and to have maximum distance from the remainder hypersphere centers. Our experiments show that training standard classification model architectures with the proposed objectives, significantly increases their robustness to white-box adversarial attacks, without adverse (if not beneficial) effects to their classification accuracy.}
}
@article{WANG2022108424,
title = {Velocity-to-velocity human motion forecasting},
journal = {Pattern Recognition},
volume = {124},
pages = {108424},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108424},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006002},
author = {Hongsong Wang and Liang Wang and Jiashi Feng and Daquan Zhou},
keywords = {Human motion prediction, Action anticipation},
abstract = {Forecasting human motion from a sequence of human poses is an important problem in the fields of computer vision and robotics. Most previous approaches merely consider learning the temporal dynamics of body joints or joint angles, while neglect derivatives of body joints (i.e., pose velocities) which could reasonably reduce noise impact and improve stability. To exploit the benefits of pose velocities, we propose the velocity-to-velocity learning paradigm for human motion prediction which attempts to directly build the sequence-to-sequence model in the velocity space. Two variant architectures based on recurrent encoder-decoder networks are introduced under this paradigm. Considering human motion as kinematics of rigid bodies, joint angles which denote transformation are the computations of inverse kinematics. Accordingly, a novel loss function in terms of rotation matrices is designed during training for human motion prediction through a rotation matrix transformation (RMT) layer. Finally, we present an effective training algorithm which exploits sequence transformation to improve model generalization. Our approaches substantially outperform state-of-the-art approaches on two large-scale datasets, Human3.6M and CMU Motion Capture, for both short-term prediction and long-term prediction. In particular, our model can competently forecast human-like and meaningful poses up to 1000 milliseconds. The code is available on GitHub: https://github.com/hongsong-wang/RNN_based_human_motion_prediction.}
}
@article{KARTHIK2022108538,
title = {Contour-enhanced attention CNN for CT-based COVID-19 segmentation},
journal = {Pattern Recognition},
volume = {125},
pages = {108538},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108538},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200019X},
author = {R. Karthik and R. Menaka and Hariharan M and Daehan Won},
keywords = {COVID-19, Segmentation, Deep learning, Attention, Decoder, CNN},
abstract = {Accurate detection of COVID-19 is one of the challenging research topics in today's healthcare sector to control the coronavirus pandemic. Automatic data-powered insights for COVID-19 localization from medical imaging modality like chest CT scan tremendously augment clinical care assistance. In this research, a Contour-aware Attention Decoder CNN has been proposed to precisely segment COVID-19 infected tissues in a very effective way. It introduces a novel attention scheme to extract boundary, shape cues from CT contours and leverage these features in refining the infected areas. For every decoded pixel, the attention module harvests contextual information in its spatial neighborhood from the contour feature maps. As a result of incorporating such rich structural details into decoding via dense attention, the CNN is able to capture even intricate morphological details. The decoder is also augmented with a Cross Context Attention Fusion Upsampling to robustly reconstruct deep semantic features back to high-resolution segmentation map. It employs a novel pixel-precise attention model that draws relevant encoder features to aid in effective upsampling. The proposed CNN was evaluated on 3D scans from MosMedData and Jun Ma benchmarked datasets. It achieved state-of-the-art performance with a high dice similarity coefficient of 85.43% and a recall of 88.10%.}
}
@article{FATEMIFAR2022108500,
title = {Developing a generic framework for anomaly detection},
journal = {Pattern Recognition},
volume = {124},
pages = {108500},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108500},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006762},
author = {Soroush Fatemifar and Muhammad Awais and Ali Akbari and Josef Kittler},
keywords = {Anomaly detection, One-class classification, Score normalisation, Face spoofing detection, Convolutional neural network},
abstract = {The fusion of one-class classifiers (OCCs) has been shown to exhibit promising performance in a variety of machine learning applications. The ability to assess the similarity or correlation between the output of various OCCs is an important prerequisite for building of a meaningful OCCs ensemble. However, this aspect of the OCC fusion problem has been mostly ignored so far. In this paper, we propose a new method of constructing a fusion of OCCs with three contributions: (a) As a key contribution, enabling an OCC ensemble design using exclusively non anomalous samples, we propose a novel fitness function to evaluate the competency of OCCs without requiring samples from the anomalous class; (b) As a minor, but impactful contribution, we investigate alternative forms of score normalisation of OCCs, and identify a novel two-sided normalisation method as the best in coping with long tail non anomalous data distributions; (c) In the context of building our proposed OCC fusion system based on the weighted averaging approach, we find that the weights optimised using a particle swarm optimisation algorithm produce the most effective solution. We evaluate the merits of the proposed method on 15 benchmarking datasets from different application domains including medical, anti-spam and face spoofing detection. The comparison of the proposed approach with state-of-the-art methods alongside the statistical analysis confirm the effectiveness of the proposed model.}
}
@article{ZHANG2022108543,
title = {Auto uning of price prediction models for high-frequency trading via reinforcement learning},
journal = {Pattern Recognition},
volume = {125},
pages = {108543},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108543},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000243},
author = {Weipeng Zhang and Ning Zhang and Junchi Yan and Guofu Li and Xiaokang Yang},
keywords = {High-frequency trading, Inverse reinforcement learning, Parameter optimization, Multi-armed bandit},
abstract = {In this paper, we propose an online model optimization algorithm based on reinforcement learning for quantitative trading. The combination of prediction model and trading policy is the most commonly used framework in practical quantitative trading. Integrated with machine learning methods, this framework brings huge profits to quantified companies. In the framework, the prediction model is used to predict future trading price trend, and the trading policy is used to determine the price and number of orders. Even though, the shortcomings of machine learning models are obvious, mainly are, (1) Slow prediction speed. Huge human-craft features and model computing cost much time, which is ten times of pure trading policy without model. (2) Poor generalization. This kind of models can hardly adapt to market data in each period, because market traders will change time to time at micro level, thus the distribution of market data will change. But current model is trained on a long period dataset, it achieves best effect at average, but can not adapt to different market at each period. To address this problem, we propose a novel online model optimization algorithm. A light model library will be constructed. Each light model in this library corresponds to a different market distribution. By devising the appropriate reward function via inverse reinforcement learning algorithm, the algorithm can accurately estimate the profits of each model. Then the model can be selected automatically in real-time trading, so that the trading policies can automatically adapt to changes in trading market, overcoming previous shortcoming of manually updating model and slow prediction speed. Experimental results show that the proposed algorithm achieves state-of-the-art performance on China Commodity Futures Market Data.}
}
@article{GU2022108432,
title = {Loss function search for person re-identification},
journal = {Pattern Recognition},
volume = {124},
pages = {108432},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108432},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006087},
author = {Hongyang Gu and Jianmin Li and Guangyuan Fu and Min Yue and Jun Zhu},
keywords = {Person re-identification, Margin-based softmax loss, Loss function search, AutoML},
abstract = {In recent years, person re-identification, which learns discriminative features for the specific person retrieval problem across non-overlapping cameras, has attracted extensive attention. One of the main challenges in person re-identification with deep neural networks is the design of the loss function, which plays a vital role in improving the discrimination of the learned features. However, most existing models utilize the hand-designed loss functions, which are usually sub-optimal and time-consuming. The search spaces of the two existing AutoML-based methods are either too complicated or too simple to include various forms of loss functions. In order to solve the irrationality of the above search spaces, in this paper, we propose a method of AutoML for loss function search named LFS-ReID for person ReID in the framework of the margin-based softmax loss function. Specifically, we first analyze the margin-based softmax loss function and conclude four key properties. Then we carefully design a sampling distribution based on the non-independent truncated Gaussian distributions to sample the loss function, which conforms to the above four properties. Finally, a method based on reinforcement learning is adopted to optimize the sampling distribution dynamically. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on four commonly used datasets.}
}
@article{ZHANG2022108515,
title = {Collaborative boundary-aware context encoding networks for error map prediction},
journal = {Pattern Recognition},
volume = {125},
pages = {108515},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108515},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006919},
author = {Zhenxi Zhang and Chunna Tian and Xinbo Gao and Jie Li and Zhicheng Jiao and Cui Wang and Zhusi Zhong},
keywords = {Segmentation quality assessment, Error map prediction, Medical image segmentation},
abstract = {Accurately assessing the medical image segmentation quality of the automatically generated predictions is essential for guaranteeing the reliability of the results of computer-assisted diagnosis (CAD). Many researchers have studied segmentation quality estimation without labeled ground truths. Recently, a novel idea is proposed, which transforms segmentation quality assessment (SQA) into the pixel-wise or voxel-wise error map segmentation task. However, the simple application of vanilla segmentation structures in medical domain fails to achieve satisfactory error segmentation results. In this paper, we propose collaborative boundary-aware context encoding networks called EP-Net for error segmentation task. Specifically, we propose a collaborative feature transformation branch for better feature fusion between images and masks, and precise localization of error regions. Further, we propose a context encoding module to utilize the global predictor from the error map to enhance the feature representation and regularize the networks. Extensive experiments on IBSR V2.0 dataset, ACDC dataset and M&Ms dataset demonstrate that EP-Net achieves better error segmentation results compared with the traditional segmentation patterns. Based on error prediction results, we obtain a proxy metric of segmentation quality, which has high Pearson correlation coefficient with the real segmentation accuracy on all datasets.}
}
@article{SONG2022108475,
title = {Learning interlaced sparse Sinkhorn matching network for video super-resolution},
journal = {Pattern Recognition},
volume = {124},
pages = {108475},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108475},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006518},
author = {Huihui Song and Yutong Jin and Yong Cheng and Bo Liu and Dong Liu and Qingshan Liu},
keywords = {Video super-resolution, Multi-scale feature, Interlaced sparse sinkhorn attention, Bidirectional fusion, Dynamic reconstruction},
abstract = {How to effectively fuse inter- and intra-frame spatio-temporal information plays a key role in video super-resolution (VSR). Most existing works rely heavily on the accuracy of motion estimation and compensation for spatio-temporal feature alignment. However, they cannot perform well when suffering from large-scale and complex motions. To this end, this paper introduces an efficient and effective Interlaced Sparse Sinkhorn Matching (ISSM) network for VSR, which aligns supporting frames with the reference one in the feature space by learning optimal matching between image regions across frames. Specifically, the ISSM divides the input dense affinity matrix into two sparse block matrixes: one can match long-distance regions while the other can match short-distance regions, and then we leverage an efficient Sinkhorn method on each block to learn optimal matching. Moreover, we insert a residual atrous spatial pyramid pooling module before the ISSM, which can flexibly generate multi-scale features frame by frame to capture the multi-scale context information in images. The aligned features of each adjacent frame are then fed to a bidirectional temporal fusion module to capture the rich temporal information. Finally, the fused features are sent into a frame-wise dynamic reconstruction network to produce an HR frame. Extensive evaluations on three benchmark datasets demonstrate the superiority of our method over the state-of-the-art methods in terms of PSNR and SSIM.}
}
@article{MALDONADO2022108511,
title = {FW-SMOTE: A feature-weighted oversampling approach for imbalanced classification},
journal = {Pattern Recognition},
volume = {124},
pages = {108511},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108511},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006877},
author = {Sebastián Maldonado and Carla Vairetti and Alberto Fernandez and Francisco Herrera},
keywords = {Data resampling, SMOTE, OWA Operators, Feature selection, Imbalanced data classification},
abstract = {The Synthetic Minority Over-sampling Technique (SMOTE) is a well-known resampling strategy that has been successfully used for dealing with the class-imbalance problem, one of the most challenging pattern recognition tasks in the last two decades. In this work, we claim that SMOTE has an important issue when defining the neighborhood in order to create new minority samples: the use of the Euclidean distance may not be suitable in high-dimensional settings. Our hypothesis is that the use of a weighted metric that does not assume that all features are equally important could improve performance in the presence of noisy/redundant variables. In this line, we present a novel SMOTE-like method that uses the weighted Minkowski distance for defining the neighborhood for each example of the minority class. This methodology leads to a better definition of the neighborhood since it prioritizes those features that are more relevant for the classification task. A complementary advantage of the proposal is performing feature selection since attributes can be discarded when their corresponding weights are below a given threshold. Our experiments on 42 class-imbalance datasets show the virtues of the proposed SMOTE variant, achieving the best predictive performance when compared with the traditional SMOTE approach and other recent variants on low- and high-dimensional settings, handling issues such as class overlap and hubness adequately without increasing the complexity of the method.}
}
@article{HOU2022108526,
title = {Hypergraph matching via game-theoretic hypergraph clustering},
journal = {Pattern Recognition},
volume = {125},
pages = {108526},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108526},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000073},
author = {Jian Hou and Marcello Pelillo and Huaqiang Yuan},
keywords = {Feature matching, Hypergraph matching, Game-theoretic, Hypergraph clustering},
abstract = {Feature matching is used to build correspondences between features in the model and test images. As the extension of graph matching, hypergraph matching is able to encode rich invariance between feature tuples and improve matching accuracy. Different from many existing algorithms based on maximizing the matching score between correspondences, our approach formulates hypergraph matching as a non-cooperative multi-player game and obtains matches by extracting the evolutionary stable strategies (ESS). While this approach generates a high matching accuracy, the number of matches is usually small and it involves a large computation load to obtain more matches. To solve this problem, we extract multiple ESS clusters instead of one single ESS group, thereby transforming hypergraph matching of features to hypergraph clustering of candidate matches. By extracting an appropriate number of clusters, we increase the number of matches efficiently, and improve the matching accuracy by imposing the one-to-one constraint. In experiments with three real datasets, our algorithm is shown to generate a large number of matches efficiently. It also shows significant advantage in matching accuracy in comparison with some other hypergraph matching algorithms.}
}
@article{MAZZIA2022108487,
title = {Action Transformer: A self-attention model for short-time pose-based human action recognition},
journal = {Pattern Recognition},
volume = {124},
pages = {108487},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108487},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006634},
author = {Vittorio Mazzia and Simone Angarano and Francesco Salvetti and Federico Angelini and Marcello Chiaberge},
keywords = {Human action recognition, Deep learning, Computer vision, Transformer},
abstract = {Deep neural networks based purely on attention have been successful across several domains, relying on minimal architectural priors from the designer. In Human Action Recognition (HAR), attention mechanisms have been primarily adopted on top of standard convolutional or recurrent layers, improving the overall generalization capability. In this work, we introduce Action Transformer (AcT), a simple, fully, self-attentional architecture that consistently outperforms more elaborated networks that mix convolutional, recurrent, and attentive layers. In order to limit computational and energy requests, building on previous human action recognition research, the proposed approach exploits 2D pose representations over small temporal windows, providing a low latency solution for accurate and effective real-time performance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as an attempt to build a formal training and evaluation benchmark for real-time, short-time HAR. The proposed methodology was extensively tested on MPOSE2021 and compared to several state-of-the-art architectures, proving the effectiveness of the AcT model and laying the foundations for future work on HAR.}
}
@article{KIM2022108435,
title = {Discriminative deep attributes for generalized zero-shot learning},
journal = {Pattern Recognition},
volume = {124},
pages = {108435},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108435},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006117},
author = {Hoseong Kim and Jewook Lee and Hyeran Byun},
keywords = {Generalized zero-shot learning, Deep attribute, Discriminative latent attribute},
abstract = {We indirectly predict a class by deriving user-defined (i.e., existing) attributes (UA) from an image in generalized zero-shot learning (GZSL). High-quality attributes are essential for GZSL, but the existing UAs are sometimes not discriminative. We observe that the hidden units at each layer in a convolutional neural network (CNN) contain highly discriminative semantic information across a range of objects, parts, scenes, textures, materials, and color. The semantic information in CNN features is similar to the attributes that can distinguish each class. Motivated by this observation, we employ CNN features like novel class representative semantic data, i.e., deep attribute (DA). Precisely, we propose three objective functions (e.g., compatible, discriminative, and intra-independent) to inject the fundamental properties into the generated DA. We substantially outperform the state-of-the-art approaches on four challenging GZSL datasets, including CUB, FLO, AWA1, and SUN. Furthermore, the existing UA and our proposed DA are complementary and can be combined to enhance performance further.}
}
@article{BAO2022108499,
title = {COVID-MTL: Multitask learning with Shift3D and random-weighted loss for COVID-19 diagnosis and severity assessment},
journal = {Pattern Recognition},
volume = {124},
pages = {108499},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108499},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006750},
author = {Guoqing Bao and Huai Chen and Tongliang Liu and Guanzhong Gong and Yong Yin and Lisheng Wang and Xiuying Wang},
keywords = {COVID-19, Multitask learning, 3D CNNs, Diagnosis, Severity assessment, Deep learning, Computer tomography},
abstract = {There is an urgent need for automated methods to assist accurate and effective assessment of COVID-19. Radiology and nucleic acid test (NAT) are complementary COVID-19 diagnosis methods. In this paper, we present an end-to-end multitask learning (MTL) framework (COVID-MTL) that is capable of automated and simultaneous detection (against both radiology and NAT) and severity assessment of COVID-19. COVID-MTL learns different COVID-19 tasks in parallel through our novel random-weighted loss function, which assigns learning weights under Dirichlet distribution to prevent task dominance; our new 3D real-time augmentation algorithm (Shift3D) introduces space variances for 3D CNN components by shifting low-level feature representations of volumetric inputs in three dimensions; thereby, the MTL framework is able to accelerate convergence and improve joint learning performance compared to single-task models. By only using chest CT scans, COVID-MTL was trained on 930 CT scans and tested on separate 399 cases. COVID-MTL achieved AUCs of 0.939 and 0.846, and accuracies of 90.23% and 79.20% for detection of COVID-19 against radiology and NAT, respectively, which outperformed the state-of-the-art models. Meanwhile, COVID-MTL yielded AUC of 0.800 ± 0.020 and 0.813 ± 0.021 (with transfer learning) for classifying control/suspected, mild/regular, and severe/critically-ill cases. To decipher the recognition mechanism, we also identified high-throughput lung features that were significantly related (P < 0.001) to the positivity and severity of COVID-19.}
}
@article{CAO2022108447,
title = {Multi-complementary and unlabeled learning for arbitrary losses and models},
journal = {Pattern Recognition},
volume = {124},
pages = {108447},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108447},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006233},
author = {Yuzhou Cao and Shuqi Liu and Yitian Xu},
keywords = {Multi-complementary, Unlabeled learning, Empirical risk minimization, Unbiased estimator, Classification},
abstract = {A weakly-supervised learning framework named as complementary-label learning has been proposed recently, where each sample is equipped with a single complementary label that denotes one of the classes the sample does not belong to. However, the existing complementary-label learning methods cannot learn from the easily accessible unlabeled samples and samples with multiple complementary labels, which are more informative. In this paper, to remove these limitations, we propose the novel multi-complementary and unlabeled learning framework that allows unbiased estimation of classification risk from samples with any number of complementary labels and unlabeled samples, for arbitrary loss functions and models. We first give an unbiased estimator of the classification risk from samples with multiple complementary labels, and then further improve the estimator by incorporating unlabeled samples into the risk formulation. The estimation error bounds show that the proposed methods are in the optimal parametric convergence rate. We also propose a risk correction scheme for alleviating over-fitting caused by negative empirical risk. Finally, the experiments on both linear and deep models show the effectiveness of our proposed methods.}
}
@article{ZHANG2022108471,
title = {Rapid construction of 4D high-quality microstructural image for cement hydration using partial information registration},
journal = {Pattern Recognition},
volume = {124},
pages = {108471},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108471},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006476},
author = {Liangliang Zhang and Lin Wang and Bo Yang and Sijie Niu and Yamin Han and Sung-Kwun Oh},
keywords = {Cement hydration, Rapid image construction, Image registration, Particle swarm optimization, Microstructural temporal image sequences},
abstract = {Studying on the microstructural evolution of cement paste during hydration is of considerable significance for understanding its mechanism and designing such material in cement industry. With the use of microtomography and image registration, the four-dimensional (4D) microstructure of cement paste can be captured, thereby assisting material scientists in studying the hydration process in situ. However, as a challenging task, the construction of high-quality 4D microstructural image is remarkably impeded by image size, isotropy, and homogeneity. This paper proposes an image processing framework to construct 4D high quality microstructural image rapidly for cement hydration. This framework improves and accelerates microstructural image registration and enhancement by using bias field correction, temporal intensity calibration and fast image registration. Additionally, a partial information registration method adopting partial information on the spatial and phased scales, is proposed to improve the registration speed and accuracy. Furthermore, a multi-factor multi-layer particle swarm optimization is proposed to improve the optimization in registration. Experimental results indicate that the 4D high quality microstructural image can be constructed rapidly with promising precision.}
}
@article{CHEN2022108431,
title = {An approach to boundary detection for 3D point clouds based on DBSCAN clustering},
journal = {Pattern Recognition},
volume = {124},
pages = {108431},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108431},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006075},
author = {Hui Chen and Man Liang and Wanquan Liu and Weina Wang and Peter Xiaoping Liu},
keywords = {3D point cloud, Plane segmentation, Boundary detection, DBSCAN},
abstract = {This paper introduces a new DBSCAN-based method for boundary detection and plane segmentation for 3D point clouds. The proposed method is based on candidate samples selection in 3D space and plane validity detection via revising the classical DBSCAN clustering algorithm to obtain a valid fitting plane. Technically, a coplanar threshold is designed as an additional clustering condition to group 3D points whose distances to the fitting plane satisfy the constraint of the threshold as one cluster. The threshold value is automatically adjusted to fit the local distribution of samples in the input dataset, which is free of parameter tuning. Planar objects can be detected by the proposed method since a cluster contains only data points belonging to one plane, and the boundaries among different planes can be correctly detected. Experimental evaluations are performed on both synthetic and real point cloud datasets. Results show that the proposed approach is effective for planar segmentation and high-quality segmentation of intersection boundaries.}
}
@article{YANG2022108443,
title = {Poisson kernel: Avoiding self-smoothing in graph convolutional networks},
journal = {Pattern Recognition},
volume = {124},
pages = {108443},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108443},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006191},
author = {Ziqing Yang and Shoudong Han and Jun Zhao},
keywords = {Graph convolutional kernel, Graph convolutional network, Graph neural network, Graph structure, Self-smoothing},
abstract = {Graph convolutional network is now an effective tool to deal with non-Euclidean data, such as social behavior analysis, molecular structure analysis, and skeleton-based action recognition. Graph convolutional kernel is one of the most significant factors in graph convolutional networks to extract nodes’ feature, and some variants of it have achieved highly satisfactory performance theoretically and experimentally. However, there was limited research about how exactly different graph structures influence the performance of these kernels. Some existing methods used an adaptive convolutional kernel to deal with a given graph structure, which still not explore the internal reasons. In this paper, we start from theoretical analysis of the spectral graph and study the properties of existing graph convolutional kernels, revealing the self-smoothing phenomenon and its effect in specific structured graphs. After that, we propose the Poisson kernel that can avoid self-smoothing without training any adaptive kernel. Experimental results demonstrate that our Poisson kernel not only works well on the benchmark datasets where state-of-the-art methods work fine, but also is evidently superior to them in synthetic datasets.}
}
@article{LAN2022108514,
title = {Unsupervised cross-domain person re-identification by instance and distribution alignment},
journal = {Pattern Recognition},
volume = {124},
pages = {108514},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108514},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006907},
author = {Xu Lan and Xiatian Zhu and Shaogang Gong},
keywords = {Unsupervise person re-identification, Domain adaptation},
abstract = {Most existing person re-identification (re-id) methods assume supervised model training on a separate large set of training samples from the target domain. While performing well in the training domain, such trained models are seldom generalisable to a new independent unsupervised target domain without further labelled training data from the target domain. To solve this scalability limitation, we develop a novel Hierarchical Unsupervised Domain Adaptation (HUDA) method. It can transfer labelled information of an existing dataset (a source domain) to an unlabelled target domain for unsupervised person re-id. Specifically, HUDA is designed to model jointly global distribution alignment and local instance alignment in a two-level hierarchy for discovering transferable source knowledge in unsupervised domain adaptation. Crucially, this approach aims to overcome the under-constrained learning problem of existing unsupervised domain adaptation methods. Extensive evaluations show the superiority of HUDA for unsupervised cross-domain person re-id over a wide variety of state-of-the-art methods on four re-id benchmarks: Market-1501, DukeMTMC, MSMT17 and CUHK03.}
}
@article{WANG2022108498,
title = {Uncertainty estimation for stereo matching based on evidential deep learning},
journal = {Pattern Recognition},
volume = {124},
pages = {108498},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108498},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006749},
author = {Chen Wang and Xiang Wang and Jiawei Zhang and Liang Zhang and Xiao Bai and Xin Ning and Jun Zhou and Edwin Hancock},
keywords = {Stereo matching, Uncertainty estimation, Evidential deep learning},
abstract = {Although deep learning-based stereo matching approaches have achieved excellent performance in recent years, it is still a non-trivial task to estimate the uncertainty of the produced disparity map. In this paper, we propose a novel approach to estimate both aleatoric and epistemic uncertainties for stereo matching in an end-to-end way. We introduce an evidential distribution, named Normal Inverse-Gamma (NIG) distribution, whose parameters can be used to calculate the uncertainty. Instead of directly regressed from aggregated features, the uncertainty parameters are predicted for each potential disparity and then averaged via the guidance of matching probability distribution. Furthermore, considering the sparsity of ground truth in real scene datasets, we design two additional losses. The first one tries to enlarge uncertainty on incorrect predictions, so uncertainty becomes more sensitive to erroneous regions. The second one enforces the smoothness of the uncertainty in the regions with smooth disparity. Most stereo matching models, such as PSM-Net, GA-Net, and AA-Net, can be easily integrated with our approach. Experiments on multiple benchmark datasets show that our method improves stereo matching results. We prove that both aleatoric and epistemic uncertainties are well-calibrated with incorrect predictions. Particularly, our method can capture increased epistemic uncertainty on out-of-distribution data, making it effective to prevent a system from potential fatal consequences. Code is available at https://github.com/Dawnstar8411/StereoMatching-Uncertainty.}
}
@article{ZHOU2022108417,
title = {A Tri-Attention fusion guided multi-modal segmentation network},
journal = {Pattern Recognition},
volume = {124},
pages = {108417},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108417},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005938},
author = {Tongxue Zhou and Su Ruan and Pierre Vera and Stéphane Canu},
keywords = {Multi-modality fusion, Correlation, Brain tumor segmentation, Deep learning},
abstract = {In the field of multimodal segmentation, the correlation between different modalities can be considered for improving the segmentation results. Considering the correlation between different MR modalities, in this paper, we propose a multi-modality segmentation network guided by a novel tri-attention fusion. Our network includes N model-independent encoding paths with N image sources, a tri-attention fusion block, a dual-attention fusion block, and a decoding path. The model independent encoding paths can capture modality-specific features from the N modalities. Considering that not all the features extracted from the encoders are useful for segmentation, we propose to use dual attention based fusion to re-weight the features along the modality and space paths, which can suppress less informative features and emphasize the useful ones for each modality at different positions. Since there exists a strong correlation between different modalities, based on the dual attention fusion block, we propose a correlation attention module to form the tri-attention fusion block. In the correlation attention module, a correlation description block is first used to learn the correlation between modalities and then a constraint based on the correlation is used to guide the network to learn the latent correlated features which are more relevant for segmentation. Finally, the obtained fused feature representation is projected by the decoder to obtain the segmentation results. Our experiment results tested on BraTS 2018 dataset for brain tumor segmentation demonstrate the effectiveness of our proposed method.}
}
@article{WEN2022108445,
title = {Hierarchical domain adaptation with local feature patterns},
journal = {Pattern Recognition},
volume = {124},
pages = {108445},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108445},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100621X},
author = {Jun Wen and Junsong Yuan and Qian Zheng and Risheng Liu and Zhefeng Gong and Nenggan Zheng},
keywords = {Domain adaptation, Local feature patterns, Adversarial learning, Hierarchical alignment},
abstract = {Domain adaptation is proposed to generalize learning machines and address performance degradation of models that are trained from one specific source domain but applied to novel target domains. Existing domain adaptation methods focus on transferring holistic features whose discriminability is generally tailored to be source-specific and inferiorly generic to be transferable. As a result, standard domain adaptation on holistic features usually damages feature structures, especially local feature statistics, and deteriorates the learned discriminability. To alleviate this issue, we propose to transfer primitive local feature patterns, whose discriminability are shown to be inherently more sharable, and perform hierarchical feature adaptation. Concretely, we first learn a cluster of domain-shared local feature patterns and partition the feature space into cells. Local features are adaptively aggregated inside each cell to obtain cell features, which are further integrated into holistic features. To achieve fine-grained adaptations, we simultaneously perform alignment on local features, cell features and holistic features, within which process the local and cell features are aligned independently inside each cell to maintain the learned local structures and prevent negative transfer. Experimenting on typical one-to-one unsupervised domain adaptation for both image classification and action recognition tasks, partial domain adaptation, and domain-agnostic adaptation, we show that the proposed method achieves more reliable feature transfer by consistently outperforming state-of-the-art models and the learned domain-invariant features generalize well to novel domains.}
}
@article{XUE2022108474,
title = {Automated search space and search strategy selection for AutoML},
journal = {Pattern Recognition},
volume = {124},
pages = {108474},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108474},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006506},
author = {Chao Xue and Mengting Hu and Xueqi Huang and Chun-Guang Li},
keywords = {AutoML, Search space selection, Combinatorial optimization for AutoML},
abstract = {Existing works on Automated Machine Learning (AutoML) are mainly based on predefined search space. This paper seeks synergetic automation of two ingredients, i.e., search space and search strategies. Specifically, we formulate the automation of search space and search strategies as a combinatorial optimization problem. Our empirical study on many architecture benchmarks shows that identifying the suitable search space exerts more effect than choosing a sophisticated search strategy. Motivated by this, we attempt to leverage a machine learning method to solve the discrete optimization problem, and thus develop a Layered Architecture Search Tree (LArST) approach to synergize these two components. In addition, we use a probe model-based method to extract dataset-wise features, i.e., meta-features, which is able to facilitate the estimation of proper search space and search strategy for a given task. Experimental results show the efficacy of our approach under different search mechanisms and various datasets and hardware platforms.}
}
@article{DING2022108525,
title = {Graph label prediction based on local structure characteristics representation},
journal = {Pattern Recognition},
volume = {125},
pages = {108525},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108525},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000061},
author = {Jingyi Ding and Ruohui Cheng and Jian Song and Xiangrong Zhang and Licheng Jiao and Jianshe Wu},
keywords = {Graph classification, Graph neural network, Betweenness centrality node, Feature fusion, Characteristics representation},
abstract = {A recent study has shown that the real-time anti-noise challenges faced by molecular activity prediction algorithms can be solved by using the part structure features of the molecular graph. However, the sub-structures selected by this method are distributed in a scattered manner such that although they include as many block features as possible, they do not fully consider the connections between these blocks. Therefore, this study was conducted to fully consider the physical interpretation of the betweenness centrality node in the graph, and a sub-structure was obtained by depth-first search (DFS) from this node. This sub-structure not only contains the characteristics of each region but also retains the connections between each region. Then, a cascading multi-layer perception (MLP) model was designed to learn the characteristic representation of the graph from its local structure features. Experiments demonstrated that the performance of our algorithm is superior to that of other algorithms when evaluated on different datasets.}
}
@article{SHI2022108486,
title = {Sparse CapsNet with explicit regularizer},
journal = {Pattern Recognition},
volume = {124},
pages = {108486},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108486},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006622},
author = {Ruiyang Shi and Lingfeng Niu and Ruizhi Zhou},
keywords = {Capsule network, Model compression, Sparse regularization, Proximal gradient descent},
abstract = {Capsule Network (CapsNet) achieves great improvements in recognizing pose and deformation through a novel encoding mode. However, it carries a large number of parameters, leading to the challenge of heavy memory and computational cost. To solve this problem, we propose sparse CapsNet with an explicit regularizer in this paper. To our knowledge, it’s the first work that utilizes sparse optimization to compress CapsNet. Specifically, to reduce unnecessary weight parameters, we first introduce the component-wise absolute value regularizer into the objective function of CapsNet based on zero-means Laplacian prior. Then, to reduce the computational cost and speed up CapsNet, the weight parameters are further grouped by 2D filters and sparsified by 1-norm regularization. To train our model efficiently, a new stochastic proximal gradient algorithm, which has analytical solutions at each iteration, is presented. Extensive numerical experiments on four commonly used datasets validate the effectiveness and efficiency of the proposed method.}
}
@article{FANG2022108434,
title = {Brain tumor segmentation based on the dual-path network of multi-modal MRI images},
journal = {Pattern Recognition},
volume = {124},
pages = {108434},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108434},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006105},
author = {Lingling Fang and Xin Wang},
keywords = {Brain tumor segmentation, Deep learning, Dual-path model, Magnetic resonance imaging, Multi-modal images},
abstract = {Because of the tumor with infiltrative growth, the glioma boundary is usually fused with the brain tissue, which leads to the failure of accurately segmenting the brain tumor structure through single-modal images. The multi-modal ones are relatively complemented to the inherent heterogeneity and external boundary, which provide complementary features and outlines. Besides, it can retain the structural characteristics of brain diseases from multi angles. However, due to the particularity of multi-modal medical image sampling that increases uneven data density and dense structural vascular tumor mitosis, the glioma may have atypical boundary fuzzy and more noise. To solve this problem, in this paper, the dual-path network based on multi-modal feature fusion (MFF-DNet) is proposed. Firstly, the proposed network uses different kernels multiplexing methods to realize the combination of the large-scale perceptual domain and the non-linear mapping features, which effectively enhances the coherence of information flow. Then, the over-lapping frequency and the vanishing gradient phenomenon are reduced by the residual connection and the dense connection, which alleviate the mutual influence of multi-modal channels. Finally, a dual-path model based on the DenseNet network and the feature pyramid networks (FPN) is established to realize the fusion of low-level, middle-level, and high-level features. Besides, it increases the diversification of glioma non-linear structural features and improves the segmentation precision. A large number of ablation experiments show the effectiveness of the proposed model. The precision of the whole brain tumor and the core tumor can reach 0.92 and 0.90, respectively.}
}
@article{FANFANI2022108413,
title = {PRNU registration under scale and rotation transform based on convolutional neural networks},
journal = {Pattern Recognition},
volume = {124},
pages = {108413},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108413},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005896},
author = {Marco Fanfani and Alessandro Piva and Carlo Colombo},
keywords = {Image forensics, PRNU, Deep learning, CNN, Rotation, Scale},
abstract = {Assessing if an image comes from a specific device is fundamental in many application scenarios. The most promising techniques to solve this problem rely on the Photo Response Non Uniformity (PRNU), a unique trace left during image acquisition. A PRNU fingerprint is computed from several images of a given device, then it is compared with the probe residual noise by means of correlation. However, such a comparison requires that PRNUs are synchronized: even small image transformations can spoil this task. Most of the attempts to solve the registration problem rely on time consuming brute-force search, which is prone to missing detections and false positives. In this paper, the problem is addressed from a computer vision perspective, exploiting recent image registration techniques based on deep learning, and focusing on scaling and rotation transformations. Experiments show that the proposed method is both more accurate and faster than state-of-the-art approaches.}
}
@article{SADIQ2022108510,
title = {Attentive occlusion-adaptive deep network for facial landmark detection},
journal = {Pattern Recognition},
volume = {125},
pages = {108510},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108510},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006865},
author = {Muhammad Sadiq and Daming Shi},
keywords = {Facial landmarks detection, Channel-wise attention, Spatial attention, Deep learning, Face alignment},
abstract = {To be very specific in this paper, an Attentive Occlusion-adaptive Deep Network, hereafter referred as AODN, is proposed for facial landmark detection, consisting of the geometry-aware module, attention module, and low-rank learning module. Facial Landmark Detection (FLD) is a fundamental pre-processing step of facial related tasks. Occlusion, extreme pose, different expressions and illumination are the main challenges in facial landmark detection related tasks. Convolutional Neural Network (CNN) based FLD methods have attained significant improvement regarding accurate FLD but, to deal with occlusion is still very challenging even for CNN. It is because; probably occlusion misleads CNN on feature representation learning. If faces are partially occluded, the localization accuracy will drop significantly. The role of attention in the human visual system is vital, and researchers proved its significance for the computer vision problem. Taking advantage of geometric relationships among different facial components and attention, we extended our already established Occlusion-adaptive Deep Network (ODN). We introduced the attention module consisting of Channel-wise Attention (CA) and Spatial Attention (SA) to improve its ability to deal with the occlusion and enhance feature representation ability simultaneously. The occlusion probability assists as adaptive weights of high-level features and minimizes the effect of the occlusion and assist in modelling the occlusion. Ablation studies prove the synergistic effect of each module. The summary of our trifold contribution is as follows: i) we introduced attention mechanism in our already established ODN model, to deal with occlusion more precisely, and get the rich feature representation to achieve better performance. ii) As per our best of knowledge, we are the pioneers to introduce CA and SA for FLD to model occlusion. iii) Our proposed methodology reduces the number of entire network parameters, which effectually decreases training time and cost. So, the proposed model is more suitable for scalable data processing. Experimental results prove the better performance of proposed AODN on challenging benchmark datasets.}
}
@article{CAO2022108446,
title = {Face photo-sketch synthesis via full-scale identity supervision},
journal = {Pattern Recognition},
volume = {124},
pages = {108446},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108446},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006221},
author = {Bing Cao and Nannan Wang and Jie Li and Qinghua Hu and Xinbo Gao},
keywords = {Face photo-sketch synthesis, Identity supervision, Cross-domain translation, Intra-domain adaptation},
abstract = {Face photo-sketch synthesis refers transforming a face image between photo domain and sketch domain. It plays a crucial role in law enforcement and digital entertainment. A great deal of efforts have been devoted on face photo-sketch synthesis. However, limited by the weak identity supervision, existing methods mostly yield indistinct details or great deformation, resulting in poor perceptual appearance or low recognition accuracy. In the past several years, face identification achieved great progress, which represents the face images much more precisely than before. Considering the face image translation is also a type of face image re-representation, we attempt to introduce face recognition models to improve the synthesis performance. First, we applied existing synthesis models to augment the training set. Then, we proposed a full-scale identity supervision method to reduce redundant information introduced by these pseudo samples and take the valid information to enhance the intra-class variations. The proposed framework consists of two sub-networks: cross-domain translation (CT) network and intra-domain adaptation (IA) network. The CT network translates the input image from source domain to latent image of target domain, which overcomes the great gap between two domains with less structural deformation. The IA network adapts the perceptual appearance of latent image to target image by adversarial learning. Experimental results on CUHK Face Sketch Database and CUHK Face Sketch FERET Database demonstrate the proposed method preserved best perceptual appearance and more distinct details with less deformation.}
}
@article{NGUYEN2022108470,
title = {SibNet: Food instance counting and segmentation},
journal = {Pattern Recognition},
volume = {124},
pages = {108470},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108470},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006464},
author = {Huu-Thanh Nguyen and Chong-Wah Ngo and Wing-Kwong Chan},
keywords = {Food counting, Food instance segmentation},
abstract = {Food computing has recently attracted considerable research attention due to its significance for health risk analysis. In the literature, the majority of research efforts are dedicated to food recognition. Relatively few works are conducted for food counting and segmentation, which are essential for portion size estimation. This paper presents a deep neural network, named SibNet, for simultaneous counting and extraction of food instances from an image. The problem is challenging due to varying size and shape of food as well as arbitrary viewing angle of camera, not to mention that food instances often occlude each other. SibNet is novel for proposal of learning seed map to minimize the overlap between instances. The map facilitates counting and can be completed as an instance segmentation map that depicts the arbitrary shape and size of individual instance under occlusion. To this end, a novel sibling relation sub-network is proposed for pixel connectivity analysis. Along with this paper, three new datasets covering Western, Chinese and Japanese food are also constructed for performance evaluation. The three datasets and SibNet source code are publicly available.}
}
@article{LI2022108537,
title = {Phase retrieval from incomplete data via weighted nuclear norm minimization},
journal = {Pattern Recognition},
volume = {125},
pages = {108537},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108537},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000188},
author = {Zhi Li and Ming Yan and Tieyong Zeng and Guixu Zhang},
keywords = {Phase retrieval, Partial magnitudes, Nuclear norm minimization, Impulse noise},
abstract = {Recovering an unknown object from the magnitude of its Fourier transform is a phase retrieval problem. Here, we consider a much difficult case, where those observed intensity values are incomplete and contaminated by both salt-and-pepper and random-valued impulse noise. To take advantage of the low-rank property within the image of the object, we use a regularization term which penalizes high weighted nuclear norm values of image patch groups. For outliers (impulse noise) in the observation, the ℓ1−2 metric is adopted as the data fidelity term. Then we break down the resulting optimization problem into smaller ones, for example, weighted nuclear norm proximal mapping and ℓ1−2 minimization, because the nonconvex and nonsmooth subproblems have available closed-form solutions. The convergence results are also presented, and numerical experiments are provided to demonstrate the superior reconstruction quality of the proposed method.}
}
@article{LIU2022108430,
title = {A Two-Way alignment approach for unsupervised multi-Source domain adaptation},
journal = {Pattern Recognition},
volume = {124},
pages = {108430},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108430},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006063},
author = {Yong-Hui Liu and Chuan-Xian Ren},
keywords = {Domain adaptation, Feature extraction, Category prototype, Adversarial training, Instance weighting},
abstract = {Domain adaptation aims at transferring knowledge from labeled source domain to unlabeled target domain. Current advances primarily concern single source domain and neglect the setting of multiple source domains. Previous unsupervised multi-source domain adaptation (MDA) algorithms only consider domain-level alignment, while neglecting the category-level information among multiple domains and the instance variations inside each domain. This paper introduces a Two-Way alignment framework for MDA (TWMDA), which considers both domain-level and category-level alignments, and addresses the instance variations. We first align the target and multiple sources on the domain-level by an adversarial learning process. To circumvent the drawbacks of adversarial learning, we further reduce the domain gap on the category-level by minimizing the distance between the category prototypes and unlabeled target instances. To address the instance variations, we design an instance weighting strategy for diverse source instances. The effectiveness of TWMDA is demonstrated on three benchmark datasets for image classification.}
}
@article{QIAN2022108524,
title = {BADet: Boundary-Aware 3D Object Detection from Point Clouds},
journal = {Pattern Recognition},
volume = {125},
pages = {108524},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108524},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200005X},
author = {Rui Qian and Xin Lai and Xirong Li},
keywords = {3D object detection, autonomous driving, graph neural network, boundary aware, point clouds},
abstract = {Currently, existing state-of-the-art 3D object detectors are in two-stage paradigm. These methods typically comprise two steps: 1) Utilize a region proposal network to propose a handful of high-quality proposals in a bottom-up fashion. 2) Resize and pool the semantic features from the proposed regions to summarize RoI-wise representations for further refinement. Note that these RoI-wise representations in step 2) are considered individually as uncorrelated entries when fed to following detection headers. Nevertheless, we observe these proposals generated by step 1) offset from ground truth somehow, emerging in local neighborhood densely with an underlying probability. Challenges arise in the case where a proposal largely forsakes its boundary information due to coordinate offset while existing networks lack corresponding information compensation mechanism. In this paper, we propose BADet for 3D object detection from point clouds. Specifically, instead of refining each proposal independently as previous works do, we represent each proposal as a node for graph construction within a given cut-off threshold, associating proposals in the form of local neighborhood graph, with boundary correlations of an object being explicitly exploited. Besides, we devise a lightweight Region Feature Aggregation Module to fully exploit voxel-wise, pixel-wise, and point-wise features with expanding receptive fields for more informative RoI-wise representations. We validate BADet both on widely used KITTI Dataset and highly challenging nuScenes Dataset. As of Apr. 17th, 2021, our BADet achieves on par performance on KITTI 3D detection leaderboard and ranks 1st on Moderate difficulty of Car category on KITTI BEV detection leaderboard. The source code is available at https://github.com/rui-qian/BADet.}
}
@article{LI2022108400,
title = {CR-GAN: Automatic craniofacial reconstruction for personal identification},
journal = {Pattern Recognition},
volume = {124},
pages = {108400},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108400},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005768},
author = {Yuan Li and Jian Wang and Weibo Liang and Hui Xue and Zhenan He and Jiancheng Lv and Lin Zhang},
keywords = {Craniofacial reconstruction, CT scans, Deep learning},
abstract = {Craniofacial reconstruction is applied to identify human remains in the absence of determination data (e.g., fingerprinting, dental records, radiological materials, or DNA), by predicting the likeness of the unidentified remains based on the internal relationship between the skull and face. Conventional 3D methods are usually based on statistical models with poor capacity, which limit the description of such complex relationship. Moreover, the required high-quality data are difficult to collect. In this study, we present a novel craniofacial reconstruction paradigm that synthesize craniofacial images from 2D computed tomography scan of skull data. The key idea is to recast craniofacial reconstruction as an image translation task, with the goal of generating corresponding craniofacial images from 2D skull images. To this end, we design an automatic skull-to-face transformation system based on deep generative adversarial nets. The system was trained on 4551 paired skull-face images obtained from 1780 CT head scans of the Han Chinese population. To the best of our knowledge, this is the only database of this magnitude in the literature. Finally, to accurately evaluate the performance of the model, a face recognition task employing five existing deep learning algorithms, —FaceNet, —SphereFace, —CosFace, —ArcFace, and —MagFace, was tested on 102 reconstruction cases in a face pool composed of 1744 CT-scan face images. The experimental results demonstrate that the proposed method can be used as an effective forensic tool.}
}
@article{PANG2022108497,
title = {Progressive polarization based reflection removal via realistic training data generation},
journal = {Pattern Recognition},
volume = {124},
pages = {108497},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108497},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006737},
author = {Youxin Pang and Mengke Yuan and Qiang Fu and Peiran Ren and Dong-Ming Yan},
keywords = {Deep learning, Reflection removal, Polarization, Progressive network, Convolutional neural networks},
abstract = {The reflection effect is unavoidable when taking photos through glasses or other transparent materials, which introduces undesired information into pictures. Hence, removing the influence of reflection becomes a key problem in computer vision. One of the main obstacles of recent learning based approaches is the lacking of realistic training data. To address this issue, we introduce a new dataset synthesis method as well as a novel neural network architecture for single image reflection removal. First, we make use of the polarization characteristics of light into the synthesis of datasets, so as to obtain more realistic and diversified training dataset POL. Then, we design a novel Progressive Polarization based Reflection Removal Network (P2R2Net), which preliminary estimates the coarse background layer to guide the final reflection removal. We demonstrate that our method performs better than the state-of-the-art single image reflection removal methods through quantitative and qualitative experimental comparisons. Specifically, the average PSNR of our restored images selected from three representative benchmark datesets: “Real20”, “SIR2” and “Nature” is improved at least 0.49 compared with existing methods and reaches to 24.52.}
}
@article{ZHANG2022108428,
title = {Weighted clustering ensemble: A review},
journal = {Pattern Recognition},
volume = {124},
pages = {108428},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108428},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100604X},
author = {Mimi Zhang},
keywords = {Ensemble selection, Fuzzy clustering, Labeling correspondence, Multi-view data, Temporal data},
abstract = {Clustering ensemble, or consensus clustering, has emerged as a powerful tool for improving both the robustness and the stability of results from individual clustering methods. Weighted clustering ensemble arises naturally from clustering ensemble. One of the arguments for weighted clustering ensemble is that elements (clusterings or clusters) in a clustering ensemble are of different quality, or that objects or features are of varying significance. However, it is not possible to directly apply the weighting mechanisms from classification (supervised) domain to clustering (unsupervised) domain, also because clustering is inherently an ill-posed problem. This paper provides an overview of weighted clustering ensemble by discussing different types of weights, major approaches to determining weight values, and applications of weighted clustering ensemble to complex data. The unifying framework presented in this paper will help clustering practitioners select the most appropriate weighting mechanisms for their own problems.}
}
@article{SHAH2022108509,
title = {Exploring semantic segmentation of related subclasses from a superset of classes},
journal = {Pattern Recognition},
volume = {124},
pages = {108509},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108509},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006853},
author = {Kunjal Shah and Gururaj Bhat},
keywords = {Image segmentation, Stuff classes, Deeplab},
abstract = {Image segmentation is a very important topic in the field of computer vision. We present a method for semantic segmentation of selected stuff classes from a superset of classes. We show that in situations where only select stuff classes are required if we group them as per a strategy then it can attain much higher accuracy than the models trained on the original dataset with all classes intact. The COCO-Stuff Dataset is used for demonstrating the aforesaid strategy. For training purposes, the DeepLabv3+ with Mobilenet-v2 architecture is used. We have achieved an 80.2 percent mean Intersection over Union (mIoU) on these selected classes. We also refine the masks using Learning/Computer Vision (CV) methods and hence obtain better visualization results as compared to the existing DeepLabv3+ results.}
}
@article{ZHANG2022108469,
title = {A zero-shot learning framework via cluster-prototype matching},
journal = {Pattern Recognition},
volume = {124},
pages = {108469},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108469},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006452},
author = {Jing Zhang and Qingyong Li and YangLi-ao Geng and Wen Wang and Wenju Sun and Chuan Shi and Zhengming Ding},
keywords = {Zero-shot learning, Image classification, Cluster-prototype matching, Domain shift},
abstract = {Given the descriptions of classes, Zero-Shot Learning (ZSL) aims to recognize unseen samples by learning a projection between the visual features of samples and the semantic descriptions (prototypes) of classes from seen data. However, due to the inherent distribution gap between seen and unseen domains, the learned projection is generally biased to seen classes and may produce misleading relationships between unseen samples and prototypes (sample-prototype relationship). To tackle this problem, we propose a Cluster-Prototype Matching (CPM) framework which exploits the distribution information of samples to explore the cluster structure of samples and then use the robust cluster-prototype relationship to correct the biased sample-prototype relationship. Specifically, we first use an iterative cluster generation module to identify the underlying cluster structure of samples based on their embedding features, which are acquired via a basic ZSL model. Then each identified cluster will be matched with a specific class prototype through the Kuhn-Munkres algorithm, based on which we can export a sharp cluster-prototype similarity. Finally, the cluster-prototype similarity is combined with the sample-prototype similarity to determine the class labels of test samples. We apply CPM to five well-established ZSL methods and the experimental results show that CPM can significantly improve the performance of basic models and enable them achieve or beyond the state-of-the-art.}
}
@article{MOHAMMED2022108493,
title = {An analysis of heuristic metrics for classifier ensemble pruning based on ordered aggregation},
journal = {Pattern Recognition},
volume = {124},
pages = {108493},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108493},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006695},
author = {Amgad M. Mohammed and Enrique Onieva and Michał Woźniak and Gonzalo Martínez-Muñoz},
keywords = {Heuristic optimization, Ensemble selection, Ensemble pruning, Classifier ensemble, Machine learning, Difficult samples, Ordering-based pruning, Classifier complementariness},
abstract = {Classifier ensemble pruning is a strategy through which a subensemble can be identified via optimizing a predefined performance criterion. Choosing the optimum or suboptimum subensemble decreases the initial ensemble size and increases its predictive performance. In this article, a set of heuristic metrics will be analyzed to guide the pruning process. The analyzed metrics are based on modifying the order of the classifiers in the bagging algorithm, with selecting the first set in the queue. Some of these criteria include general accuracy, the complementarity of decisions, ensemble diversity, the margin of samples, minimum redundancy, discriminant classifiers, and margin hybrid diversity. The efficacy of those metrics is affected by the original ensemble size, the required subensemble size, the kind of individual classifiers, and the number of classes. While the efficiency is measured in terms of the computational cost and the memory space requirements. The performance of those metrics is assessed over fifteen binary and fifteen multiclass benchmark classification tasks, respectively. In addition, the behavior of those metrics against randomness is measured in terms of the distribution of their accuracy around the median. Results show that ordered aggregation is an efficient strategy to generate subensembles that improve both predictive performance as well as computational and memory complexities of the whole bagging ensemble.}
}
@article{HEWAMALAGE2022108441,
title = {Global models for time series forecasting: A Simulation study},
journal = {Pattern Recognition},
volume = {124},
pages = {108441},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108441},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006178},
author = {Hansika Hewamalage and Christoph Bergmeir and Kasun Bandara},
keywords = {Time series forecasting, Global forecasting models, Time series simulation, Data generating processes},
abstract = {The recent advances in Big Data have opened up the opportunity to develop competitive Global Forecasting Models (GFM) that simultaneously learn from many time series. Although, the concept of series relatedness has been heavily exploited with GFMs to explain their superiority over local statistical benchmarks, this concept remains largely under-investigated in an empirical setting. Hence, this study attempts to explore the factors that affect GFM performance, by simulating a number of datasets having controllable characteristics. The factors being controlled are along the homogeneity/heterogeneity of series, the complexity of patterns in the series, the complexity of forecasting models, and the lengths/number of series. We simulate time series from simple Data Generating Processes (DGP), such as Auto Regressive (AR), Seasonal AR and Fourier Terms to complex DGPs, such as Chaotic Logistic Map, Self-Exciting Threshold Auto-Regressive and Mackey-Glass Equations. We perform experiments on these datasets using Recurrent Neural Networks (RNN), Feed-Forward Neural Networks, Pooled Regression models and Light Gradient Boosting Models (LGBM) built as GFMs, and compare their performance against standard statistical forecasting techniques. Our experiments demonstrate that with respect to GFM performance, relatedness is closely associated with other factors such as the availability of data, complexity of data and the complexity of the forecasting technique used. Also, techniques such as RNNs and LGBMs having complex non-linear modelling capabilities, when built as GFMs are competitive methods under challenging forecasting scenarios such as short series, heterogeneous series and having minimal prior knowledge of the data patterns.}
}
@article{SHI2022108429,
title = {Self-weighting multi-view spectral clustering based on nuclear norm},
journal = {Pattern Recognition},
volume = {124},
pages = {108429},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108429},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006051},
author = {Shaojun Shi and Feiping Nie and Rong Wang and Xuelong Li},
keywords = {Unsupervised learning, Multi-view clustering, Nuclear norm, Self-weighting},
abstract = {Multi-view clustering attracts more and more attention due to the fact that it can utilize the complementary and compatible information from multi-view data sets. In many graph-based multi-view clustering approaches, the graph quality is important since it influences the following clustering performance. Therefore, learning a high quality similarity graph is desired. In this paper, we propose a novel clustering method which is named as Self-weighting Multi-view Spectral Clustering based on Nuclear Norm (SMSC_NN). Specifically, to fully utilize the multiple view features, the common consensus representation is learned. Moreover, to capture the principal components from various view features, the nuclear norm is introduced which can make the view-specific information be well explored. Further, due to the fact that each view feature denotes a sort of specific property, the adaptive weights are assigned instead of equal view weights. In order to verify the effectiveness of the proposed method, four multi-view data sets are used to conduct the clustering experiments. Extensive experimental results demonstrate the superiority of the proposed method comparing with state-of-the-art multi-view clustering approaches. In addition, the proposed approach is experimented on the Cal101-20 data set with ”salt and pepper” noises, and experimental results verify that the proposed SMSC_NN method can remain robust to noises.}
}
@article{LI2022108453,
title = {GaitSlice: A gait recognition model based on spatio-temporal slice features},
journal = {Pattern Recognition},
volume = {124},
pages = {108453},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108453},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006294},
author = {Huakang Li and Yidan Qiu and Huimin Zhao and Jin Zhan and Rongjun Chen and Tuanjie Wei and Zhihui Huang},
keywords = {Gait recognition, Key frame, Cross-view, Attention mechanism, Slice feature, GaitSlice},
abstract = {Improving the performance of gait recognition under multiple camera views (i.e., cross-view gait recognition) and various conditions is urgent. From observation, we find that adjacent body parts are inter-related while walking, and each frame in a gait sequence possesses different degrees of semantic information. In this paper, we propose a novel model, GaitSlice, to analyze the human gait based on spatio-temporal slice features. Spatially, we design Slice Extraction Device (SED) to form top-down inter-related slice features. Temporally, we introduce Residual Frame Attention Mechanism (RFAM) to acquire and highlight the key frames. To better simulate reality, GaitSlice combines parallel RFAMs with inter-related slice features to focus on the features’ spatio-temporal information. We evaluate our model on CASIA-B and OU-MVLP gait datasets and compare it with six typical gait recognition models by using rank-1 accuracy. The results show that GaitSlice achieves high accuracy in gait recognition under cross-view and various walking conditions.}
}
@article{YU2022108540,
title = {VD-PCR: Improving visual dialog with pronoun coreference resolution},
journal = {Pattern Recognition},
volume = {125},
pages = {108540},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108540},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000218},
author = {Xintong Yu and Hongming Zhang and Ruixin Hong and Yangqiu Song and Changshui Zhang},
keywords = {Vision and language, Visual dialog, Pronoun coreference resolution},
abstract = {The visual dialog task requires an AI agent to interact with humans in multi-round dialogs based on a visual environment. As a common linguistic phenomenon, pronouns are often used in dialogs to improve the communication efficiency. As a result, resolving pronouns (i.e., grounding pronouns to the noun phrases they refer to) is an essential step towards understanding dialogs. In this paper, we propose VD-PCR, a novel framework to improve Visual Dialog understanding with Pronoun Coreference Resolution in both implicit and explicit ways. First, to implicitly help models understand pronouns, we design novel methods to perform the joint training of the pronoun coreference resolution and visual dialog tasks. Second, after observing that the coreference relationship of pronouns and their referents indicates the relevance between dialog rounds, we propose to explicitly prune the irrelevant history rounds in visual dialog models’ input. With pruned input, the models can focus on relevant dialog history and ignore the distraction in the irrelevant one. With the proposed implicit and explicit methods, VD-PCR achieves state-of-the-art experimental results on the VisDial dataset.}
}
@article{TOCCACELI2022108507,
title = {Introduction to conformal predictors},
journal = {Pattern Recognition},
volume = {124},
pages = {108507},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108507},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100683X},
author = {Paolo Toccaceli},
keywords = {Conformal prediction, Nonparametric methods, Confidence},
abstract = {This paper aims to provide a compact but accessible introduction to Conformal Predictors (CP), a Machine Learning method with the distinguishing property of producing predictions that exhibit a chosen error rate. This property, referred to as validity, is backed by not only asymptotic, but also finite-sample probabilistic guarantees. CPs differ from the conventional approach to prediction in that they introduce hedging in the form of set-valued predictions. The CP validity guarantees do not require assumptions such as priors, but are of broad applicability as they rely solely on exchangeability. The CP framework is universal in the sense that it operates on top of virtually any Machine Learning method. In addition to the formal definition, this introduction discusses CP variants that can be computed efficiently (Inductive or “split” CP) or that are suitable for imbalanced data sets (class-conditional CP). Finally, a short survey of the field provides references for relevant research and highlights the variety of domains in which CPs have found valuable application.}
}
@article{ZHANG2022108415,
title = {DE-GAN: Domain Embedded GAN for High Quality Face Image Inpainting},
journal = {Pattern Recognition},
volume = {124},
pages = {108415},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108415},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005914},
author = {Xian Zhang and Xin Wang and Canghong Shi and Zhe Yan and Xiaojie Li and Bin Kong and Siwei Lyu and Bin Zhu and Jiancheng Lv and Youbing Yin and Qi Song and Xi Wu and Imran Mumtaz},
keywords = {Face Inpainting, Domain Embedding, Adversarial Generative Model},
abstract = {Domain knowledge of face shapes and structures plays an important role in face inpainting. However, general inpainting methods focus mainly on the resolution of generated images without considering the particular structure of human faces and generally produce inharmonious facial parts. Existing face-inpainting methods incorporate only one type of facial feature for face completion, and their results are still undesirable. To improve face inpainting quality, we propose a Domain Embedded Generative Adversarial Network (DE-GAN) for face inpainting. DE-GAN embeds three types of face domain knowledge (i.e., face mask, face part, and landmark image) via a hierarchical variational auto-encoder (HVAE) into a latent variable space to guide face completion. Two adversarial discriminators, a global discriminator and a patch discriminator, are used to judge whether the generated distribution is close to the real distribution or not. Experiments on two public face datasets demonstrate that our proposed method generates higher quality inpainting results with consistent and harmonious facial structures and appearance than existing methods and achieves the state-of-the-art performance, esp. for inpainting under-pose variations.}
}
@article{DELUSSU2022108484,
title = {Scene-specific crowd counting using synthetic training images},
journal = {Pattern Recognition},
volume = {124},
pages = {108484},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108484},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006609},
author = {Rita Delussu and Lorenzo Putzu and Giorgio Fumera},
keywords = {Crowd counting, Scene-specific settings, Synthetic training images},
abstract = {Crowd counting is a computer vision task on which considerable progress has recently been made thanks to convolutional neural networks. However, it remains a challenging task even in scene-specific settings, in real-world application scenarios where no representative images of the target scene are available, not even unlabelled, for training or fine-tuning a crowd counting model. Inspired by previous work in other computer vision tasks, we propose a simple but effective solution for the above application scenario, which consists of automatically building a scene-specific training set of synthetic images. Our solution does not require from end-users any manual annotation effort nor the collection of representative images of the target scene. Extensive experiments on several benchmark data sets show that the proposed solution can improve the effectiveness of existing crowd counting methods.}
}
@article{PARK2022108444,
title = {Robust Gaussian process regression with a bias model},
journal = {Pattern Recognition},
volume = {124},
pages = {108444},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108444},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006208},
author = {Chiwoo Park and David J. Borth and Nicholas S. Wilson and Chad N. Hunter and Fritz J. Friedersdorf},
keywords = {Robust regression, Gaussian process, Random bias estimation, Regularized likelihood maximization, Sensor data},
abstract = {This paper presents a new approach to a robust Gaussian process regression, creating a non-parametric Bayesian regression estimate robust to outliers. Most existing approaches replace an outlier-prone Gaussian likelihood with a non-Gaussian likelihood induced from a heavy tail distribution, such as the Laplace distribution and Student-t distribution. However, the use of a non-Gaussian likelihood would incur the need for a computationally expensive Bayesian approximate computation in the posterior inferences. The proposed approach models an outlier as a noisy and biased observation of an unknown regression function, and accordingly, the likelihood contains bias terms to explain the degree of deviations from the regression function. We introduce two bias models that handle the bias terms differently, treating a bias as an unknown and fixed quantity or treating a bias as a random quantity. We entail how the biases can be estimated accurately with other hyperparameters by a regularized maximum likelihood estimation. Conditioned on the bias estimates, the robust GP regression can be reduced to a standard GP regression problem with analytical forms of the predictive mean and variance estimates. Therefore, the proposed approach is simple and very computationally attractive. It also gives a very robust and accurate GP estimate for many tested scenarios. For the numerical evaluation, we perform a comprehensive simulation study to evaluate the proposed approach with the comparison to the existing robust GP approaches under various simulated scenarios of different outlier proportions and different noise levels. The approach is applied to data from two measurement systems, where the predictors are based on robust environmental parameter measurements and the response variables utilize more complex chemical sensing methods that contain a certain percentage of outliers. The utility of the measurement systems and value of the environmental data are improved through the computationally efficient GP regression and bias model.}
}
@article{PEALAT2022108423,
title = {Improved time series clustering based on new geometric frameworks},
journal = {Pattern Recognition},
volume = {124},
pages = {108423},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108423},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005999},
author = {Clément Péalat and Guillaume Bouleux and Vincent Cheutet},
keywords = {Clustering, Time series, Delayed coordinate embedding, Embedding, Stiefel manifold, UMAP, HDBSCAN},
abstract = {Most existing methods for time series clustering rely on distances calculated from the entire raw data using the Euclidean distance or Dynamic Time Warping distance. In this work, we propose to embed the time series onto higher-dimensional spaces to obtain geometric representations of the time series themselves. Particularly, the embedding on Rn×p, on the Stiefel manifold and on the unit Sphere are analyzed for their performances with respect to several yet well-known clustering algorithms. The gain brought by the geometrical representation for the time series clustering is illustrated through a large benchmark of databases. We particularly exhibit that, firstly, the embedding of the time series on higher dimensional spaces gives better results than classical approaches and, secondly, that the embedding on the Stiefel manifold - in conjunction with UMAP and HDBSCAN clustering algorithms - is the recommended framework for time series clustering.}
}
@article{ZHANG2022108508,
title = {A deformable CNN-based triplet model for fine-grained sketch-based image retrieval},
journal = {Pattern Recognition},
volume = {125},
pages = {108508},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108508},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006841},
author = {Xianlin Zhang and Mengling Shen and Xueming Li and Fangxiang Feng},
keywords = {Freehand sketches, FG-SBIR, Semantic attributes, Deformable CNNs, Preprocessing},
abstract = {With the popularity of electronic touch-screen and pressure sensing devices, fine-grained sketch based image retrieval (FG-SBIR) has become a research hotspot. In this paper, we stress the core problems of FG-SBIR: a. how to reduce the difference between the non-homogenous of heterogeneous media, and b. how to improve the distinguishability of sketch features. Specifically, a sketch generation model is first proposed to replace the conventional pre-processing of roughly extracting image edges, moreover, this model can alleviate the dilemma of sketch data scarcity. We then construct a novel FG-SBIR model which takes advantage of deformable convolutional neural network while taking into consideration of semantic attributes together. In addition, we build a fine-grained clothing sketch-image dataset, which has rich attribute annotations, for the first time. Extensive experiments exhibit that our proposed model achieves a better performance in improving the retrieval accuracy over the state-of-the-art baselines.}
}
@article{DAI2022108397,
title = {Infinite-dimensional feature aggregation via a factorized bilinear model},
journal = {Pattern Recognition},
volume = {124},
pages = {108397},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108397},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005598},
author = {Jindou Dai and Yuwei Wu and Zhi Gao and Yunde Jia},
keywords = {Feature aggregation, Infinite-dimensional features, Non-approximate method, Second-order statistics},
abstract = {Aggregating infinite-dimensional features has demonstrated superiority compared with their finite-dimensional counterparts. However, most existing methods approximate infinite-dimensional features with finite-dimensional representations, which inevitably results in approximation error and inferior performance. In this paper, we propose a non-approximate aggregation method that directly aggregates infinite-dimensional features rather than relying on approximation strategies. Specifically, since infinite-dimensional features are infeasible to store, represent and compute explicitly, we introduce a factorized bilinear model to capture pairwise second-order statistics of infinite-dimensional features as a global descriptor. It enables the resulting aggregation formulation to only involve the inner product in an infinite-dimensional space. The factorized bilinear model is calculated by a Sigmoid kernel to generate informative features containing infinite order statistics. Experiments on four visual tasks including the fine-grained, indoor scene, texture, and material classification, demonstrate that our method consistently achieves the state-of-the-art performance.}
}
@article{ZHOU2022108468,
title = {CANet: Co-attention network for RGB-D semantic segmentation},
journal = {Pattern Recognition},
volume = {124},
pages = {108468},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108468},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006440},
author = {Hao Zhou and Lu Qi and Hai Huang and Xu Yang and Zhaoliang Wan and Xianglong Wen},
keywords = {RGB-D, Multi-modal fusion, Co-attention, Semantic segmentation},
abstract = {Incorporating the depth (D) information to RGB images has proven the effectiveness and robustness in semantic segmentation. However, the fusion between them is not trivial due to their inherent physical meaning discrepancy, in which RGB represents RGB information but D depth information. In this paper, we propose a co-attention network (CANet) to build sound interaction between RGB and depth features. The key part in the CANet is the co-attention fusion part. It includes three modules. Specifically, the position and channel co-attention fusion modules adaptively fuse RGB and depth features in spatial and channel dimensions. An additional fusion co-attention module further integrates the outputs of the position and channel co-attention fusion modules to obtain a more representative feature which is used for the final semantic segmentation. Extensive experiments witness the effectiveness of the CANet in fusing RGB and depth features, achieving state-of-the-art performance on two challenging RGB-D semantic segmentation datasets, i.e., NYUDv2 and SUN-RGBD.}
}
@article{ZHENG2022108492,
title = {Semi-supervised node classification via adaptive graph smoothing networks},
journal = {Pattern Recognition},
volume = {124},
pages = {108492},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108492},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006683},
author = {Ruigang Zheng and Weifu Chen and Guocan Feng},
keywords = {Adaptive graph smoothing networks, Graph convolutional networks, Semi-supervised learning, Graph node classification},
abstract = {Inspections on current graph neural networks suggest us to reconsider the computational aspect of the final aggregation. We consider that such aggregations perform a prediction smoothing and impute their potential drawbacks to be the inter-class interference implied by the underlying graphs. We aim at weakening the inter-class connections so that aggregations focus more on intra-class relations and producing smooth predictions according to weakening results. We apply a metric learning module to learn new edge weights and combine entropy losses to ensure the correspondence between the predictions and the learnt distances so that the weights of inter-class edges are reduced and predictions are smoothed according to the modified graph. Experiments on four citation networks and a Wiki network show that in comparison with other state-of-the-art graph neural networks, the proposed algorithm can improve the classification accuracy.}
}
@article{MA2022108440,
title = {Semantic clustering based deduction learning for image recognition and classification},
journal = {Pattern Recognition},
volume = {124},
pages = {108440},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108440},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006166},
author = {Wenchi Ma and Xuemin Tu and Bo Luo and Guanghui Wang},
keywords = {Deduction learning, Clustering prior, Semantic space, Smooth semantic clustering},
abstract = {The paper proposes a semantic clustering based deduction learning by mimicking the learning and thinking process of human brains. Human beings can make judgments based on experience and cognition, and as a result, no one would recognize an unknown animal as a car. Inspired by this observation, we propose to train deep learning models using the clustering prior that can guide the models to learn with the ability of semantic deducing and summarizing from classification attributes, such as a cat belonging to animals while a car pertaining to vehicles. The proposed approach realizes the high-level clustering in the semantic space, enabling the model to deduce the relations among various classes during the learning process. In addition, the paper introduces a semantic prior based random search for the opposite labels to ensure the smooth distribution of the clustering and the robustness of the classifiers. The proposed approach is supported theoretically and empirically through extensive experiments. We compare the performance across state-of-the-art classifiers on popular benchmarks, and the generalization ability is verified by adding noisy labeling to the datasets. Experimental results demonstrate the superiority of the proposed approach.}
}
@article{HU2022108452,
title = {Deep co-supervision and attention fusion strategy for automatic COVID-19 lung infection segmentation on CT images},
journal = {Pattern Recognition},
volume = {124},
pages = {108452},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108452},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006282},
author = {Haigen Hu and Leizhao Shen and Qiu Guan and Xiaoxin Li and Qianwei Zhou and Su Ruan},
keywords = {Semantic segmentation, Multi-scale features, Attention mechanism, Feature fusion, COVID-19},
abstract = {Due to the irregular shapes,various sizes and indistinguishable boundaries between the normal and infected tissues, it is still a challenging task to accurately segment the infected lesions of COVID-19 on CT images. In this paper, a novel segmentation scheme is proposed for the infections of COVID-19 by enhancing supervised information and fusing multi-scale feature maps of different levels based on the encoder-decoder architecture. To this end, a deep collaborative supervision (Co-supervision) scheme is proposed to guide the network learning the features of edges and semantics. More specifically, an Edge Supervised Module (ESM) is firstly designed to highlight low-level boundary features by incorporating the edge supervised information into the initial stage of down-sampling. Meanwhile, an Auxiliary Semantic Supervised Module (ASSM) is proposed to strengthen high-level semantic information by integrating mask supervised information into the later stage. Then an Attention Fusion Module (AFM) is developed to fuse multiple scale feature maps of different levels by using an attention mechanism to reduce the semantic gaps between high-level and low-level feature maps. Finally, the effectiveness of the proposed scheme is demonstrated on four various COVID-19 CT datasets. The results show that the proposed three modules are all promising. Based on the baseline (ResUnet), using ESM, ASSM, or AFM alone can respectively increase Dice metric by 1.12%, 1.95%,1.63% in our dataset, while the integration by incorporating three models together can rise 3.97%. Compared with the existing approaches in various datasets, the proposed method can obtain better segmentation performance in some main metrics, and can achieve the best generalization and comprehensive performance.}
}