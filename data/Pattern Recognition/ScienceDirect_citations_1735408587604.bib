@article{HIRCHOUA2023109730,
title = {β-Random Walk: Collaborative sampling and weighting mechanisms based on a single parameter for node embeddings},
journal = {Pattern Recognition},
volume = {142},
pages = {109730},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109730},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004284},
author = {Badr Hirchoua and Saloua {El Motaki}},
keywords = {Node embedding, Random walk, Knowledge representation, Link prediction, Knowledge completion, Node behavior},
abstract = {Graph embedding transforms a graph into vector representations to facilitate subsequent graph-analytic tasks. Existing graph embedding methods ignore efficient node sampling and intelligent node weighting, leading to a weak node representation. This paper introduces the β-random walk model with two main contributions. Firstly, the traditional random walk sampling reveals instability. Thus, we associate a parameter β with each node to balance and stabilize the sampling process, producing high-efficient trajectories. Secondly, we design a weighting mechanism that incorporates these trajectories to generate accurate representations. The designed mechanism models the behavior of each node contextually at each episode, considering the current state and the previous weights to produce the next episode’s weights. The parameter β optimizes the node weights by simulating multiple high-order proximity walks from each node. This approach provides summarized insights about each node’s behavior and its neighbors’ context, which enables a consistent discovery of prominent paths variation in the graph. Experimental results demonstrate that the β-random walk outperforms the state-of-the-art baselines in handling small and large graphs.}
}
@article{ZAHRA2023109669,
title = {Person re-identification: A retrospective on domain specific open challenges and future trends},
journal = {Pattern Recognition},
volume = {142},
pages = {109669},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109669},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003709},
author = {Asmat Zahra and Nazia Perwaiz and Muhammad Shahzad and Muhammad Moazam Fraz},
keywords = {Person re-Identification, Literature survey, Deep learning, Open challenges, Specific application-driven},
abstract = {Person Re-Identification (Re-ID) is a critical aspect of visual surveillance systems, which aims to automatically recognize and locate individuals across a multi-camera network with non-overlapping fields-of-view. Despite significant progress in recent years through the use of deep learning-based approaches, there remain many vision-related challenges, such as occlusion, pose, background clutter, misalignment, scale, viewpoint, low resolution & illumination, and cross-domain generalization across camera modalities, that hinder the accurate identification of individuals. The majority of the proposed approaches directly or indirectly aim to solve one or multiple of these existing challenges. To further advance the development of Re-ID solutions, a comprehensive review of the current approaches is necessary. However, no focused review currently exists that analyses and highlights specific aspects for further development. To fill this gap, we present a systematic challenge-specific literature survey of about 300 papers published between 2015 and 2022, which reviews Re-ID approaches from a solution-oriented perspective. This survey is the first of its kind to provide an in-depth analysis of the different approaches used to address the various challenges in Re-ID. Furthermore, our review highlights several prominent and diverse research trends in the Re-ID domain. These trends offer a visionary perspective regarding ongoing person Re-ID research, and they may eventually lead to the development of practical real-world solutions. We highlighted the AI ethics that must be followed while developing a Re-ID solution, and recently being practiced as well. Another exciting future dimension of person Re-ID research is the long-term Re-ID, which is still under evolution. Overall, our survey aims to serve as a valuable resource for researchers and practitioners working in the field of Re-ID and to inspire the development of innovative and effective Re-ID solutions.}
}
@article{CARBONERALUVIZON2023109714,
title = {SSP-Net: Scalable sequential pyramid networks for real-Time 3D human pose regression},
journal = {Pattern Recognition},
volume = {142},
pages = {109714},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109714},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004120},
author = {Diogo {Carbonera Luvizon} and Hedi Tabia and David Picard},
keywords = {3D Human pose estimation, Neural nets, Computer vision},
abstract = {In this paper we propose a highly scalable convolutional neural networks, end-to-end trainable, for real-time 3D human pose regression from still RGB images. We call this approach Scalable Sequential Pyramid Networks (SSP-Net) as it is trained with refined supervision at multiple scales in a sequential manner. Our network requires a single training procedure and is capable of producing its best predictions at 120 frames per second (FPS), or acceptable predictions at more than 200 FPS when cut at test time. We show that the proposed regression approach is invariant to the size of feature maps, allowing our method to perform multi-resolution intermediate supervisions and reaching results comparable to the state-of-the-art with very low resolution feature maps. We demonstrate the accuracy and the effectiveness of our method by providing extensive experiments on two of the most important publicly available datasets for 3D pose estimation, Human3.6M and MPI-INF-3DHP. Additionally, we provide relevant insights about our decisions on the network architecture and show its flexibility to meet the best precision-speed compromise.}
}
@article{LI2023109638,
title = {Correlated and individual feature learning with contrast-enhanced MR for malignancy characterization of hepatocellular carcinoma},
journal = {Pattern Recognition},
volume = {142},
pages = {109638},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109638},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003394},
author = {Yunling Li and Shangxuan Li and Hanqiu Ju and Tatsuya Harada and Honglai Zhang and Ting Duan and Guangyi Wang and Lijuan Zhang and Lin Gu and Wu Zhou},
keywords = {Multimodal fusion, Hepatocellular carcinoma, Deep feature, Malignancy characterization, Contrast-enhanced MR},
abstract = {Malignancy characterization of hepatocellular carcinoma (HCC) is of great importance in patient management and prognosis prediction. In this study, we propose an end-to-end correlated and individual feature learning framework to characterize the malignancy of HCC from Contrast-enhanced MR. From the phases of pre-contrast, arterial and portal venous, our framework simultaneously and explicitly learns both the shareable and phase-specific features that are discriminative to malignancy grades. We evaluate our method on the Contrast enhanced MR of 112 consecutive patients with 117 histologically proven HCCs. Experimental results demonstrate that arterial phase yields better results than portal vein and pre-contrast phase. Furthermore, phase specific components show better discriminant ability than the shareable components. Finally, combining the extracted shareable and individual features components has yielded significantly better performance than traditional feature fusion methods. We also conduct t-SNE analysis and feature scoring analysis to qualitatively assess the effectiveness of the proposed method for malignancy characterization.}
}
@article{WENG2023109670,
title = {A Decomposition Dynamic graph convolutional recurrent network for traffic forecasting},
journal = {Pattern Recognition},
volume = {142},
pages = {109670},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109670},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003710},
author = {Wenchao Weng and Jin Fan and Huifeng Wu and Yujie Hu and Hao Tian and Fu Zhu and Jia Wu},
keywords = {Traffic forecasting, Dynamic graph generation, Residual decomposition, Segmented learning, Graph convolution network},
abstract = {Our daily lives are greatly impacted by traffic conditions, making it essential to have accurate predictions of traffic flow within a road network. Traffic signals used for forecasting are usually generated by sensors along roads, which can be represented as nodes on a graph. These sensors typically produce normal signals representing normal traffic flows and abnormal signals indicating unknown traffic disruptions. Graph convolution networks are widely used for traffic prediction due to their ability to capture correlations between network nodes. However, existing approaches use a predefined or adaptive adjacency matrix that does not accurately reflect real-world relationships between signals. To address this issue, we propose a decomposition dynamic graph convolutional recurrent network (DDGCRN) for traffic forecasting. DDGCRN combines a dynamic graph convolution recurrent network with an RNN-based model that generates dynamic graphs based on time-varying traffic signals, allowing for the extraction of both spatial and temporal features. Additionally, DDGCRN separates abnormal signals from normal traffic signals and models them using a data-driven approach to further improve predictions. Results from our analysis of six real-world datasets demonstrate the superiority of DDGCRN compared to the current state-of-the-art. The source codes are available at: https://github.com/wengwenchao123/DDGCRN.}
}
@article{ZHENG2023109662,
title = {Continuous cross-modal hashing},
journal = {Pattern Recognition},
volume = {142},
pages = {109662},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109662},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003631},
author = {Hao Zheng and Jinbao Wang and Xiantong Zhen and Jingkuan Song and Feng Zheng and Ke Lu and Guo-Jun Qi},
keywords = {Cross-modal hashing, Fast retrieval, Multiple modalities, Continual learning},
abstract = {Generally, multimodal data with new classes arrive continuously in the real world. While advanced cross-modal hashing (CMH) focuses primarily on batch-based data with previously observed classes (ASCs), it disregards the effect of newly arriving classes (ANCs) on hash-code conflicts. In addition, class-level continuous hashing scenarios do not suit themselves well with the generic CMH configuration. To solve the aforementioned issues, we propose a novel framework, called CT-CMH, for the new task of continuous cross-modal hashing. For dealing with ANCs, CMH models require the ability of continuous learning, i.e. they can preserve the knowledge of previously observed data and, more crucially, they can be adapted to unseen data with ANCs. Specifically, we introduce the adaptive weight importance updating (AWIU) mechanism to alleviate the catastrophic forgetting problem of CMH and a new hash-code divergence (HCD) method to eliminate hash-code conflicts between ASCs and ANCs. When CT-CMH is equipped with both AWIU and HCD, it can consistently achieve high retrieval performance. The experiment results and visualization analyses validate the effectiveness of our approach. To the best of our knowledge, we are the first to introduce and implement the task of CCMH for ANCs.}
}
@article{NANFACK2023109610,
title = {Learning Customised Decision Trees for Domain-knowledge Constraints},
journal = {Pattern Recognition},
volume = {142},
pages = {109610},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109610},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003114},
author = {Géraldin Nanfack and Paul Temple and Benoît Frénay},
keywords = {Decision trees, Constraints, Domain knowledge},
abstract = {When applied to critical domains, machine learning models usually need to comply with prior knowledge and domain-specific requirements. For example, one may require that a learned decision tree model should be of limited size and fair, so as to be easily interpretable, trusted, and adopted. However, most state-of-the-art models, even on decision trees, only aim to maximising expected accuracy. In this paper, we propose a framework in which a diverse family of prior and domain knowledge can be formalised and imposed as constraints on decision trees. This framework is built upon a newly introduced tree representation that leads to two generic linear programming formulations of the optimal decision tree problem. The first one targets binary features, while the second one handles continuous features without the need for discretisation. We theoretically show how a diverse family of constraints can be formalised in our framework. We validate the framework with constraints on several applications and perform extensive experiments, demonstrating empirical evidence of comparable performance w.r.t. state-of-the-art tree learners.}
}
@article{LIU2023109682,
title = {GlobalAP: Global average precision optimization for person re-identification},
journal = {Pattern Recognition},
volume = {142},
pages = {109682},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109682},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003795},
author = {Yifei Liu and Yaling Liang and Pengfei Wang and Ziheng Chen and Changxing Ding},
keywords = {Person re-identification, Image retrieval, Average precision},
abstract = {Average Precision (AP) measures the overall performance on the Person Re-Identification (ReID) task. Optimizing AP using all instances in the training set is accordingly an excellent choice for learning a discriminative ReID model. However, exploiting this method directly is unacceptable in practice due to the high cost of computation on the entire dataset. To this end, this paper proposes an effective and easy-to-use approach called GlobalAP that optimizes AP globally at negligible computational cost. More specifically, GlobalAP adopts a memory module to acquire the embedding features of all instances in the training set. To reduce the required computational complexity, GlobalAP utilizes only a few instances with high similarities to the query, to compute AP; this is because we observe that only these instances significantly affect AP and model optimization. Moreover, we propose to gradually increase the difficulty of GlobalAP to further encourage intra-class compactness and inter-class separability. Ultimately, GlobalAP can globally optimize AP and dramatically boost the model performance at negligible computational cost. We evaluate GlobalAP on six large-scale ReID datasets. Experimental results show that GlobalAP exhibits obvious advantages in terms of both computational efficiency and ReID accuracy.}
}
@article{WANG2023109654,
title = {Class-specific and self-learning local manifold structure for domain adaptation},
journal = {Pattern Recognition},
volume = {142},
pages = {109654},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109654},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003552},
author = {Wei Wang and Mengzhu Wang and Xiao Dong and Long Lan and Quannan Zu and Xiang Zhang and Cong Wang},
keywords = {Domain adaptation, Wrongly labeled, Feature-corrupted, Local manifold, Global discriminative, Class-specific local manifold, Self-learning},
abstract = {Domain adaptation (DA) is a powerful technology that allows a classifier trained on a well-labeled source domain to be adapted to an unlabeled target domain with different distributions. Existing DA methods aim to enhance feature transferability by reducing distribution distance, and they often rely on preserving the global discriminative (GD) structure to boost feature discriminability. However, strict GD loss may degrade transferability, and poor discriminability may result from wrongly labeled samples. To address these issues, we propose a new approach called class-specific and self-learning local manifold structure (CSSL-LM), to extract more desirable features for better DA effects. Specifically, it draws two data points close with large weight if they are from the same class and close in the original feature space. This approach is more relaxed than GD and thus mitigates the negative effect on transferability. Moreover, CSSL-LM is more robust to wrongly labeled samples than GD since two data points that are wrongly labeled as the same classes have small weight and are not required to be close. Inspired by previous adaptive local manifold learning, we utilize a self-learning mechanism to model CSSL-LM more accurately and reliably, particularly for feature-corrupted samples. Extensive experiments on four DA benchmarks verify that CSSL-LM outperforms some state-of-the-art methods. We also construct DA datasets that are randomly wrongly labeled or feature-corrupted to further evaluate CSSL-LM’s robustness.}
}
@article{ZHANG2023109701,
title = {Features kept generative adversarial network data augmentation strategy for hyperspectral image classification},
journal = {Pattern Recognition},
volume = {142},
pages = {109701},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109701},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003990},
author = {Mingyang Zhang and Zhaoyang Wang and Xiangyu Wang and Maoguo Gong and Yue Wu and Hao Li},
keywords = {Hyperspectral images (HSIs), Deep learning, Generative adversarial network (GAN), Data augmentation},
abstract = {In recent years, significant breakthroughs have been achieved in hyperspectral image (HSI) processing using deep learning techniques, including classification, object detection, and anomaly detection. However, the practical application of deep learning in HSI processing is limited by challenges such as small-sample size and sample imbalance issues. To mitigate these limitations, we propose a novel data augmentation strategy called Feature-Preserving Generative Adversarial Network Data Augmentation (FPGANDA). What sets our data augmentation strategy apart from existing generative model-based approaches is that we preserve the main spectral bands of HSI data using a newly designed band selection method. Additionally, our proposed generative model generates synthetic spectral bands, which are combined with the real spectral bands using a mixture strategy to create augmented data. This approach ensures that the augmented data retain the main features of the original data while also incorporating diverse features from the generated data. We evaluate our method on three different HSI datasets, comparing it with state-of-the-art techniques. Experimental results demonstrate that our proposed method significantly improves classification performance in most scenes and exhibits remarkable compatibility.}
}
@article{COMIC2023109693,
title = {Discrete analytical objects in the body-centered cubic grid},
journal = {Pattern Recognition},
volume = {142},
pages = {109693},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109693},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003916},
author = {Lidija Čomić and Gaëlle Largeteau-Skapin and Rita Zrour and Ranita Biswas and Eric Andres},
keywords = {Discrete geometry, BCC Grid, Discrete analytical plane, Discrete analytical sphere, Discrete analytical line, 3D Coordinate system},
abstract = {We propose a characterization of discrete analytical spheres, planes and lines in the body-centered cubic (BCC) grid, both in the Cartesian and in the recently proposed alternative compact coordinate system, in which each integer triplet addresses some voxel in the grid. We define spheres and planes through double Diophantine inequalities and investigate their relevant topological features, such as functionality or the interrelation between the thickness of the objects and their connectivity and separation properties. We define lines as the intersection of planes. The number of the planes (up to six) is equal to the number of the pairs of faces of a BCC voxel that are parallel to the line.}
}
@article{ARCE2023109685,
title = {Learning an artificial neural network to discover bit-quad-based formulas to compute basic object properties},
journal = {Pattern Recognition},
volume = {142},
pages = {109685},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109685},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003837},
author = {Fernando Arce and Wilfrido Gómez-Flores and Uriel Escalona and Humberto Sossa},
keywords = {Shape analysis, Artificial neural network, Computation time, Bit-quads, Area, Perimeter, Contact perimeter},
abstract = {Shape analysis requires estimating object properties in many applications, including optical character recognition, tumor classification, skin cancer recognition, leaf and plant identification, and cell analysis. In particular, bit-quad-based linear expressions proposed by Gray and Duda for calculating the area and perimeter of binary images are widely used in the literature. Nevertheless, these formulas require computing 14 or 15 bit-quad patterns out of 16 possible, becoming critical in applications with limited computing resources. Hence, this paper introduces a method based on a single-layer artificial neural network (ANN) to discover new expressions to calculate the area and perimeter with fewer bit-quads than the original formulas without losing measuring accuracy. Besides, an iterative elimination process removes irrelevant bit-quads whose corresponding weights approach zero. After that, an inductive analysis from observing the learned weights provides interpretable formulas for the area and perimeter. Furthermore, the proposed approach is also applied to find bit-quad-based formulas for directly computing the contact perimeter property, whose original formula requires precomputing Gray’s area and perimeter of the object. In addition, aiming to show our method’s versatility in other applications, we address a real-world problem to discover a bit-quad-based formula to distinguish between two classes of loss of bone density caused by hyperthyroidism and aging in rats. The experimental results show that the proposed approach reduces by approximately half the bit-quads needed to calculate the area and perimeter of Gray and Duda. Likewise, the number of bit-quads to compute the contact perimeter is reduced from nine to six. Besides, the estimated value on test sets by all the found formulas is the same as their original counterparts. On the other hand, the classification of loss of bone density type using the found bit-quad-based formula reaches an accuracy of 100% in the test set. Therefore, the proposed method is an alternative to finding linear expressions with few bit-quads to measure basic object properties.}
}
@article{WANG2023109718,
title = {Simultaneous local clustering and unsupervised feature selection via strong space constraint},
journal = {Pattern Recognition},
volume = {142},
pages = {109718},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109718},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004168},
author = {Zheng Wang and Qi Li and Haifeng Zhao and Feiping Nie},
keywords = {Unsupervised feature selection, -Norm constraint optimization, Local structure learning},
abstract = {Clustering is a fashion method applied in machine learning tasks. However, high dimensional data brings many obstacles for clustering approaches. To address such a problem, the unsupervised feature selection (UFS) method can be incorporated into clustering to reduce dimensionality. In general, most of the UFS methods adopt ℓ2,1-norm for subspace sparsity learning. However, its sparsity highly relies on the setting of trade-off parameter, which may lead to instability of ranking results and the difficulty in obtaining the optimal solution of projection matrix. In this paper, we propose to directly learn an absolutely row-sparsity subspace via the ℓ2,0-norm constraint, called Sparse constraint and Local learning for Unsupervised Feature Selection (SLUFS). It is an ideal sparse subspace constraint which can overcome the drawbacks of the ℓ2,1-norm. However, optimizing the ℓ2,0-norm constraint is an NP-hard problem, and at present, only some approximate solutions can be given, but the convergence can not be guaranteed. To tackle this challenge, we design a novel alternative iterative algorithm to directly optimize the ℓ2,0-norm based model. Most importantly, our strategy can obtain a closed-form solution with strict convergence guarantee. Comprehensive experiments are conducted on several real-world datasets to evaluate the performance of SLUFS with comparison to several related state-of-the-art methods.}
}
@article{SHI2023109702,
title = {Global- and local-aware feature augmentation with semantic orthogonality for few-shot image classification},
journal = {Pattern Recognition},
volume = {142},
pages = {109702},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109702},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004004},
author = {Boyao Shi and Wenbin Li and Jing Huo and Pengfei Zhu and Lei Wang and Yang Gao},
keywords = {Few-shot image classification, Transfer learning, Feature augmentation, Semantic orthogonal learning},
abstract = {As for few-shot image classification, recently, some works revisit the standard transfer learning paradigm, i.e., pre-training and fine-tuning, and have achieved some success. However, we find that this kind of methods heavily relies on a naive image-level data augmentation (e.g., cropping and flipping) at the fine-tuning stage, which will easily suffer from the overfitting problem because of the limited-data regime. To tackle this issue, in this paper, we attempt to perform a novel feature-level semantic augmentation at the fine-tuning stage and propose a Global- and Local-aware Feature Augmentation method (GLFA) from both the channel- and spatial-wise perspectives. In addition, at the pre-training stage, we further propose a Semantic Orthogonal Learning Framework (SOLF) to make the learned feature channels more independently, orthogonal and diverse. Extensive experiments demonstrate that the proposed method can obtain significant performance improvements over the state of the arts. Code is available at https://github.com/onlyyao/GLFA-SOLF.}
}
@article{ZHU2023109674,
title = {GARNet: Global-aware multi-view 3D reconstruction network and the cost-performance tradeoff},
journal = {Pattern Recognition},
volume = {142},
pages = {109674},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109674},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003758},
author = {Zhenwei Zhu and Liying Yang and Xuxin Lin and Lin Yang and Yanyan Liang},
keywords = {3D Reconstruction, Multi-view input, Voxel, Cost-performance tradeoff, Deep learning},
abstract = {Deep learning technology has made great progress in multi-view 3D reconstruction tasks. At present, the mainstream solutions adopt different ways to fusion the features from several views. Among them, attention-based aggregation function performs relatively well and stably, however, it still has an obvious shortcoming the strong independence of each view during predicting the weights for merging leads to a lack of adaption of the global state. In this paper, we propose a global-aware attention-based fusion approach that builds the correlation between each branch and the global feature to provide a comprehensive foundation for weights inference. On the basis of this, we design a complete reconstruction algorithm. Experiments on ShapeNet verify that our method outperforms existing SOTA methods. Furthermore, we propose a view-reduction method based on maximizing diversity and discuss the cost-performance tradeoff of our model to achieve a better performance when facing a heavy input amount and limited computational cost.}
}
@article{SUN2023109726,
title = {Attentional prototype inference for few-shot segmentation},
journal = {Pattern Recognition},
volume = {142},
pages = {109726},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109726},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004247},
author = {Haoliang Sun and Xiankai Lu and Haochen Wang and Yilong Yin and Xiantong Zhen and Cees G.M. Snoek and Ling Shao},
keywords = {Few-shot segmentation, Variational inference, Probabilistic model, Latent attention},
abstract = {This paper aims to address few-shot segmentation. While existing prototype-based methods have achieved considerable success, they suffer from uncertainty and ambiguity caused by limited labeled examples. In this work, we propose attentional prototype inference (API), a probabilistic latent variable framework for few-shot segmentation. We define a global latent variable to represent the prototype of each object category, which we model as a probabilistic distribution. The probabilistic modeling of the prototype enhances the model’s generalization ability by handling the inherent uncertainty caused by limited data and intra-class variations of objects. To further enhance the model, we introduce a local latent variable to represent the attention map of each query image, which enables the model to attend to foreground objects while suppressing the background. The optimization of the proposed model is formulated as a variational Bayesian inference problem, which is established by amortized inference networks. We conduct extensive experiments on four benchmarks, where our proposal obtains at least competitive and often better performance than state-of-the-art prototype-based methods. We also provide comprehensive analyses and ablation studies to gain insight into the effectiveness of our method for few-shot segmentation.}
}
@article{YU2023109666,
title = {eX-ViT: A Novel explainable vision transformer for weakly supervised semantic segmentation},
journal = {Pattern Recognition},
volume = {142},
pages = {109666},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109666},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003679},
author = {Lu Yu and Wei Xiang and Juan Fang and Yi-Ping Phoebe Chen and Lianhua Chi},
keywords = {Explainable, Attention map, Transformer, Weakly supervised},
abstract = {Recently vision transformer models have become prominent models for a multitude of vision tasks. These models, however, are usually opaque with weak feature interpretability, making their predictions inaccessible to the users. While there has been a surge of interest in the development of post-hoc solutions that explain model decisions, these methods can not be broadly applied to different transformer architectures, as rules for interpretability have to change accordingly based on the heterogeneity of data and model structures. Moreover, there is no method currently built for an intrinsically interpretable transformer, which is able to explain its reasoning process and provide a faithful explanation. To close these crucial gaps, we propose a novel vision transformer dubbed the eXplainable Vision Transformer (eX-ViT), an intrinsically interpretable transformer model that is able to jointly discover robust interpretable features and perform the prediction. Specifically, eX-ViT is composed of the Explainable Multi-Head Attention (E-MHA) module, the Attribute-guided Explainer (AttE) module with the self-supervised attribute-guided loss. The E-MHA tailors explainable attention weights that are able to learn semantically interpretable representations from tokens in terms of model decisions with noise robustness. Meanwhile, AttE is proposed to encode discriminative attribute features for the target object through diverse attribute discovery, which constitutes faithful evidence for the model predictions. Additionally, we have developed a self-supervised attribute-guided loss for our eX-ViT architecture, which utilizes both the attribute discriminability mechanism and the attribute diversity mechanism to enhance the quality of learned representations. As a result, the proposed eX-ViT model can produce faithful and robust interpretations with a variety of learned attributes. To verify and evaluate our method, we apply the eX-ViT to several weakly supervised semantic segmentation (WSSS) tasks, since these tasks typically rely on accurate visual explanations to extract object localization maps. Particularly, the explanation results obtained via eX-ViT are regarded as pseudo segmentation labels to train WSSS models. Comprehensive simulation results illustrate that our proposed eX-ViT model achieves comparable performance to supervised baselines, while surpassing the accuracy and interpretability of state-of-the-art black-box methods using only image-level labels.}
}
@article{LIU2023109735,
title = {A Lie group kernel learning method for medical image classification},
journal = {Pattern Recognition},
volume = {142},
pages = {109735},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109735},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004338},
author = {Li Liu and Haocheng Sun and Fanzhang Li},
keywords = {Medical image classification, Feature representation, Lie group manifold, Lie group machine learning, Kernel learning},
abstract = {Medical image classification is a basic step in medical image analysis and has been an essential task in computer-aided diagnosis. Existing classification methods are proved to be effective in conventional image classification tasks, but they often achieve a suboptimal performance when applied to medical images characterizing by complex nonlinear variation. Aiming at this challenge, this paper proposes a Lie group kernel learning method for medical image classification by combining Lie group theory, kernel functions, SVM and KNN classifiers. The method represents each image with a Lie group feature descriptor constructed from low-level features and builds a SVM classifier from the training images. Geodesic distances between categorical pivots and each testing image are calculated with Lie group kernel functions to select either the SVM or a KNN classifier to do the classification. The proposed method is applied to three medical image datasets and the results demonstrate the efficacy of the method.}
}
@article{WANG2023109690,
title = {Extending version-space theory to multi-label active learning with imbalanced data},
journal = {Pattern Recognition},
volume = {142},
pages = {109690},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109690},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003886},
author = {Ran Wang and Shuyue Chen and Yu Yu},
keywords = {Multi-label active learning, Sample-label pairs, Inconsistency, Version space, Imbalanced data},
abstract = {Version space, defined as the subset of the hypothesis space consistent with the training samples, is an important concept in supervised learning. It has been successfully applied for evaluating the informativeness of unlabeled samples in traditional single-label active learning. Specifically, the most inconsistent samples among the version space members can reduce the size of the version space as fast as possible, these samples are given high priority for domain expert annotation, thereby the learner can construct a high-performance classifier by labeling as few samples as possible. We point out that the concept of version space has not been extended to multi-label environments yet, which hinders its application in multi-label active learning. This paper makes an attempt to extend the version space theory from single-label scenario to multi-label scenario, builds up a spatial structure for the multi-label version space, generalizes it from finite case to infinite case, puts forward a simplified representation for it and accordingly proposes a new multi-label active learning algorithm. Moreover, considering the imbalance issue in multi-label data, the algorithm is further improved by allocating different annotation numbers to the labels. Experimental comparisons verify the feasibility and effectiveness of the proposed methods.}
}
@article{HUANG2023109658,
title = {GriT-DBSCAN: A spatial clustering algorithm for very large databases},
journal = {Pattern Recognition},
volume = {142},
pages = {109658},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109658},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300359X},
author = {Xiaogang Huang and Tiefeng Ma and Conan Liu and Shuangzhe Liu},
keywords = {DBSCAN, Clustering, Indexing methods, Spatial databases},
abstract = {DBSCAN is a fundamental spatial clustering algorithm with numerous practical applications. However, a bottleneck of DBSCAN is its O(n2) worst-case time complexity. To address this limitation, we propose a new grid-based algorithm for exact DBSCAN in Euclidean space called GriT-DBSCAN, which is based on the following two techniques. First, we introduce grid tree to organize the non-empty grids for the purpose of efficient non-empty neighboring grids queries. Second, by utilizing the spatial relationships among points, we propose a technique that iteratively prunes unnecessary distance calculations when determining whether the minimum distance between two sets is less than or equal to a certain threshold. We theoretically demonstrate that GriT-DBSCAN has excellent reliability in terms of time complexity. In addition, we obtain two variants of GriT-DBSCAN by incorporating heuristics, or by combining the second technique with an existing algorithm. Experiments are conducted on both synthetic and real-world data sets to evaluate the efficiency of GriT-DBSCAN and its variants. The results show that our algorithms outperform existing algorithms.}
}
@article{GUO2023109639,
title = {Joint A-SNN: Joint training of artificial and spiking neural networks via self-Distillation and weight factorization},
journal = {Pattern Recognition},
volume = {142},
pages = {109639},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109639},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003400},
author = {Yufei Guo and Weihang Peng and Yuanpei Chen and Liwen Zhang and Xiaode Liu and Xuhui Huang and Zhe Ma},
keywords = {Spiking neural networks, Artificial neural networks, Knowledge distillation, Weight factorization},
abstract = {Emerged as a biology-inspired method, Spiking Neural Networks (SNNs) mimic the spiking nature of brain neurons and have received lots of research attention. SNNs deal with binary spikes as their activation and therefore derive extreme energy efficiency on hardware. However, it also leads to an intrinsic obstacle that training SNNs from scratch requires a re-definition of the firing function for computing gradient. Artificial Neural Networks (ANNs), however, are fully differentiable to be trained with gradient descent. In this paper, we propose a joint training framework of ANN and SNN, in which the ANN can guide the SNN’s optimization. This joint framework contains two parts: First, the knowledge inside ANN is distilled to SNN by using multiple branches from the networks. Second, we restrict the parameters of ANN and SNN, where they share partial parameters and learn different singular weights. Extensive experiments over several widely used network structures show that our method consistently outperforms many other state-of-the-art training methods. For example, on the CIFAR100 classification task, the spiking ResNet-18 model trained by our method can reach to 77.39% top-1 accuracy with only 4 time steps.}
}
@article{NADEEM2023109655,
title = {Cross domain 2D-3D descriptor matching for unconstrained 6-DOF pose estimation},
journal = {Pattern Recognition},
volume = {142},
pages = {109655},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109655},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003564},
author = {Uzair Nadeem and Mohammed Bennamoun and Roberto Togneri and Ferdous Sohel and Aref {Miri Rekavandi} and Farid Boussaid},
keywords = {2D-3D Matching, Cross-Domain feature matching, 6-DOF Pose estimation, Image localization, Camera localization, Visual localization},
abstract = {This paper presents a novel approach for cross-domain descriptor matching between 2D and 3D modalities. The 2D-3D matching is applied to localize 2D images in 3D point clouds. Direct cross-domain matching allows our technique to localize images in any type of 3D point cloud without any constraints on the nature or mechanism by which it is obtained. We propose a learning based framework, called Desc-Matcher, to directly match features between the two modalities. A dataset of 2D and 3D features with corresponding locations in images and point clouds is generated to train the Desc-Matcher. To estimate the pose of an image in any 3D cloud, keypoints and feature descriptors are extracted from the query image and the point cloud. The trained Desc-Matcher is then used to match the features from the image and the point cloud. A robust pose estimator is used to predict the location and orientation of the query image from the corresponding positions of the matched 2D and 3D features. We carried out an extensive evaluation of the proposed method for indoor and outdoor scenarios and with different types of point clouds to verify the feasibility of our approach. Experimental results show that the proposed approach can reliably estimate the 6-DOF poses of query cameras in any type of 3D point cloud with high precision. We achieved average median errors of 1.09cm/0.27∘ and 19cm/0.39∘ on the Stanford and Cambridge datasets, respectively.}
}
@article{ALSUMAIDAEE2023109647,
title = {Spatio-temporal modelling with multi-gradient features and elongated quinary pattern descriptor for dynamic facial expression recognition},
journal = {Pattern Recognition},
volume = {142},
pages = {109647},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109647},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003485},
author = {S.A.M. Al-Sumaidaee and M.A.M. Abdullah and R.R.O. Al-Nima and S.S. Dlay and J.A. Chambers},
keywords = {Dynamic facial expression recognition, Gaussian mask, Three orthogonal planes, Block volume, Three-dimensional histogram features},
abstract = {We propose a new spatio-temporal modelling approach for Dynamic Facial Expression Recognition (DFER). We first convert the domain of the spatial images in the sequence to the gradient of magnitude and angle images at different orientations. Robust gradient components are developed to deal with difficult types of illuminations, such as darkness, by forming the eight edge responses of the Gaussian mask. To describe the dynamic Facial Expression (FE) changes we extend the Elongated Quinary Pattern (EQP) descriptor to encode separately the anisotropic structure of the uniform patterns from Three Orthogonal Planes (TOP) of each gradient sequence. Then each encoded sequence is divided into a stack of block volumes in the XY, XT and YT planes. For each plane, the co-occurrence of histogram features are calculated from each block volume and concatenated together. Simple three-dimensional histogram features are generated by concatenating the histogram features of all planes. A Multi Classifier System (MCS) based on a multi-class Support Vector Machine (SVM) is adopted to combine all scores for the encoded sequences. The proposed approach is evaluated with the challenging MMI and Oulu-CASIA databases with different set-ups and advantage has been shown in terms of generalisation to different databases, together with robustness against difficult pose variations and illumination changes. In terms of Recognition Accuracy (RA), a comparison is established with DFER methods in the literature. A high recognition rate of 79.23% is attained in the case of six classes when applied to the MMI database which surpasses all the state-of-the-art results.}
}
@article{SU2023109686,
title = {Neighborhood-based credibility anchor learning for universal domain adaptation},
journal = {Pattern Recognition},
volume = {142},
pages = {109686},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109686},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003849},
author = {Wan Su and Zhongyi Han and Rundong He and Benzheng Wei and Xueying He and Yilong Yin},
keywords = {Universal domain adaptation, Threshold-free, Neighborhood learning},
abstract = {Universal domain adaptation (UniDA) aims to transfer knowledge from the source domain to the target domain in the presence of distribution shift and class mismatch. Most existing works design threshold-relied methods to reject target private classes by carefully-proposed uncertainty scoring functions which are very sensitive to thresholds. To overcome this problem, a few threshold-free methods are proposed but ignore the neighborhood structure information of the target domain, leading to poor performance. In this paper, we propose Neighborhood-based Credibility Anchor Learning (NCAL), a new threshold-free framework that fully mines the neighborhood structure information to explore better target representations. NCAL contains three key components: a class anchor learning module to learn target class distribution, a credibility-weighted conditional adversarial module to learn class-invariant features of common classes, and an open-set neighborhood clustering module to learn well-clustered features. Extensive experiments demonstrate that our method outperforms the state-of-the-art.}
}
@article{WANG2023109723,
title = {A novel classification method combining phase-field and DNN},
journal = {Pattern Recognition},
volume = {142},
pages = {109723},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109723},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004211},
author = {Jian Wang and Ziwei Han and Wenjing Jiang and Junseok Kim},
keywords = {Phase-field-DNN, Phase-field, DNN, Classification},
abstract = {This paper proposes a novel classification method. Firstly, we use the deep neural network (DNN) to classify the training set. After several iterations, we obtain the output vector Y. The component of the largest value in vector Y is represented as the label being classified, which we take as the output value. Because we chose the sigmoid function as our activation function, the output value is between 0 and 1. Therefore, the output value can represents the probability of the classified label by the DNN. Depending on the distribution of output values, we set tolerance values (Tol) that categorize similar output values as the same label in the DNN. If the output value is lower than Tol, we consider it categorically anomalous. Subsequently, we use the Phase-Field model to classify these anomalies and obtain better classification results. As this classification method combines Phase-Field model and DNN, we named it Phase-Field-DNN. In the numerical experiment using MNIST handwritten digit data set as experimental data, the classification accuracy of Phase-Field-DNN model is higher than that of Phase-Field model and DNN model through the analysis of the classification results of binary classification and multi-classification problems with this data. In addition, the model we proposed is used to classify the normal and abnormal brain MRIs, and the classification results are compared with those of others. After comparison, we find that our proposed model achieve the best classification results.}
}
@article{XUE2023109651,
title = {Hybrid neural-like P systems with evolutionary channels for multiple brain metastases segmentation},
journal = {Pattern Recognition},
volume = {142},
pages = {109651},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109651},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003527},
author = {Jie Xue and Qi Li and Xiyu Liu and Yujie Guo and Jie Lu and Bosheng Song and Pu Huang and Qiong An and Guanzhong Gong and Dengwang Li},
keywords = {Hybrid neural-like P system, Evolutionary channels, Segmentation of brain metastases},
abstract = {Neural-like P systems are membrane computing models inspired by natural computing. Spiking neurral (SN) P systems, a kind of neural-like P systems, are viewed as third-generation neural network models. Although real neurons have complex structures, classical SN P systems simplify the structures and corresponding mechanisms to stationary two-dimensional graphs and lack related evolution mechanisms on spikes and channels, which limits the real applications of these models. In this paper, we propose a new hybrid SN P system with evolutionary channels (HN P systems), including three new types of rules for dynamically generating or removing one-one and one-many/many-one channels with related evolutions of spikes on the hybrid neuron structures. Two dynamic regulatory factors are also presented on rules to help guide the optimization of the HN P systems automatically. Based on the new P system, a multiple brain metastases (BMs) segmentation model is developed. The experimental results indicate that the proposed models outperform the state-of-the-art methods on the BMs, which have large variations in sizes, positions and shapes, and low contrast with their surroundings. Performances on the head and neck segmentation dataset also verifies the effectiveness of the HN P system.}
}
@article{HE2023109680,
title = {NRPose: Towards noise resistance for multi-person pose estimation},
journal = {Pattern Recognition},
volume = {142},
pages = {109680},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109680},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003801},
author = {Jianhang He and Junyao Sun and Qiong Liu and Shaowu Peng},
keywords = {Multi-person pose estimation, Noise resistance, Region proposal, Keypoint relation},
abstract = {The high signal-to-noise ratio is one of the main challenges of multi-person pose estimation (MPE) and receives little attention. In this work, we find that MPE suffers from two types of noise: aleatoric noise and epistemic noise. The former represents the noise inherent in the observations, such as the background. The latter indicates the noise brought by the priori hypotheses, such as the inappropriate keypoint relations. Both of them reduce the saliency of information available for keypoint localization. We propose the noise-resistance pose estimation (NRPose) that integrates keypoint-oriented region proposal module (KRPM) and pose-aware sparse relation module (PSRM). To mitigate aleatoric noise, KRPM generates keypoint-level RoIs by circumscribing semantically significant regions. To reduce epistemic noise, PSRM filters out the noisy relations dynamically by modeling the noise propagation and keypoint interaction. NRPose outperforms the state-of-the-art methods by at least 1.0 AP on COCO and OCHuman dataset.}
}
@article{2023109755,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {142},
pages = {109755},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(23)00453-3},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004533}
}
@article{MONDAL2023109698,
title = {Dataset agnostic document object detection},
journal = {Pattern Recognition},
volume = {142},
pages = {109698},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109698},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003965},
author = {Ajoy Mondal and Madhav Agarwal and C.V. Jawahar},
keywords = {Document object detection, Table detection, Figure detection, Equation detection, Cascade Mask , Deformable convolution},
abstract = {Localizing document objects such as tables, figures, and equations is a primary step for extracting information from document images. We propose a novel end-to-end trainable deep network, termed Document Object Localization Network (dolnet), for detecting various objects present in the document images. The proposed network is a multi-stage extension of Mask r-cnn with a dual backbone having deformable convolution for detecting document objects with high detection accuracy at a higher IoU threshold. We also empirically evaluate the proposed dolnet on the publicly available benchmark datasets. The proposed DOLNet achieves state-of-the-art performance for most of the bench-mark datasets under various existing experimental environments. Our solution has three important properties: (i) a single trained model dolnet‡ that performs well across all the popular benchmark datasets, (ii) reports excellent performances across multiple, including with higher IoU thresholds, and (iii) consistently demonstrate the superior quantitative performance by following the same protocol of the recent works for each of the benchmarks.}
}
@article{AHERRAHROU2023109643,
title = {A novel cancelable finger vein templates based on LDM and RetinexGan},
journal = {Pattern Recognition},
volume = {142},
pages = {109643},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109643},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003448},
author = {N. Aherrahrou and H. Tairi},
keywords = {Cancelable biometric, Security, Template protection, Finger vein, LDM, RetinexGan},
abstract = {In this paper, we propose a new biometric template protection scheme, which can deal with the finger vein biometric security threats, through using the LDM and RetinexGAN model. The RetinexGAN model is mainly used to handle the illumination and low contrast problems effectively, while efficiently extracting discriminative features from the finger vein images. The projection of extracted features into dissimilarity space is done using Local Dissimilarity Map (LDM). LDM is an efficient way for finger vein features representation, which investigates the relationships and correlation inter and intra classes, while effectively coming up with the accidental shifts/rotations caused by the arbitrary position of the finger during image acquisition. The proposed approach is successfully evaluated in terms of non-invertibility, non-linkability, revocability and performances. Experimental results and comparison analysis with the state of arts methods confirm that the proposed framework can achieve promising results.}
}
@article{DONG2023109720,
title = {Generalization capacity of multi-class SVM based on Markovian resampling},
journal = {Pattern Recognition},
volume = {142},
pages = {109720},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109720},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004181},
author = {Zijie Dong and Chen Xu and Jie Xu and Bin Zou and Jingjing Zeng and Yuan Yan Tang},
keywords = {MSVM, Markovian resampling, Learning rate, Generalization bound},
abstract = {The generalization performance of “All-in-one” Multi-class SVM (AIO-MSVM) based on uniformly ergodic Markovian chain (u.e.M.c.) samples is considered. We establish the fast learning rate of AIO-MSVM algorithm with u.e.M.c. samples and prove that AIO-MSVM algorithm with u.e.M.c. samples is consistent. We also propose a novel AIO-MSVM algorithm based on q-times Markovian resampling (AIO-MSVM-MR), and show the numerical investigation on the learning performance of AIO-MSVM-MR based on public datasets. The experimental studies indicate that compared to the classical AIO-MSVM algorithm and other MSVM algorithms, the proposed AIO-MSVM-MR algorithm has not only smaller misclassification rate, but also less sampling and training total time. We present some discussions on the case of unbalanced training samples, the choices of q and two technical parameters, and present some explanations on the learning performance of the proposed algorithm.}
}
@article{CHEN2023109728,
title = {Rethinking the unpretentious U-net for medical ultrasound image segmentation},
journal = {Pattern Recognition},
volume = {142},
pages = {109728},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109728},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004260},
author = {Gongping Chen and Lei Li and Jianxun Zhang and Yu Dai},
keywords = {Nested U-nets, Ultrasound Image, Breast tumor, Renal ultrasound, Automatic segmentation},
abstract = {Breast tumor segmentation from ultrasound images is one of the key steps that help us characterize and localize tumor regions. However, variable tumor morphology, blurred boundaries, and similar intensity distributions bring challenges for radiologists to segment breast tumors manually. During clinical diagnosis, there are higher demands on the segmentation accuracy and efficiency of breast ultrasound images, so there is an urgent need for an automated method to improve the segmentation accuracy as a technical tool to assist diagnosis. Inspired by the U-net and its many variations, this paper proposed an unpretentious nested U-net (NU-net) for accurate and efficient breast tumor segmentation. The key idea is to utilize U-nets with different depths and shared weights to achieve robust characterization of breast tumors. Specifically, we first utilize the deeper U-net (fifteen layers) as the backbone network to extract more sufficient breast tumor features. Then, we developed a multi-output U-net to be taken as the bond between the encoder and the decoder to enhance the network adaptability for breast tumors with different scales. Finally, the short-connection based on multi-step down-sampling is used to enhance the correlation of long-range information of encoded features. Extensive experimental results with fifteen state-of-the-art segmentation methods on three public breast ultrasound datasets demonstrate that our method has a more competitive segmentation performance on breast tumors. Furthermore, the robustness of our approach is further illustrated by the segmentation of renal ultrasound images. The source code is publicly available on https://github.com/CGPxy/NU-net.}
}
@article{MIAO2023109687,
title = {Triplet teaching graph contrastive networks with self-evolving adaptive augmentation},
journal = {Pattern Recognition},
volume = {142},
pages = {109687},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109687},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003850},
author = {Jiaxing Miao and Feilong Cao and Ming Li and Bing Yang and Hailiang Ye},
keywords = {Contrastive learning, Graph representation learning, Graph augmentation, Node classification},
abstract = {Unsupervised graph contrastive learning has recently emerged as the solution to the crisis of label information scarcity for graph data in the real world. However, from the general paradigm of graph contrastive learning, most of the existing methods are still flawed in the design and use of augmented views and the design of contrastive targets. Therefore, the works on how to generate reasonable augmented views and utilize them canonically and how to construct efficient and comprehensive contrastive objectives are very meaningful. Based on the teaching concept, this paper proposes a new triplet teaching graph contrastive network with self-evolving adaptive augmentation. Firstly, after carefully analyzing the internal relationships between different augmented perspectives, we present a triple teaching graph neural network framework based on the improved triplet idea. It creates contrastive objectives depending on different contrastive angle levels, providing thorough guidance for graph encoders. Secondly, a self-evolving adaptive graph augmentation scheme based on topology and feature information is proposed. It is worth mentioning that with the continuous deepening of the training process, the scheme can utilize the learnable self-attention mechanism to constantly supply our network framework with an increasing number of reliable augmented views as input. Finally, when designing the contrastive objectives, we introduce a stochastic hybrid module to mine the unexploited information, which opportunely complements the contrastive sample space formed by our network framework. Furthermore, extensive experiments on multiple real-world node classification datasets demonstrate that our model can generate better-quality node embedding for downstream tasks. The implementation of this paper is available at https://github.com/PaperMiao/T-GCSA.}
}
@article{ZIHAO2023109648,
title = {Multi-directional broad learning system for the unsupervised stereo matching method},
journal = {Pattern Recognition},
volume = {142},
pages = {109648},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109648},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003497},
author = {Zhang zihao and Niu Ying and Meng Fanman and Yang Tiejun and Fan Chao and Ren Xiaozhen and Wu Ruiqi and Cao Kun and Wang Haocheng},
keywords = {Multi-directional broad learning system, Unsupervised stereo matching, Local gravity weight method},
abstract = {The supervised stereo matching methods usually rely on ground truth disparity maps as the training labels, this limits its practical application in many situations. In this study, we propose a novel unsupervised stereo matching method based on a multi-directional broad learning system. A multi-directional broad learning system was constructed to generate multiple candidate disparity maps. During the generation of each candidate disparity map, an update criterion is proposed for the disparity value based on the maximum similarity of the inverse mapping region to remove the abnormal disparity values of the training samples. Subsequently, multi-direction consistency verification is performed to further eliminate abnormal disparity values, which are based on the uniqueness principle of disparity truth values at the same location. Finally, an invalid depth redefinition based on a local gravity weight method is introduced to select the appropriate disparity value to fill the invalid pixel positions from their neighborhood, which is calculated based on the local region of the color, matching cost, and geometric spaces in the stereo images. We provide the results of experiments on both indoor and outdoor scenarios to demonstrate the effectiveness and flexibility of our approach, including comparisons with state-of-the-art methods.}
}
@article{HUANG2023109676,
title = {Robust unsupervised feature selection via data relationship learning},
journal = {Pattern Recognition},
volume = {142},
pages = {109676},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109676},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003771},
author = {Pei Huang and Zhaoming Kong and Mengying Xie and Xiaowei Yang},
keywords = {Unsupervised feature selection, Outlier, Robustness},
abstract = {Unsupervised feature selection robust to many outliers is a challenging task. The crucial difficulty is learning a robust subspace, which preserves local structure. The most common solution is to reduce fitting error by applying different robust norms. However, there are three shortcomings. Firstly, they are not robust enough when outliers distributed both randomly and concentratedly are widely present. Secondly, outlier removal is not considered. Thirdly, it is not easy to understand and choose an euclidean distance threshold that decides a sample as an outlier in different scenarios. The first two shortcomings make previous methods fail to achieve their expected learning results, and the third one increases the application difficulty in different fields. To address these issues, a robust unsupervised feature selection via data relationship learning (RUFSDR) is proposed in this paper. Specifically, scores representing the data’s importance will be learned and assigned to each sample. Inliers will be given different positive scores. Outliers will be given 0 such that a subspace, which preserves the local structure better, can be learned without prior knowledge about the distance threshold. The experiments conducted on various datasets with several scenarios show the superiority of RUFSDR.}
}
@article{CHEN2023109668,
title = {Knowledge driven weights estimation for large-scale few-shot image recognition},
journal = {Pattern Recognition},
volume = {142},
pages = {109668},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109668},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003692},
author = {Jingjing Chen and Linhai Zhuo and Zhipeng Wei and Hao Zhang and Huazhu Fu and Yu-Gang Jiang},
keywords = {Few-shot image, Recognition, Knowledge transfer},
abstract = {We study the topic of large-scale few-shot image recognition with semantic-visual relational knowledge-based transfer learning. Compared with classical few-shot learning, which is defined as a k-way (k denotes the number of categories, usually 5/10-way) classification problem, large-scale few-shot recognition contains more categories (100-way +) with few samples per category and is easier to overfit. A promising direction of large-scale few-shot learning is transferring prior relevant semantic/visual knowledge from outside data to accelerate the convergence on limited positives. Inspired by this, we propose a novel Knowledge Driven Weights Estimation framework. Specifically, the framework leverages semantic and visual relations between new few-shot and existed many-shot categories to transfer knowledge trained on many-shot datasets (e.g., ImageNet-1000). We show that the transferred knowledge provides a good initialization for novel few-shot categories leading to faster convergence speed and higher performance than random/imprinting initialization. Experimental results on additional un-seen ImageNet categories (other than the 1000 categories) with few positives show that our method is effective on large-scale few-shot recognition.}
}
@article{HAGHPANAH2023109683,
title = {Determining the trustworthiness of DNNs in classification tasks using generalized feature-based confidence metric},
journal = {Pattern Recognition},
volume = {142},
pages = {109683},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109683},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003813},
author = {Mohammad Amin Haghpanah and Mehdi {Tale Masouleh} and Ahmad Kalhor},
keywords = {Machine learning, Deep learning, Confidence metric, Generalized feature-based confidence, Model trust score, Feature quality evaluation},
abstract = {Determining the confidence of Deep Neural Networks in predictions is crucial for building reliable and robust systems. However, it has received minor attention among other areas related to Deep Learning. The confidence of DNNs in predictions is highly correlated with their ability in feature extraction. Consequently, a more robust feature extractor in DNNs leads to a more confident and trustworthy model. In this study, a method is designed in order to determine the trustworthiness of DNNs based on the quality of their feature extraction components. The concept of feature quality is defined based on the models’ confidence in predictions. In a situation where two DNNs have approximately the same accuracy, the superior model has more confidence in its predictions. Hence, it is less influenced by overfitting, making it more robust and reliable in unseen and noisy environments. Determining such a model is not always possible with the well-known accuracy metric. Accordingly, a novel metric named Generalized Feature-Based Confidence Metric is proposed, which is capable of profoundly evaluating the models’ confidence in predictions. It analyzes layer-by-layer feature vectors generated by DNNs and evaluates their quality. Altogether, these utilities boost assessing and comparing different models with varying widths and depths, improving them, and picking the best one. The practicality of the proposed method and metric is investigated through four significantly diverse case studies and empirically proved. Three of them are reputable benchmarking datasets, namely, CIFAR-10, CIFAR-100, and Fashion-MNIST. Moreover, a new high-quality dataset for the Hand Rubbing problem (made by the authors) is used to analyze the proposed method’s performance in a real-world application. Overall, the proposed metric is able to distinguish between different models from about 1% to 8% in terms of confidence in predictions where the models possess almost the same accuracy (0.5% difference or lower).}
}
@article{PAN2023109699,
title = {Hyperspectral image denoising via spectral noise distribution bootstrap},
journal = {Pattern Recognition},
volume = {142},
pages = {109699},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109699},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003977},
author = {Erting Pan and Yong Ma and Xiaoguang Mei and Fan Fan and Jiayi Ma},
keywords = {Hyperspectral image denoising, Image restoration, Spectral distribution, Noise estimation, Noise distribution},
abstract = {Hyperspectral image (HSI) denoising is an ill-posed problem, leading to integrating proper prior knowledge about hyperspectral noise is critical to developing an efficient denoising method. Most existing methods share a common assumption that all bands have equal noise intensity. However, such assumption runs counter to the practical HSIs, leading to unpleasant denoising results. To tackle this, we intend to investigate the intrinsic properties of real HSI noise in the spectral dimension and construct a novel denoising framework bootstrapping by spectral noise distribution N^, termed N^-Net. On the one hand, we develop dense and sparse recurrent calculations, exploiting intrinsic properties of HSI noise (i.e., diversity, dense dependency, and global sparsity) to estimate spectral noise distribution. On the other hand, having the estimated spectral noise distribution, we develop a bootstrap mechanism with a repetitive emphasis on its guidance for subsequent spatial noise separation and clean HSI recovery, ensuring a more delicate denoising effect. In particular, we verify that the proposed denoising framework can achieve promising denoising performances due to the merit of spectral noise distribution bootstrapping, which also promotes new insights for future related research. The code is avaliable at https://github.com/EtPan/N-Net.}
}
@article{XIE2023109681,
title = {FBN: Federated Bert Network with client-server architecture for cross-lingual signature verification},
journal = {Pattern Recognition},
volume = {142},
pages = {109681},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109681},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300376X},
author = {Liyang Xie and Zhongcheng Wu and Xian Zhang and Yong Li},
keywords = {Online signature verification, Federated Bert Network, Client-server architecture, Length Alignment Algorithm, Federated Average Algorithm with Reward-Punishment Mechanism},
abstract = {Online signature verification has a great challenge due to the poor performance of deep learning techniques on cross-lingual datasets under privacy constraints. In this paper, we propose a novel Federated Bert Network (FBN) by embedding the Bidirectional Encoder Representations from Transformers (Bert) into a Federated Learning (FL) framework with client-server architecture. A new Length Alignment Algorithm is employed to unify the signature pairs’ sequence length, and the input representations are fed into the different clients to complete the independent learning of local-models. In addition, the server (coordinator) uses the improved Federated Average Algorithm with Reward-Punishment Mechanism (FedAvgRP) to aggregate these local-models and further generate a global-model. After multiple iterations, the optimal model can be obtained and cross-tested on four datasets (SVC 2004, MCYT-330, BioecurID, and Ours) with skilled forged (random forged) EERs of 7.65% (4.76%), 10.73% (8.46%), 10.09% (7.13%), and 8.28% (5.74%), respectively, far higher than that of the independent learning of state-of-the-art methods. Compared with the domain adaptation and improved FL models, our FBN model performs best in random and skilled forgery scenarios. Moreover, the FedAvgRP algorithm helps our model maintain high performance in the face of data attacks.}
}
@article{FREITAS2023109672,
title = {Time-constrained learning},
journal = {Pattern Recognition},
volume = {142},
pages = {109672},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109672},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003734},
author = {Sergio Freitas and Eduardo Laber and Pedro Lazera and Marco Molinaro},
keywords = {Machine teaching, Time-constrained learning, Classification methods},
abstract = {Consider a scenario in which we have a huge labeled dataset D and a limited time to train a given learner using D. Since we may not be able to use the whole dataset, how should we proceed? We propose TCT, an algorithm for this task, whose design relies on principles from Machine Teaching. We present an experimental study involving 5 different learners and 20 datasets where we show that TCT consistently outperforms alternative teaching/training methods, namely: (1) Training over batches of random samples, until the time limit is reached; (2) The state-of-the-art Machine Teaching algorithm for black-box learners proposed in [Dasgupta et al., ICML 19], and (3) Stochastic Gradient Descent (when applicable). While our work is primarily practical, we also show that a stripped-down version of TCT has provable guarantees.}
}
@article{LI2023109691,
title = {Multi-Scale correlation module for video-based facial expression recognition in the wild},
journal = {Pattern Recognition},
volume = {142},
pages = {109691},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109691},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003898},
author = {Tankun Li and Kwok-Leung Chan and Tardi Tjahjadi},
keywords = {Facial expression recognition, Convolutional neural networks, Motion estimation, Adaptive fusion},
abstract = {The detection of facial muscle movements (e.g., mouth opening) is crucial for facial expression recognition (FER). However, extracting these facial motion features is challenging for a deep-learning recognition system for the following reasons: (1) without explicit labels of motion for training, there is no guarantee that convolutional neural networks (CNNs) can extract motions effectively; (2) compared to human action recognition (e.g., the object moving from left to right), some facial motions (e.g., raising eyebrows) are more subtle and thus harder to extract; and (3) the use of optical flow to extract motion features is time-consuming when using a commonly-used camera. In this work, we propose a Multi-Scale Correlation Module (MSCM) together with an adaptive fusion. Firstly, large as well as small facial motions are extracted by MSCM and encoded by CNNs. Then, an adaptive fusion module is used to aggregate motion features. With these modules, our recognition network is able to model both subtle and large motion features for video-based FER with only the RGB image frames as input. Experiments on two datasets, AFEW and DFEW, show that the network achieves state-of-art performances on the benchmarks.}
}
@article{LI2023109637,
title = {Robust rank-one matrix completion with rank estimation},
journal = {Pattern Recognition},
volume = {142},
pages = {109637},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109637},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003382},
author = {Ziheng Li and Feiping Nie and Rong Wang and Xuelong Li},
keywords = {Matrix completion, Robust, Rank-one matrix pursuit, Rank estimation},
abstract = {Matrix completion aims at estimating the missing entries of a low-rank and incomplete data matrix. It frequently arises in many applications such as computer vision, pattern recognition, recommendation system, and data mining. Most of the existing methods face two problems. Firstly, the data matrix in real world is often disturbed by noise. Noise may change the date structure of the incomplete matrix, thereby degrade the performance of matrix completion algorithms. Secondly, some existing methods need to preset a reasonable rank as input, and the value of rank will affect the performance of the algorithms. Therefore, we proposed a robust rank-one matrix completion method with rank estimation in this paper. To mitigate the influence of noise, we divide the incomplete and noisy data matrix into two parts iteratively: low-rank and sparse parts. Besides, we use a weighted rank-one matrix pursuit algorithm to approximate the low-rank part of the data matrix, and the rank of the matrix can be estimated with the adaptive weight vector. The performance of the proposed method is demonstrated by experiments on both synthetic datasets and image datasets. The experimental results demonstrate the performance of the proposed method with incompleted matrices distrubed by sparse noise.}
}
@article{UCHIGASAKI2023109696,
title = {Deep image compression using scene text quality assessment},
journal = {Pattern Recognition},
volume = {142},
pages = {109696},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109696},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003941},
author = {Shohei Uchigasaki and Tomo Miyazaki and Shinichiro Omachi},
keywords = {Image compression, Scene text image, Text quality, Quality assessment, Regression model},
abstract = {Image compression is a fundamental technology for Internet communication engineering. However, a high compression rate with general methods may degrade images, resulting in unreadable texts. In this paper, we propose an image compression method for maintaining text quality. We developed a scene text image quality assessment model to assess text quality in compressed images. The assessment model iteratively searches for the best-compressed image holding high-quality text. Objective and subjective results showed that the proposed method was superior to existing methods. Furthermore, the proposed assessment model outperformed other deep-learning regression models.}
}
@article{ALI2023109641,
title = {A k nearest neighbour ensemble via extended neighbourhood rule and feature subsets},
journal = {Pattern Recognition},
volume = {142},
pages = {109641},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109641},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003424},
author = {Amjad Ali and Muhammad Hamraz and Naz Gul and Dost Muhammad Khan and Saeed Aldahmani and Zardad Khan},
keywords = {Features subset, Nearest Neighbours Rule, NN Ensemble, Classification},
abstract = {kNN based ensemble methods minimise the effect of outliers by identifying a set of data points in the given feature space that are nearest to an unseen observation in order to predict its response by using majority voting. The ordinary ensembles based on kNN find out the k nearest observations in a region (bounded by a sphere) based on a predefined value of k. This scenario, however, might not work in situations where the test observation follows the pattern of the closest data points with the same class that lie on a certain path not contained in the given sphere. This paper proposes a k nearest neighbour ensemble where the neighbours are determined in k steps. Starting from the first nearest observation of the test point, the algorithm identifies a single observation that is closest to the observation at the previous step. At each base learner in the ensemble, this search is extended to k steps on a random bootstrap sample with a random subset of features selected from the feature space. The final predicted class of the test point is determined by using a majority vote in the predicted classes given by all base models. This new ensemble method is applied on 20 benchmark datasets and compared with other classical methods, including kNN based models, in terms of classification accuracy, kappa and Brier score as performance metrics. Boxplots are also utilised to illustrate the difference in the results given by the proposed and other state-of-the-art methods. The proposed method outperformed the considered classical methods in the majority of cases. The proposed method is further assessed through a detailed simulation study.}
}
@article{KORBAN2023109713,
title = {A Multi-Modal Transformer network for action detection},
journal = {Pattern Recognition},
volume = {142},
pages = {109713},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109713},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004119},
author = {Matthew Korban and Peter Youngs and Scott T. Acton},
keywords = {Action detection, Transformer network, Optical flow, Motion features},
abstract = {This paper proposes a novel multi-modal transformer network for detecting actions in untrimmed videos. To enrich the action features, our transformer network utilizes a new multi-modal attention mechanism that computes the correlations between different spatial and motion modalities combinations. Exploring such correlations for actions has not been attempted previously. To use the motion and spatial modality more effectively, we suggest an algorithm that corrects the motion distortion caused by camera movement. Such motion distortion, common in untrimmed videos, severely reduces the expressive power of motion features such as optical flow fields. Our proposed algorithm outperforms the state-of-the-art methods on two public benchmarks, THUMOS14 and ActivityNet. We also conducted comparative experiments on our new instructional activity dataset, including a large set of challenging classroom videos captured from elementary schools.}
}
@article{SUGAHARA2023109657,
title = {Hierarchical co-clustering with augmented matrices from external domains},
journal = {Pattern Recognition},
volume = {142},
pages = {109657},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109657},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003588},
author = {Kai Sugahara and Kazushi Okamoto},
keywords = {Hierarchical co-clustering, Transfer learning, Relational data analysis, Unsupervised machine learning, Information theory, Mutual information},
abstract = {Co-clustering simultaneously classifies row and column objects of data matrices and is considered to have better accuracy than conventional one-way clustering methods. In the era of big data, extracting classification knowledge about objects from several domains has become increasingly feasible. This study proposes a hierarchical co-clustering with augmented matrices (HICCAM), which co-clusters the row and column objects of a target matrix while utilizing the augmented data matrices of these target objects extracted from the external domains. The algorithm is designed to improve classification accuracy by transferring knowledge in augmented matrices and simultaneously improves cluster interpretability using hierarchical cluster structures. Experiments on document clustering confirmed that HICCAM achieved the highest accuracy among comparison methods with and without external knowledge. Its clusters exhibit hierarchical relationships according to their topics. In addition, we provide the experimental results with multiview synthetic datasets that demonstrate a clustering situation in which HICCAM can be effectively identified.}
}
@article{SONG2023109688,
title = {A new methodology in constructing no-reference focus quality assessment metrics},
journal = {Pattern Recognition},
volume = {142},
pages = {109688},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109688},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003862},
author = {Jie Song and Mengjun Liu},
keywords = {Autofocus, Microscopy image analysis, No-reference image quality assessment},
abstract = {This paper proposed a new methodology which converts a full-reference focus quality assessment metric into a no-reference one. The methodology consists of three hypotheses which describe the relationship in focus quality between the original image and its variants. Using the proposed methodology, two no-reference metrics were constructed. The first used Brenner Gradient and the second used a full-reference metric proposed by ourselves. Evaluation was conducted on a public dataset and our own proposed dataset. Comparing with other no-reference metrics, our second one exhibited best performance on both datasets, with calculation time comparable to some fastest metrics considered.}
}
@article{ZHANG2023109633,
title = {Dual-branch spatio-temporal graph neural networks for pedestrian trajectory prediction},
journal = {Pattern Recognition},
volume = {142},
pages = {109633},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109633},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003345},
author = {Xingchen Zhang and Panagiotis Angeloudis and Yiannis Demiris},
keywords = {Pedestrian trajectory prediction, Social interactions, Graph convolutional networks, Graph attention networks, Spatio-temporal graph},
abstract = {Pedestrian trajectory prediction is an important area in computer vision, with wide applications in autonomous driving, robot path planning, and surveillance systems. The core underlying technique of these applications is pattern recognition. A key challenge in this area is modeling social interactions between pedestrians, such as pedestrian view area and group behaviors. However, although many methods have been proposed to model social interactions, pedestrian view area and group behaviors have not been explored together to account for complex situations. Additionally, most existing studies require additional detectors and manual annotations to handle view area and group interactions, respectively. In this paper, we propose a dual-branch spatio-temporal graph neural network to automatically model view area and grouping together. Specifically, a spatio-temporal graph attention network (STGAT) branch is designed to handle pedestrian view area, and a spatio-temporal graph convolutional network (STGCN) branch is designed to model group interactions. The features of these branches are then fused to provide better feature representations, on which a temporal convolution operation (TCN) is performed for trajectory prediction. Experiments on public standard datasets demonstrate that the proposed method achieves very competitive performance and predicts socially acceptable trajectories in different challenging scenarios.}
}
@article{CHEN2023109677,
title = {Multi-semantic hypergraph neural network for effective few-shot learning},
journal = {Pattern Recognition},
volume = {142},
pages = {109677},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109677},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003783},
author = {Hao Chen and Linyan Li and Fuyuan Hu and Fan Lyu and Liuqing Zhao and Kaizhu Huang and Wei Feng and Zhenping Xia},
keywords = {Hypergraph, Few-shot learning, Multi-semantic learning, Orthogonal training},
abstract = {Recently, Graph-based Few-Shot Learning (FSL) methods exhibit good generalization by mining relations among few samples with Graph Neural Networks. However, most Graph-based FSL methods consider only binary relations and ignore the multi-semantic information of the global context knowledge. We propose a framework of Multi-Semantic Hypergraph for FSL (MSH-FSL) to explore complex latent high-order multi-semantic relations among the few samples. By mining the complex relationship structure of multi-node and multi-semantics, more refined feature representation can be learned, which yields better classification robustness. Specifically, we first construct a novel Multi-Semantic Hypergraph by obtaining associated instances with different semantic features via orthogonal mapping. With the constructed hypergraph, we then develop the Hyergraph Neural Network along with a novel multi-generation hypergraph message passing so as to better leverage the complex latent semantic relations among samples. Finally, after a number of generations, the hyper-node representations embedded in the learned hypergraph become more accurate for obtaining few-shot prediction. In the 5-way 1-shot task of ResNet-12 on mini-Imagenet dataset, the multi-semantic hypergraph outperforms single-semantic graph by 3.1%, and with the proposed semantic-distribution message passing, the improvement can further reach 6.1%.}
}
@article{ZOU2023109653,
title = {Adaptive reweighted quaternion sparse learning for data recovery and classification},
journal = {Pattern Recognition},
volume = {142},
pages = {109653},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109653},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003540},
author = {Cuiming Zou and Kit Ian Kou and Yuan Yan Tang and Hao Deng},
keywords = {Quaternion sparse representation, Weight learning, Supervised learning},
abstract = {Sparse representation (SR) methods in quaternion space have been attracting increasing interests recently. However, most existing quaternion SR methods adopt the quaternion ℓ1 norm, which penalizes all the entries of the quaternion sparse vector equally and ignores the differences and significance of different entries. Ideally, the entries with large magnitude should be less penalized while those with small magnitude (such as zero entries) should be more penalized. Therefore, we propose an Adaptive Weighted Quaternion Sparse Representation (AWQSR) method in this paper, which can learn weights for distinct entries of the quaternion sparse entries in an adaptive manner. Due to the noncommutativity of quaternion multiplication, it is difficult to tackle the resulting optimization problem of AWQSR. For this reason, we devise an effective iteratively reweighted optimization algorithm based on quaternion operators. To further improve the classification performance, we also develop a Supervised AWQSR based Classification (SAWQSRC) method by leveraging the label information of training samples to learn discriminative weights. Theoretical analysis of SAWQSRC has also been established to show that SAWQSRC succeeds in classification under appropriate conditions. The experiments on simulated data and real data prove the validity of the proposed methods for quaternion signal recovery and classification.}
}
@article{VIDAL2023109695,
title = {End-to-End page-Level assessment of handwritten text recognition},
journal = {Pattern Recognition},
volume = {142},
pages = {109695},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109695},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300393X},
author = {Enrique Vidal and Alejandro H. Toselli and Antonio Ríos-Vila and Jorge Calvo-Zaragoza},
keywords = {Handwritten text recognition, Full-Page end-to-End text image transcription, Layout analysis, Text line detection, Reading order, Evaluation measures, Bag of words, Regularised hungarian algorithm, Word error rate},
abstract = {The evaluation of Handwritten Text Recognition (HTR) systems has traditionally used metrics based on the edit distance between HTR and ground truth (GT) transcripts, at both the character and word levels. This is very adequate when the experimental protocol assumes that both GT and HTR text lines are the same, which allows edit distances to be independently computed to each given line. Driven by recent advances in pattern recognition, HTR systems increasingly face the end-to-end page-level transcription of a document, where the precision of locating the different text lines and their corresponding reading order (RO) play a key role. In such a case, the standard metrics do not take into account the inconsistencies that might appear. In this paper, the problem of evaluating HTR systems at the page level is introduced in detail. We analyse the convenience of using a two-fold evaluation, where the transcription accuracy and the RO goodness are considered separately. Different alternatives are proposed, analysed and empirically compared both through partially simulated and through real, full end-to-end experiments. Results support the validity of the proposed two-fold evaluation approach. An important conclusion is that such an evaluation can be adequately achieved by just two simple and well-known metrics: the Word Error Rate (WER), that takes transcription sequentiality into account, and the here re-formulated Bag of Words Word Error Rate (bWER), that ignores order. While the latter directly and very accurately assess intrinsic word recognition errors, the difference between both metrics (ΔWER) gracefully correlates with the Normalised Spearman’s Foot Rule Distance (NSFD), a metric which explicitly measures RO errors associated with layout analysis flaws. To arrive to these conclusions, we have introduced another metric called Hungarian Word Word Rate (hWER), based on a here proposed regularised version of the Hungarian Algorithm. This metric is shown to be always almost identical to bWER and both bWER and hWER are also almost identical to WER whenever HTR transcripts and GT references are guarantee to be in the same RO.}
}
@article{KANG2023109692,
title = {Self-paced principal component analysis},
journal = {Pattern Recognition},
volume = {142},
pages = {109692},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109692},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003904},
author = {Zhao Kang and Hongfei Liu and Jiangxin Li and Xiaofeng Zhu and Ling Tian},
keywords = {Dimension reduction, Outliers, Manifold learning},
abstract = {Principal Component Analysis (PCA) has been widely used for dimensionality reduction and feature extraction. Robust PCA (RPCA), under different robust distance metrics, such as ℓ1-norm and ℓ2,p-norm, can deal with noise or outliers to some extent. However, real-world data may display structures that can not be fully captured by these simple functions. In addition, existing methods treat complex and simple samples equally. By contrast, a learning pattern typically adopted by human beings is to learn from simple to complex and less to more. Based on this principle, we propose a novel method called Self-paced PCA (SPCA) to further reduce the effect of noise and outliers. Notably, the complexity of each sample is calculated at the beginning of each iteration in order to integrate samples from simple to more complex into training. Based on an alternating optimization, SPCA finds an optimal projection matrix and filters out outliers iteratively. Theoretical analysis is presented to show the rationality of SPCA. Extensive experiments on popular data sets demonstrate that the proposed method can improve the state-of-the-art results considerably.}
}
@article{YING2023109717,
title = {Region-aware RGB and near-infrared image fusion},
journal = {Pattern Recognition},
volume = {142},
pages = {109717},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109717},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004156},
author = {Jiacheng Ying and Can Tong and Zehua Sheng and Bowen Yao and Si-Yuan Cao and Heng Yu and Hui-Liang Shen},
keywords = {Image fusion, RGB and near-infrared, overexposed sky recovery, vegetation enhancement, gradient-domain optimization},
abstract = {This paper proposes a region-aware fusion method, called RaIF, for RGB and near-infrared (NIR) outdoor scenery image fusion. The method is motivated by the observation that current fusion approaches produce gray appearance in overexposed sky regions and distortion in vegetation regions. RaIF generates the region probability maps by exploiting their specific characteristics in the visible and NIR spectra. It recovers the overexposed sky regions by employing the intrinsic channel correlation between RGB and NIR images, and enhances the vegetation regions in an adjustable manner. RaIF formulates image fusion problem as a gradient-domain optimization problem with luminance and chromaticity regularizations. Experimental results validate the superiority of RaIF that produces fused images with improved appearance in the sky and vegetation regions, and achieves the state-of-the-art performance quantitatively and qualitatively. Furthermore, RaIF can act as a refinement module that improves the fusion results of current deep learning based approaches. It is also capable of recovering specular highlight regions other than sky overexposure.}
}
@article{LI2023109684,
title = {Truncated attention-aware proposal networks with multi-scale dilation for temporal action detection},
journal = {Pattern Recognition},
volume = {142},
pages = {109684},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109684},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003825},
author = {Ping Li and Jiachen Cao and Li Yuan and Qinghao Ye and Xianghua Xu},
keywords = {Temporal action detection, Attention mechanism, Graph convolution, Multi-scale dilation, Proposal network},
abstract = {Detecting actions temporally in untrimmed videos is very challenging, and it accomplishes action classification and localization simultaneously. Capturing the relations among action proposals (i.e., candidate video segments) is of vital importance. While there have been several attempts to encode such relations, they neglect the adverse effects of those irrelevant or negative relations among proposals. Besides, there is a crucial fact that action durations are flexible in videos, which has not been well explored. For the former, we develop a truncated attention mechanism that learns positive proposal relations by dynamically adjusting edge weights of proposal nodes in a graph, and construct the proposal network model using graph convolution networks to suppress disadvantageous relations of proposal pairs by truncating negative attention scores. For the latter, we devise a light multi-scale dilation module shared by all proposals to handle different action durations by enlarging temporal receptive field, thus capturing temporal context to increase the representation capacity of proposals. Unifying these considerations, we present the Multi-scale Dilation based Truncated Attention Proposal Network (MD-TAPN) model for temporal action detection. Our model achieves state-of-the-art performances of detecting actions on two benchmark databases, and especially it outperforms the most competitive method by a significant gain of 3.6% mAP at tIoU0.5 on THUMOS14.}
}
@article{CHENG2023109629,
title = {TAT: Targeted backdoor attacks against visual object tracking},
journal = {Pattern Recognition},
volume = {142},
pages = {109629},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109629},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003308},
author = {Ziyi Cheng and Baoyuan Wu and Zhenya Zhang and Jianjun Zhao},
keywords = {Backdoor attack, Visual object tracking, Targeted attack},
abstract = {Visual object tracking (VOT) is a fundamental computer vision task that aims to track a target in a sequence of video frames. It has been broadly adopted in safety- and security-critical applications, such as self-driving systems and traffic control systems. However, the VOT models (i.e., the trackers) that rely on third-party training resources face a severe threat of backdoor attacks, which refer to the type of the attacks that poison a portion of training data and mislead the tracker to track a wrong target. A surge of research interest has arisen in backdoor attacks in the domain of image classification, as a measure to expose the potential security risks of the classifiers and inspire new defense techniques. Despite the prosperity of the research in backdoor attacks in image classification, there still lacks investigation in backdoor attacks against VOT, due to their unique challenges: first, the architecture of a VOT model is much more complicated than that of an image classifier; second, VOT targets a sequence of video frames rather than individual images. To bridge the gap, we propose a novel and effective targeted backdoor attack approach TAT specifically against VOT tasks. In particular, TAT includes a basic version TAT-BA that can achieve effective and stealthy backdoor attacks against VOT trackers, and an advanced version TAT-DA that can evade two representative defense techniques. Our large-scale experimental evaluation demonstrates the effectiveness and the stealthiness of TAT. Moreover, we also demonstrate the performances of TAT-BA under real-world settings and the abilities of TAT-DA to counter defense techniques. The code will be available at https://github.com/MisakaZipi/TAT.}
}
@article{ZHANG2023109725,
title = {Interpreting vulnerabilities of multi-instance learning to adversarial perturbations},
journal = {Pattern Recognition},
volume = {142},
pages = {109725},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109725},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004235},
author = {Yu-Xuan Zhang and Hua Meng and Xue-Mei Cao and Zhengchun Zhou and Mei Yang and Avik Ranjan Adhikary},
keywords = {Customized perturbation, Multi-instance learning, Universal perturbation, Vulnerability},
abstract = {Multi-instance learning (MIL) is a recent machine learning paradigm which is immensely useful in various real-life applications, like image analysis, video anomaly detection, text classification, etc. It is well known that most of the existing machine learning classifiers are highly vulnerable to adversarial perturbations. Since MIL is a weakly supervised learning, where information is available for a set of instances, called bag and not for every instance, adversarial perturbations can be fatal. In this paper, we have proposed two adversarial perturbation methods to analyze the effect of adversarial perturbations to interpret the vulnerabilities of MIL methods. Out of the two algorithms, one can be customized for every bag, and the other is a universal one, which can affect all bags in a given data set and thus has some generalizability. Furthermore, through simulations, we have demonstrated the efficacy of the proposed algorithms in fooling state-of-the-art MIL approaches, such that these models make incorrect predictions regarding the label assigned to the bag. Finally, we have discussed, through experiments, about taking care of these kind of adversarial perturbations through a simple strategy. Source codes are available at https://github.com/InkiInki/MI-UAP.}
}
@article{SU2023109700,
title = {Physical model and image translation fused network for single-image dehazing},
journal = {Pattern Recognition},
volume = {142},
pages = {109700},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109700},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003989},
author = {Yan Zhao Su and Chuan He and Zhi Gao Cui and Ai Hua Li and Nian Wang},
keywords = {Single-image dehazing, Image translation, Physical model, Feature fusion},
abstract = {The visibility and contrast of images captured in adverse weather such as haze or fog degrade dramatically, which further hinders the accomplishment of high-level computer vision tasks such as object detection and semantic segmentation in these conditions. Many methods have been proposed to solve image dehazing problem by using image translation networks or physical model embedding in CNNs. However, the physical model cannot effectively describe the hazy generation process in complex scenes and estimating the model parameters with only a hazy image is an ill-posed problem. Image translation-based methods may lead to artefacts or colour shifts in the recovered results without the guidance or constraints of physical model information. In this paper, an end-to-end physical model and image translation fused network is proposed to generate realistic haze-free images. Since the transmission map can express the haze distribution in the scene, the proposed method adopts an encoder with a multiscale residual block to extract hazy image features, and two separate decoders to recover a clear image and to estimate the transmission map. The multiscale features of the transmission map and image translation are fused to guide the decode processes with a conditional attention feature fusion block, which is composed of sequential channelwise and spatialwise attention. Moreover, a multitask and multiscale deep supervision mechanism is adopted to enhance the feature fusion and recover more image details. The algorithm can efficiently fuse the physical model information and the hazy image translation to address the problem existent in the methods only based on physical model embedding or direct image translation. Experimental results on the visual quality enhancement of hazy images and semantic segmentation tasks in hazy scenes demonstrate that our model can efficiently recover haze-free images, while performing on par with state-of-the-art methods.}
}
@article{GIRAUD2023109673,
title = {Generalization of the shortest path approach for superpixel segmentation of omnidirectional images},
journal = {Pattern Recognition},
volume = {142},
pages = {109673},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109673},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003746},
author = {Rémi Giraud and Rodrigo {Borba Pinheiro} and Yannick Berthoumieu},
keywords = {3D Spherical images, Superpixels, Unsupervised segmentation, Shape regularity},
abstract = {With the growing use of image capture devices using wide angles and the need for fast and accurate image analysis in computer vision, there is a demand for dedicated under-representation methods. Most decomposition methods segment an image into a small number of irregular homogeneous regions, called superpixels. Nevertheless, these approaches are generally designed to process natural 2D planar images, i.e., captured with a 90o angle view without distortion. In this work, we present SphSPS, a new general decomposition method (for Spherical Shortest Path-based Superpixels)11Available code at: https://github.com/rgiraud/sphsps, that is dedicated to wide 360o omnidirectional or spherical images. The produced superpixels respect both the geometry of the 3D spherical acquisition space, and the boundaries of image objects. To fastly extract relevant clustering features, we generalize the shortest path approach between a pixel and a superpixel center. We demonstrate that considering the geometry of the acquisition space to compute the shortest path enables to jointly improve the segmentation accuracy and the shape regularity of superpixels. To evaluate this regularity property, we propose a generalization of a standard 2D regularity metric to the spherical space, addressing the limitations of the only existing spherical compactness measure. Finally, SphSPS is validated on reference 360o images from the PSD (Panorama Segmentation Dataset) and also on synthetic road omnidirectional images. Our method significantly outperforms both planar and spherical state-of-the-art approaches in terms of segmentation accuracy, robustness to noise and regularity, providing a very interesting tool for superpixel-based applications on 360o images.}
}
@article{DU2023109644,
title = {Dual-channel embedding learning model for partially labeled attributed networks},
journal = {Pattern Recognition},
volume = {142},
pages = {109644},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109644},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300345X},
author = {Hangyuan Du and Wenjian Wang and Liang Bai},
keywords = {Partially labeled attributed networks, Network embedding, Mutual information, Graph convolution networks, Information redundancy},
abstract = {Network embedding is an important fundamental work in many network application tasks, which encodes the input network from the high-dimensional and sparse topological space into a low-dimensional and dense vector space. Recently, there has been a growing interest in embedding learning on Partially Labeled Attributed Networks (PLANs) due to the increasing occurrence of node attributes and partially available category labels in real-world networks. Semi-supervised embedding learning is a standard approach employed in PLANs, utilizing category labels to supervise the learning process. However, the semi-supervised learning procedure can fail when labels are scarce, noisy, or unreliable. Additionally, most existing embedding algorithms have not successfully integrated heterogeneous information, such as labels, attributes, and structure. To address these issues, we develop a new model, the Dual-Channel Network Embedding (DcNE), which integrates different types of network information into embeddings from a mutual information (MI) perspective. Specifically, we construct a dual-channel information propagation framework to encode the input network in semi-supervised and self-supervised learning paradigms in parallel. Furthermore, a redundancy elimination module is implemented to capture and eliminate the redundant information between the two encoders. Finally, we propose a unified optimization model that integrates the two learning paradigms to collaborate effectively. In the experiments, we demonstrate the effectiveness of DcNE in various network analysis tasks using real-world datasets, establishing its superiority over state-of-the-art baselines.}
}