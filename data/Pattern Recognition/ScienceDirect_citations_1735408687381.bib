@article{2023109492,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {138},
pages = {109492},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(23)00192-9},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001929}
}
@article{PENG2023109370,
title = {Learning efficient facial landmark model for human attractiveness analysis},
journal = {Pattern Recognition},
volume = {138},
pages = {109370},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109370},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000717},
author = {Tianhao Peng and Mu Li and Fangmei Chen and Yong Xu and David Zhang},
keywords = {Human attractiveness, Landmark model, Geometric feature, Genetic algorithm},
abstract = {Existing geometric features on facial attractiveness analysis only focus on the ratios and distances, which is incomplete to represent all the information of a face. In this paper, we introduce a new category of feature, i.e., the angle features, to describe the angle of different organs such as the chin and eyes, which help boost the analysis performance in experiment. In addition, existing facial beauty analysis papers usually apply existing landmark models and extract their own different geometric feature sets on the landmarks. On the one hand, the geometric features are quite chaotic between different papers. On the other hand, most of the landmarks in the existing landmark model are useless for geometric feature extraction which wastes a lot of computational resources. To tackle these issues, we suggest to define a common geometric feature set and learn a special landmark model for attractiveness analysis. Specially, we collect all the available geometric features from the previous jobs and introduce a genetic feature selection algorithm to select the most effective geometric features. Furthermore, we introduce a special landmark model which exactly covers all the extracted geometric features. The experiments show that our method with the introduced angle features and the common feature set can outperform state-of-art facial beauty estimation methods with geometric features.}
}
@article{ZHONG2023109349,
title = {Self-taught Multi-view Spectral Clustering},
journal = {Pattern Recognition},
volume = {138},
pages = {109349},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109349},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300050X},
author = {Guo Zhong and Chi-Man Pun},
keywords = {Graph clustering, spectral rotation, spectral clustering, multi-view clustering},
abstract = {By integrating multiple views, i.e., multi-view learning (ML), we can discover the underlying data structures so that the performance of learning tasks can improve. As a basic and important branch of ML, multi-view clustering has achieved great success recently in pattern recognition and machine learning communities. Most existing multi-view spectral clustering methods heavily adopt the relax-and-discretize strategy to obtain discrete cluster labels (clustering results), i.e., using predefined similarity graphs to learn a consensus Laplacian embedding shared by all views for K-means clustering. However, the above clustering strategy may significantly affect clustering performance since there is information loss between independent steps. In this paper, we establish a novel Self-taught Multi-view Spectral Clustering (SMSC) framework to address the above issue. As the main contributions of this paper, we provide two versions of SMSC based on convex combination and centroid graph fusion schemes. Specifically, a self-taught mechanism is introduced in SMSC, which can effectively feedback the manifold structure induced by Laplacian embedding and the cluster information hidden in the discrete indicator matrix to learn an optimal consensus similarity graph for graph partitioning. The effectiveness of the proposed methods has been evaluated on real-world multi-view datasets, and experimental results show that our methods outperform other state-of-the-art baselines.}
}
@article{PARK2023109387,
title = {Elucidating robust learning with uncertainty-aware corruption pattern estimation},
journal = {Pattern Recognition},
volume = {138},
pages = {109387},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109387},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000882},
author = {Jeongeun Park and Seungyoun Shin and Sangheum Hwang and Sungjoon Choi},
keywords = {Robust learning, Training with noisy labels, Uncertainty estimation, Corruption pattern estimation},
abstract = {Robust learning methods aim to learn a clean target distribution from noisy and corrupted training data where a specific corruption pattern is often assumed a priori. Our proposed method can not only successfully learn the clean target distribution from a dirty dataset but also can estimate the underlying noise pattern. To this end, we leverage a mixture-of-experts model that can distinguish two different types of predictive uncertainty, aleatoric and epistemic uncertainty. We show that the ability to estimate the uncertainty plays a significant role in elucidating the corruption patterns as these two objectives are tightly intertwined. We also present a novel validation scheme for evaluating the performance of the corruption pattern estimation. Our proposed method is extensively assessed in terms of both robustness and corruption pattern estimation in the computer vision domain. Code has been made publicly available at https://github.com/jeongeun980906/Uncertainty-Aware-Robust-Learning.}
}
@article{WANG2023109335,
title = {Memory-augmented appearance-motion network for video anomaly detection},
journal = {Pattern Recognition},
volume = {138},
pages = {109335},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109335},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000365},
author = {Le Wang and Junwen Tian and Sanping Zhou and Haoyue Shi and Gang Hua},
keywords = {Anomaly detection, Memory network, Autoencoder, Abnormal events},
abstract = {Video anomaly detection is a promising yet challenging task, where only normal events are observed in the training phase. Without any explicit classification boundary between normal and abnormal events, anomaly detection can be turned into an outlier detection problem by regarding any event that does not conform to the normal patterns as an anomaly. Most of the existing works mainly focus on improving the representation of normal events, while ignore the relationship between normal and abnormal events. Besides, the lack of restrictions on classification boundaries also leads to performance degradation. To address the above problems, we design a novel autoencoder-based Memory-Augmented Appearance-Motion Network (MAAM-Net), which consists of a novel end-to-end network to learn appearance and motion feature of a given input frame, a fused memory module to build a bridge for normal and abnormal events, a well-designed margin-based latent loss to relieve the computation costs, and a pointed Patch-based Stride Convolutional Detection (PSCD) algorithm to eliminate the degradation phenomenon. Specifically, the memory module is embedded between the encoder and decoder, which serves as a sparse dictionary of normal patterns, therefore it can be further employed to reintegrate abnormal events during inference. To further distort the reintegration quality of abnormal events, the margin-based latent loss is leveraged to enforce the memory module to select a sparse set of critical memory items. Last but not least, the simple yet effective detection method focuses on patches rather than the overall frame responses, which can benefit from the distortion of abnormal events. Extensive experiments and ablation studies on three anomaly detection benchmarks, i.e., UCSD Ped2, CUHK Avenue, and ShanghaiTech, demonstrate the effectiveness and efficiency of our proposed MAAM-Net. Notably, we achieve superior AUC performances on UCSD Ped2 (0.977), CHUK Avenue (0.909), and ShanghaiTech (0.713). The code is publicly available at https://github.com/Owen-Tian/MAAM-Net.}
}
@article{YAN2023109432,
title = {3D Medical image segmentation using parallel transformers},
journal = {Pattern Recognition},
volume = {138},
pages = {109432},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109432},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001334},
author = {Qingsen Yan and Shengqiang Liu and Songhua Xu and Caixia Dong and Zongfang Li and Javen Qinfeng Shi and Yanning Zhang and Duwei Dai},
keywords = {3D Medical image segmentation, Deep learning, Transformers, Attention, Fusion, High-resolution representations, Low-resolution representations},
abstract = {Most recent 3D medical image segmentation methods adopt convolutional neural networks (CNNs) that rely on deep feature representation and achieve adequate performance. However, due to the convolutional architectures having limited receptive fields, they cannot explicitly model the long-range dependencies in the medical image. Recently, Transformer can benefit from global dependencies using self-attention mechanisms and learn highly expressive representations. Some works were designed based on the Transformers, but the existing Transformers suffer from extreme computational and memories, and they cannot take full advantage of the powerful feature representations in 3D medical image segmentation. In this paper, we aim to connect the different resolution streams in parallel and propose a novel network, named Transformer based High Resolution Network (TransHRNet), with an Effective Transformer (EffTrans) block, which has sufficient feature representation even at high feature resolutions. Given a 3D image, the encoder first utilizes CNN to extract the feature representations to capture the local information, and then the different feature maps are reshaped elaborately for tokens that are fed into each Transformer stream in parallel to learn the global information and repeatedly exchange the information across streams. Unfortunately, the proposed framework based on the standard Transformer needs a huge amount of computation, thus we introduce a deep and effective Transformer to deliver better performance with fewer parameters. The proposed TransHRNet is evaluated on the Multi-Atlas Labeling Beyond the Cranial Vault (BCV) dataset that consists of 11 major human organs and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Experimental results show that it performs better than the convolutional and other related Transformer-based methods on the 3D multi-organ segmentation tasks. Code is available at https://github.com/duweidai/TransHRNet.}
}
@article{ZHANG2023109400,
title = {Learning from multiple annotators for medical image segmentation},
journal = {Pattern Recognition},
volume = {138},
pages = {109400},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109400},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001012},
author = {Le Zhang and Ryutaro Tanno and Moucheng Xu and Yawen Huang and Kevin Bronik and Chen Jin and Joseph Jacob and Yefeng Zheng and Ling Shao and Olga Ciccarelli and Frederik Barkhof and Daniel C. Alexander},
keywords = {Multi-Annotator, Label fusion, Segmentation},
abstract = {Supervised machine learning methods have been widely developed for segmentation tasks in recent years. However, the quality of labels has high impact on the predictive performance of these algorithms. This issue is particularly acute in the medical image domain, where both the cost of annotation and the inter-observer variability are high. Different human experts contribute estimates of the ”actual” segmentation labels in a typical label acquisition process, influenced by their personal biases and competency levels. The performance of automatic segmentation algorithms is limited when these noisy labels are used as the expert consensus label. In this work, we use two coupled CNNs to jointly learn, from purely noisy observations alone, the reliability of individual annotators and the expert consensus label distributions. The separation of the two is achieved by maximally describing the annotator’s “unreliable behavior” (we call it “maximally unreliable”) while achieving high fidelity with the noisy training data. We first create a toy segmentation dataset using MNIST and investigate the properties of the proposed algorithm. We then use three public medical imaging segmentation datasets to demonstrate our method’s efficacy, including both simulated (where necessary) and real-world annotations: 1) ISBI2015 (multiple-sclerosis lesions); 2) BraTS (brain tumors); 3) LIDC-IDRI (lung abnormalities). Finally, we create a real-world multiple sclerosis lesion dataset (QSMSC at UCL: Queen Square Multiple Sclerosis Center at UCL, UK) with manual segmentations from 4 different annotators (3 radiologists with different level skills and 1 expert to generate the expert consensus label). In all datasets, our method consistently outperforms competing methods and relevant baselines, especially when the number of annotations is small and the amount of disagreement is large. The studies also reveal that the system is capable of capturing the complicated spatial characteristics of annotators’ mistakes.}
}
@article{ZHANG2023109424,
title = {ThumbDet: One thumbnail image is enough for object detection},
journal = {Pattern Recognition},
volume = {138},
pages = {109424},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109424},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001255},
author = {Yongqiang Zhang and Yin Zhang and Rui Tian and Zian Zhang and Yancheng Bai and Wangmeng Zuo and Mingli Ding},
keywords = {Object detection, Down-sampling network, Knowledge distillation},
abstract = {Computer vision fields have witnessed great success thanks to deep convolutional neural networks (CNNs). However, state-of-the-art methods often benefit from large models and datasets, which introduce heavy parameters and computational requirements. Deploying such large models in real-world applications is very difficult because of the limited computing resources. Although many researchers focus on designing efficient block structures to compress model parameters, they ignore that the role of large-scale input images is also an important factor for algorithm efficiency. Reducing input resolution is a useful method to boost runtime efficiency, however, traditional interpolation methods assume a fixed degradation criterion that greatly hurts performance. To solve the above problems, in this paper, we propose a novel framework named ThumbDet for reducing model computation while maintaining detection accuracy. In our framework, we first design an image down-sampling module to learn a small-scale image that looks realistic and contains discriminative properties. Furthermore, we propose a distillation-boost supervision strategy to maintain the detection performance of small-scaled images as the original-size inputs. Extensive experiments conducted on a standard object detection dataset MS COCO demonstrate the effectiveness of the proposed method when using very low-resolution images (i.e. 4× down-sampling) as inputs. In particular, ThumbDet achieves satisfactory detection performance (i.e. 32.3% in mAP) while drastically reducing computation and memory requirements (i.e. speed up of 1.26×), outperforming the traditional interpolation methods (e.g. bicubic) by +3.2% absolutely in terms of mAP.}
}
@article{LIN2023109416,
title = {Cycle-object consistency for image-to-image domain adaptation},
journal = {Pattern Recognition},
volume = {138},
pages = {109416},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109416},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001176},
author = {Che-Tsung Lin and Jie-Long Kew and Chee Seng Chan and Shang-Hong Lai and Christopher Zach},
keywords = {Generative adversarial networks, Instance-aware image-translation, Domain adaptation, Cross-domain object detection},
abstract = {Recent advances in generative adversarial networks (GANs) have been proven effective in performing domain adaptation for object detectors through data augmentation. While GANs are exceptionally successful, those methods that can preserve objects well in the image-to-image translation task usually require an auxiliary task, such as semantic segmentation to prevent the image content from being too distorted. However, pixel-level annotations are difficult to obtain in practice. Alternatively, instance-aware image-translation model treats object instances and background separately. Yet, it requires object detectors at test time, assuming that off-the-shelf detectors work well in both domains. In this work, we present AugGAN-Det, which introduces Cycle-object Consistency (CoCo) loss to generate instance-aware translated images across complex domains. The object detector of the target domain is directly leveraged in generator training and guides the preserved objects in the translated images to carry target-domain appearances. Compared to previous models, which e.g., require pixel-level semantic segmentation to force the latent distribution to be object-preserving, this work only needs bounding box annotations which are significantly easier to acquire. Next, as to the instance-aware GAN models, our model, AugGAN-Det, internalizes global and object style-transfer without explicitly aligning the instance features. Most importantly, a detector is not required at test time. Experimental results demonstrate that our model outperforms recent object-preserving and instance-level models and achieves state-of-the-art detection accuracy and visual perceptual quality.}
}
@article{BREITENBACH2023109355,
title = {On a method for detecting periods and repeating patterns in time series data with autocorrelation and function approximation},
journal = {Pattern Recognition},
volume = {138},
pages = {109355},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109355},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000560},
author = {Tim Breitenbach and Bartosz Wilkusz and Lauritz Rasbach and Patrick Jahnke},
keywords = {Time series analysis, Time series modelling, Seasonality detection, Period detection, Time series decomposition},
abstract = {Detecting recurrent patterns in time series data is an important capability. The reason is that repeating patterns on the one hand indicate well defined processes that can be further analyzed once detected and on the other hand are a reliable feature to predict future occurrences and adapt accordingly. The challenge in real data to define a period is that a time series is usually also influenced by non-periodic dynamics and noise. In this work, a mathematical framework is proved to define regular patterns. Their properties are used within a suggested algorithm based on the concept of autocorrelation and function approximation to fit a model capturing the periodic part of the time series. Based on that model and a corresponding autocorrelation, a new score is defined to evaluate how well a hypothesized period fits to the time series. This score is particularly useful in a big data scenario where decisions for periodicity are needed to be taken automatically, which is one of the main achievement of the presented work. The period analysis algorithm is applied to data from two different use cases. The first one is a data center scenario where the information of the periodic pattern is used to create a feature that improves a machine learning framework predicting future resource demands. The feature represents the phase of the repeating pattern. In a second scenario, expression data from mice liver cells are investigated concerning periodic rhythms. A Python implementation of the presented algorithm is provided via a github repository under https://github.com/LauritzR/period-detection.}
}
@article{ZHANG2023109339,
title = {Learning visual question answering on controlled semantic noisy labels},
journal = {Pattern Recognition},
volume = {138},
pages = {109339},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109339},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000407},
author = {Haonan Zhang and Pengpeng Zeng and Yuxuan Hu and Jin Qian and Jingkuan Song and Lianli Gao},
keywords = {Visual question answering, Noisy datasets, Semantic labels, Contrastive learning},
abstract = {Visual Question Answering (VQA) has made great progress recently due to the increasing ability to understand and encode multi-modal inputs based on deep learning. However, existing VQA models are usually based on assumptions of clean labels, and it is contradictory to real scenarios where labels are expensive and inevitably contain noises. In this paper, we take the lead in addressing this issue by establishing the first benchmark of controlled semantic noisy labels for VQA task, evaluating existing methods, and coming up with corresponding solutions. Specifically, through analyzing human labels of existing VQA datasets, we first design a controlled semantic label noise by imitating human mislabeling behavior, which is more reasonable than conventional random noise. Then, we evaluate several popular VQA models on these new benchmark datasets and show that their performance degrades significantly compared to the original setting. To this end, we propose a Semantic Noisy Label Correction (SNLC) to mitigate impacts of noisy labels, including a Semantic Cross-Entropy (SCE) loss and a Semantic Embedding Contrastive (SEC) loss. Extensive experiments demonstrate the effectiveness of the proposed method SNLC. The proposed approach achieves a stable improvement on several existing models. The source code is available at https://github.com/zchoi/SNLC.}
}
@article{MA2023109420,
title = {Towards local visual modeling for image captioning},
journal = {Pattern Recognition},
volume = {138},
pages = {109420},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109420},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001218},
author = {Yiwei Ma and Jiayi Ji and Xiaoshuai Sun and Yiyi Zhou and Rongrong Ji},
keywords = {Image captioning, Attention mechanism, Local visual modeling},
abstract = {In this paper, we study the local visual modeling with grid features for image captioning, which is critical for generating accurate and detailed captions. To achieve this target, we propose a Locality-Sensitive Transformer Network (LSTNet) with two novel designs, namely Locality-Sensitive Attention (LSA) and Locality-Sensitive Fusion (LSF). LSA is deployed for the intra-layer interaction in Transformer via modeling the relationship between each grid and its neighbors. It reduces the difficulty of local object recognition during captioning. LSF is used for inter-layer information fusion, which aggregates the information of different encoder layers for cross-layer semantical complementarity. With these two novel designs, the proposed LSTNet can model the local visual information of grid features to improve the captioning quality. To validate LSTNet, we conduct extensive experiments on the competitive MS-COCO benchmark. The experimental results show that LSTNet is not only capable of local visual modeling, but also outperforms a bunch of state-of-the-art captioning models on offline and online testings, i.e., 134.8 CIDEr and 136.3 CIDEr, respectively. Besides, the generalization of LSTNet is also verified on the Flickr8k and Flickr30k datasets. The source code is available on GitHub: https://www.github.com/xmu-xiaoma666/LSTNet.}
}
@article{MARTINS2023109359,
title = {Meta-learning for dynamic tuning of active learning on stream classification},
journal = {Pattern Recognition},
volume = {138},
pages = {109359},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109359},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000602},
author = {Vinicius Eiji Martins and Alberto Cano and Sylvio {Barbon Junior}},
keywords = {Meta-learning, Active learning, Data stream, Concept drift},
abstract = {Supervised data stream learning depends on the incoming sample’s true label to update a classifier’s model. In real life, obtaining the ground truth for each instance is a challenging process; it is highly costly and time consuming. Active Learning has already bridged this gap by finding a reduced set of instances to support the creation of a reliable stream classifier. However, identifying a reduced number of informative instances to support a suitable classifier update and drift adaptation is very tricky. To better adapt to concept drifts using a reduced number of samples, we propose an online tuning of the Uncertainty Sampling threshold using a meta-learning approach. Our approach exploits statistical meta-features from adaptive windows to meta-recommend a suitable threshold to address the trade-off between the number of labelling queries and high accuracy. Experiments exposed that the proposed approach provides the best trade-off between accuracy and query reduction by dynamic tuning the uncertainty threshold using lightweight meta-features.}
}
@article{QIU2023109383,
title = {SATS: Self-attention transfer for continual semantic segmentation},
journal = {Pattern Recognition},
volume = {138},
pages = {109383},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109383},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000845},
author = {Yiqiao Qiu and Yixing Shen and Zhuohao Sun and Yanchong Zheng and Xiaobin Chang and Weishi Zheng and Ruixuan Wang},
keywords = {Continual learning, Semantic segmentation, Self-attention transfer, Class-specific region pooling},
abstract = {Continually learning to segment more and more types of image regions is a desired capability for many intelligent systems. However, such continual semantic segmentation exhibits catastrophic forgetting issues similar to those of continual classification learning. Unlike the existing knowledge distillation strategies for alleviating this problem, transferring a new type of information, namely, the relationships between elements (e.g., pixels) within each image that can capture both within-class and between-class knowledge, is proposed in this study. Such information can be effectively obtained from self-attention maps in a Transformer-style segmentation model. Considering that pixels belonging to the same class in each image typically share similar visual properties, a class-specific region pooling operator is novelly applied to provide reliable relationship information for knowledge transfer. Extensive evaluations on multiple public benchmarks reveal that the proposed self-attention transfer method can effectively alleviate the catastrophic forgetting issue. Furthermore, flexible combinations of the proposed method with widely adopted strategies considerably outperform state-of-the-art solutions.}
}
@article{NGUYEN2023109351,
title = {On a linear fused Gromov-Wasserstein distance for graph structured data},
journal = {Pattern Recognition},
volume = {138},
pages = {109351},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109351},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000523},
author = {Dai Hai Nguyen and Koji Tsuda},
keywords = {Linear optimal transport, Graph structured data, Kernel method},
abstract = {We present a framework for embedding graph structured data into a vector space, taking into account node features and structures of graphs into the optimal transport (OT) problem. Then we propose a novel distance between two graphs, named LinearFGW, defined as the Euclidean distance between their embeddings. The advantages of the proposed distance are twofold: 1) it takes into account node features and structures of graphs for measuring the dissimilarity between graphs in a kernel-based framework, 2) it is more efficient for computing a kernel matrix than pairwise OT-based distances, particularly fused Gromov-Wasserstein [1], making it possible to deal with large-scale data sets. Our theoretical analysis and experimental results demonstrate that our proposed distance leads to an increase in performance compared to the existing state-of-the-art graph distances when evaluated on graph classification and clustering tasks.}
}
@article{UMATANI2023109375,
title = {Time series clustering with an EM algorithm for mixtures of linear Gaussian state space models},
journal = {Pattern Recognition},
volume = {138},
pages = {109375},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109375},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000766},
author = {Ryohei Umatani and Takashi Imai and Kaoru Kawamoto and Shutaro Kunimasa},
keywords = {Time series clustering, Model-based clustering, State space model, EM algorithm, Mixture model},
abstract = {In this paper, we consider the task of clustering a set of individual time series while modeling each cluster, that is, model-based time series clustering. The task requires a parametric model with sufficient flexibility to describe the dynamics in various time series. To address this problem, we propose a novel model-based time series clustering method with mixtures of linear Gaussian state space models, which have high flexibility. The proposed method uses a new expectation-maximization algorithm for the mixture model to estimate the model parameters, and determines the number of clusters using the Bayesian information criterion. Experiments on a simulated dataset demonstrate the effectiveness of the method in clustering, parameter estimation, and model selection. The method is applied to real datasets commonly used to evaluate time series clustering methods. Results showed that the proposed method produces clustering results that are as accurate or more accurate than those obtained using previous methods.}
}
@article{ZHANG2023109435,
title = {Boosting transferability of physical attack against detectors by redistributing separable attention},
journal = {Pattern Recognition},
volume = {138},
pages = {109435},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109435},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300136X},
author = {Yu Zhang and Zhiqiang Gong and Yichuang Zhang and Kangcheng Bin and Yongqian Li and Jiahao Qi and Hao Wen and Ping Zhong},
keywords = {Physical attack, Transferability, Multi-layer attention, Object detection, Black-box models},
abstract = {The research on attack transferability is of great importance as it can guide how to conduct an adversarial attack without knowing any information about target models. However, it remains challenging for adversarial examples to maintain a good attack transferability performance, especially for the black-box attack implemented in the physical world. To enhance black-box transferability of physical attacks on object detectors, we present a novel adversarial learning method to produce adversarial patches by redistributing separable attention maps. Concretely, we first develop smoothed multilayer attention maps by introducing serial composite transformations, which could suppress model-specific noise on the one hand, and cover objects to be concealed at various resolutions on the other hand. Besides, our method resorts to a scalable mask to separate object attention from the background and adjust their distribution with a novel loss function. Extensive experiments show that our approach outperforms state-of-the-art methods in both the digital space and the physical world. Our code is available at https://github.com/zhangyu13a/transPhyAtt.}
}
@article{CAO2023109346,
title = {Unsupervised class-to-class translation for domain variations},
journal = {Pattern Recognition},
volume = {138},
pages = {109346},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109346},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300047X},
author = {Zhiyi Cao and Wei Wang and Lina Huo and Shaozhang Niu},
keywords = {Contrastive learning, Image-to-image translation, Adversarial learning, Image translation},
abstract = {The majority of image-to-image translation models tend to struggle in varying domain settings. For one varying domain, samples vary significantly in shape and size and have no domain labels. This paper proposes an unsupervised class-to-class translation model based on conditional contrastive learning to tackle the domain variations problem. The initial hypothesis is that the latent modalities of two varying domains are categorizable by style differences of different samples and turn the image-to-image translation problem into class-to-class translation. Firstly, unsupervised semantic clustering is performed for each domain to divide them into multiple classes and then leverage the classification features of different classes to perform class-to-class translation. Two conditional contrastive learning loss functions for each domain are proposed to perform unsupervised semantic clustering and decompose it into multiple classes. Then in the class-to-class translation stage, the classification features of different classes are employed to learn the latent modalities. The proposed model outperforms state-of-the-art baseline methods by employing the latent modalities of different classes. The sample code is available at https://github.com/c1a1o1/ucct.}
}
@article{WANG2023109379,
title = {Improving pseudo labels with intra-class similarity for unsupervised domain adaptation},
journal = {Pattern Recognition},
volume = {138},
pages = {109379},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109379},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000808},
author = {Jie Wang and Xiao-Lei Zhang},
keywords = {Unsupervised domain adaptation, Intra-class similarity, Spanning trees, Pseudo labels},
abstract = {Unsupervised domain adaptation (UDA) transfers knowledge from a label-rich source domain to a different but related fully-unlabeled target domain. To address the problem of domain shift, more and more UDA methods adopt pseudo labels of the target samples to improve the generalization ability on the target domain. However, inaccurate pseudo labels of the target samples may yield suboptimal performance with error accumulation during the optimization process. Moreover, once the pseudo labels are generated, how to remedy the generated pseudo labels is far from explored. In this paper, we propose a novel approach to improve the accuracy of the pseudo labels in the target domain. It first generates coarse pseudo labels by a conventional UDA method. Then, it iteratively exploits the intra-class similarity of the target samples for improving the generated coarse pseudo labels, and aligns the source and target domains with the improved pseudo labels. The accuracy improvement of the pseudo labels is made by first deleting dissimilar samples, and then using spanning trees to eliminate the samples with the wrong pseudo labels in the intra-class samples. We have applied the proposed approach to several conventional UDA methods as an additional term. Experimental results demonstrate that the proposed method can boost the accuracy of the pseudo labels and further lead to more discriminative and domain invariant features than the conventional baselines.}
}
@article{LIU2023109371,
title = {Capturing the few-shot class distribution: Transductive distribution optimization},
journal = {Pattern Recognition},
volume = {138},
pages = {109371},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109371},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000729},
author = {Xinyue Liu and Ligang Liu and Han Liu and Xiaotong Zhang},
keywords = {Few-shot learning, Transductive learning, Distribution estimation},
abstract = {Few-shot learning is challenging since only a few labeled samples are available for training a learning model. To alleviate the data limitation problem in few-shot learning, several works try to generate samples or features by learning a model or distribution. But complex models and biased estimation of class distribution hamper their interpretability and generalization ability, respectively. In this work, we propose a generation based Transductive Distribution Optimization (TDO) method, which introduces neither extra parameters nor complex models. We use a few labeled samples and some high-confident unlabeled samples of the target set to capture the distributions of the few-shot classes, and then generate sufficient samples from them to augment the labeled inputs. Our method can work with most pre-trained feature extractors and outperforms state-of-the-art methods with a simple linear classifier. The visualization of the generated samples shows that our method can capture an accurate distribution even though the few labeled samples deviate from the ground-truth distribution.}
}
@article{LU2023109405,
title = {Robust weighted co-clustering with global and local discrimination},
journal = {Pattern Recognition},
volume = {138},
pages = {109405},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109405},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001061},
author = {Zhoumin Lu and Shiping Wang and Genggeng Liu and Feiping Nie},
keywords = {Machine learning, Co-clustering, Nonnegative matrix factorization, Global discrimination, Local discrimination},
abstract = {In the past few decades, the clustering problem has made considerable progress, and co-clustering algorithms have attracted more attention. Compared with one-side clustering, co-clustering not only groups samples according to the distribution of features but also groups features according to the distribution of samples at the same time. This duality helps to explore the structural information of data, such as genes and texts. In this paper, a new co-clustering algorithm is proposed to simultaneously consider feature weights, data noise, local manifolds, and global scatter, named robust weighted co-clustering with global and local discrimination. Furthermore, an alternate update rule is put forward to optimize objective, theoretically proven to converge. Then, the algorithm’s duality, robustness, and effectiveness have been verified on synthetic, corrupted, and real datasets, respectively. The runtime and parameter sensitivity of the algorithm are also analyzed. Finally, sufficient experiments clarify the competitiveness of our algorithm compared to other ones.}
}
@article{HUANG2023109425,
title = {NPDN-3D: A 3D neural partial differential network for spatiotemporal prediction},
journal = {Pattern Recognition},
volume = {138},
pages = {109425},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109425},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001267},
author = {Xu Huang and Shanshan Feng and Yunming Ye and Xutao Li and Bowen Zhang and Shidong Chen},
keywords = {Spatiotemporal prediction, Neural partial differential network, Machine learning, Physical dynamics},
abstract = {Neural network-based methods have been widely applied to spatiotemporal prediction tasks, such as video prediction and weather forecasting. However, most existing works are designed for prediction in 2D space, and 3D prediction has not been extensively studied. In this paper, we propose to leverage 3D partial differential equations (PDEs) for spatiotemporal prediction in 3D space, and further develop a novel 3D neural partial differential network. This is inspired by that 3D PDEs can model both horizontal and vertical information interactions by various partial derivatives. Moreover, they can also formulate physical knowledge by equations. To integrate 3D PDEs in neural networks, we first develop the theory of approximating 3D partial derivatives by 3D convolutions, and further present an effective strategy to utilize the theory in practice. Then based on the theory and strategy, we propose a novel 3D Neural Partial Differential Network for prediction, named NPDN-3D. Specifically, NPDN-3D consists of two pivotal modules: (1) a neural partial differential module for capturing low-order spatiotemporal dynamics. This module is the key for prediction, where the dynamics are formulated by commonly-used low-order 3D PDEs. (2) A residual module for capturing the remaining non-low-order dynamics. This module performs as an extensible plug-in to enhance the expressiveness of our model. Extensive experiments on two simulated datasets and two real datasets show that our method not only achieves better prediction but also learns the correct PDEs.}
}
@article{LI2023109364,
title = {MinEnt: Minimum entropy for self-supervised representation learning},
journal = {Pattern Recognition},
volume = {138},
pages = {109364},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109364},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000651},
author = {Shuo Li and Fang Liu and Zehua Hao and Licheng Jiao and Xu Liu and Yuwei Guo},
keywords = {Self-supervised learning, Minimum entropy, Unsupervised representation learning, Image classification},
abstract = {Self-supervised representation learning is becoming more and more popular due to its superior performance. According to the information entropy theory, the smaller the information entropy of a feature, the more certain it is and the less redundant it is. Based on this, we propose a simple yet effective self-supervised representation learning method via Minimum Entropy (MinEnt). From the perspective of reducing information entropy, our MinEnt takes the output of the projector towards its nearest minimum entropy as the optimization target. The core of our MinEnt consists of three important steps: 1) normalize along the batch dimension to avoid model collapse, 2) compute the nearest minimum entropy to get the target, 3) compute the loss and backpropagate to optimize the network. Our MinEnt can learn efficient representations, even without the need for techniques such as negative sample pairs, predictors, momentum encoders, cross-correlation matrices, etc. Experimental results on four widely used datasets show that our method achieves competitive results in a simple manner.}
}
@article{JIA2023109388,
title = {Global and local structure preserving nonnegative subspace clustering},
journal = {Pattern Recognition},
volume = {138},
pages = {109388},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109388},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000894},
author = {Hongjie Jia and Dongxia Zhu and Longxia Huang and Qirong Mao and Liangjun Wang and Heping Song},
keywords = {Subspace clustering, Global structure, Local structure, Nonnegative Lagrangian relaxation, Kernel clustering},
abstract = {Most subspace clustering methods construct the similarity matrix based on self-expressive property and apply the spectral relaxation on the similarity matrix to get the final clusters. Despite the advantages of this framework, it has two limitations that are easily ignored. Firstly, the original self-expressive model only considers the global structure of data, and the ubiquitous local structure among data is not paid enough attention. Secondly, spectral relaxation is naturally suitable for 2-way clustering tasks, but when dealing with multi-way clustering tasks, the assignment of cluster members becomes indirect and requires additional steps. To overcome these problems, this paper proposes a global and local structure preserving nonnegative subspace clustering method, which learns data similarities and cluster indicators in a mutually enhanced way within a unified framework. Besides, the model is extended to kernel space to strengthen its capability of dealing with nonlinear data structures. For optimizing the objective function of the method, multiplicative updating rules based on nonnegative Lagrangian relaxation are developed, and the convergence is guaranteed in theory. Abundant experiments have shown that the proposed model is better than many advanced clustering methods in most cases.}
}
@article{NGUYEN2023109336,
title = {Robust detectors of rotationally symmetric shapes based on novel semi-shape signatures},
journal = {Pattern Recognition},
volume = {138},
pages = {109336},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109336},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000377},
author = {Thanh Phuong Nguyen and Thanh Tuan Nguyen},
keywords = {Rotational symmetry detection, LIP/-signature, Radon},
abstract = {Efficient detectors of rotationally symmetric shapes are proposed by introducing a novel concept of semi-shape signatures to overcome the main problem of projection-based approaches for studying the rotationally symmetric properties of an arbitrary binary shape. Indeed, the fact that the projection cues in these conventional approaches are periodical with a period of π has restricted an applicable exploitation of rotational symmetry detection. To this end, we propose a new concept of the profile of semi-shapes as a shape signature together with a simple yet efficient technique so that the rotational symmetry of the binary shape can be determined by considering the correlation between this signature and its circular shift. Moreover, a new meaningful measure, ranging from 0 to 1, is also introduced to indicate how perfect the rotational symmetry would be. Experimental results of detecting on single/compound shapes have clearly corroborated the competence of our proposal.}
}
@article{BADEA2023109417,
title = {Timid semi–supervised learning for face expression analysis},
journal = {Pattern Recognition},
volume = {138},
pages = {109417},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109417},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001188},
author = {Mihai Badea and Corneliu Florea and Andrei Racoviţeanu and Laura Florea and Constantin Vertan},
keywords = {Face expression, Action units, Semi–supervised learning, Diversity},
abstract = {In the last years, semi–supervised learning has been proposed as a strategy with high potential for improving machine learning capabilities. Face expression recognition may highly benefit from such a technique, as accurate labeling is both difficult and costly, whereas millions of unlabeled images with human faces are available on the Internet, but without annotations. In this paper we evaluate the benefits of semi–supervised learning in the practical scenarios of face expression analysis. Our conclusion is that better performance is indeed achievable, but by methods that put a distinct emphasis on the diversity of exploring patterns in the unlabeled data domain. The evaluation is carried on multiple tasks such as detecting Action Units on EmotioNet, assessing Action Units intensity on the spontaneous DISFA database and, respectively, recognizing expressions on static images acquired in the wild, from the RAF-DB and FER+ databases. We show that, in these scenarios, a so–called timid semi–supervised learner is more robust and achieves higher performance than standard, confident semi–supervised learners.}
}
@article{LI2023109356,
title = {Semi-supervised vector-valued learning: Improved bounds and algorithms},
journal = {Pattern Recognition},
volume = {138},
pages = {109356},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109356},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000572},
author = {Jian Li and Yong Liu and Weiping Wang},
keywords = {Vector-valued learning, Semi-supervised learning, Excess risk bound, Local rademacher complexity},
abstract = {Vector-valued learning, where the output space admits a vector-valued structure, is an important problem that covers a broad family of important domains, e.g. multi-task learning and transfer learning. Using local Rademacher complexity and unlabeled data, we derive novel semi-supervised excess risk bounds for general vector-valued learning from both kernel perspective and linear perspective. The derived bounds are much sharper than existing ones and the convergence rates are improved from the square root of labeled sample size to the square root of total sample size or directly dependent on labeled sample size. Motivated by our theoretical analysis, we propose a general semi-supervised algorithm for efficiently learning vector-valued functions, incorporating both local Rademacher complexity and Laplacian regularization. Extensive experimental results illustrate the proposed algorithm significantly outperforms the compared methods, which coincides with our theoretical findings.}
}
@article{VIRTA2023109401,
title = {Poisson PCA for matrix count data},
journal = {Pattern Recognition},
volume = {138},
pages = {109401},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109401},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001024},
author = {Joni Virta and Andreas Artemiou},
keywords = {Discrete data, Kronecker model, Matrix normal distribution, Poisson log-normal distribution},
abstract = {We develop a dimension reduction framework for data consisting of matrices of counts. Our model is based on the assumption of existence of a small amount of independent normal latent variables that drive the dependency structure of the observed data, and can be seen as the exact discrete analogue of a contaminated low-rank matrix normal model. We derive estimators for the model parameters and establish their limiting normality. An extension of a recent proposal from the literature is used to estimate the latent dimension of the model. The method is shown to outperform both its vectorization-based competitors and matrix methods assuming the continuity of the data distribution in analysing simulated data and real world abundance data.}
}
@article{YANG2023109348,
title = {Sparse possibilistic c-means clustering with Lasso},
journal = {Pattern Recognition},
volume = {138},
pages = {109348},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109348},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000493},
author = {Miin-Shen Yang and Josephine B.M. Benjamin},
keywords = {Clustering, Possibilistic c-means (PCM), Feature weights, Sparsity, Lasso, Spare PCM (S-PCM)},
abstract = {Krishnapuram and Keller first proposed possibilistic c-means (PCM) clustering in 1993. Afterward, PCM was widely studied with various extensions. The PCM algorithm and its extensions always treat feature components under equal importance, but, in real applications, different features may better have different weights. Recently, Yang and Benjamin in 2021 proposed a feature-weighted PCM clustering with feature reduction. Although Yang and Benjamin (2021) can reduce feature dimensions, it still encounters the curse of dimensionality for high dimensional data. One possible way to address this problem is to conduct a sparse clustering technique. In this paper, we further study the PCM clustering by incorporating the idea of sparsity with different feature weights. We propose two approaches that use the PCM clustering with the least absolute shrinkage and selection operator (Lasso). The first one is the sparse PCM subject to a Lasso constraint of feature weights, called S-PCM1. The second is the sparse PCM by adding a Lasso penalty term of feature weights in the objective function, called S-PCM2. We show that S-PCM1 and S-PCM2 are theoretically the same, and both can induce sparsity in features, but they use different procedures in algorithms. Synthetic and real data sets are used to compare S-PCM1 and S-PCM2 with some existing sparsity clustering algorithms. Experimental results and comparisons demonstrate the effectiveness and usefulness of the proposed S-PCM1 and S-PCM2 clustering algorithms.}
}
@article{WANG2023109384,
title = {How to Reduce Change Detection to Semantic Segmentation},
journal = {Pattern Recognition},
volume = {138},
pages = {109384},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109384},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000857},
author = {Guo-Hua Wang and Bin-Bin Gao and Chengjie Wang},
keywords = {Change detection, Semantic segmentation, Feature fusion},
abstract = {Change detection (CD) aims to identify changes that occur in an image pair taken different times. Prior methods devise specific networks from scratch to predict change masks in pixel-level, and struggle with general segmentation problems. In this paper, we propose a new paradigm that reduces CD to semantic segmentation which means tailoring an existing and powerful semantic segmentation network to solve CD. This new paradigm conveniently enjoys the mainstream semantic segmentation techniques to deal with general segmentation problems in CD. Hence we can concentrate on studying how to detect changes. We propose a novel and importance insight that different change types exist in CD and they should be learned separately. Based on it, we devise a module named MTF to extract the change information and fuse temporal features. MTF enjoys high interpretability and reveals the essential characteristic of CD. And most segmentation networks can be adapted to solve the CD problems with our MTF module. Finally, we propose C-3PO, a network to detect changes at pixel-level. C-3PO achieves state-of-the-art performance without bells and whistles. It is simple but effective and can be considered as a new baseline in this field. Our code for C-3PO is available at https://github.com/DoctorKey/C-3PO.}
}
@article{MA2023109363,
title = {Lambertian-based adversarial attacks on deep-learning-based underwater side-scan sonar image classification},
journal = {Pattern Recognition},
volume = {138},
pages = {109363},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109363},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300064X},
author = {Qixiang Ma and Longyu Jiang and Wenxue Yu},
keywords = {Adversarial attack, Classification, Side-scan sonar, Lambertian model},
abstract = {Deep convolutional neural networks (CNNs) are extensively applied to the classification tasks for Side-scan sonar (SSS) images. However, state-of-the-art neural networks are prone to be confused by adversarial attacks that generate a tiny modification of the images, threatening the security of SSS classification. The robustness of CNN to adversarial attacks can be improved by introducing adversarial examples through adversarial training. Practical adversarial examples are often generated from elaborate adversarial attackers. For the underwater scenario of sonar, a specially designed adversarial attack method to weaken SSS image classification can make the research community better understand the weakness of CNN in this scenario and improve the security measures in a well-directed way. Thus, exploring adversarial attack methods for SSS image classification is essential. Nevertheless, the existing adversarial attack methods are designed for optical images, reflecting no physical characteristics of sonar images. To fill this gap and investigate the adversarial attack related to real-world conditions, in this paper, we propose an adversarial attack method named Lambertian Adversarial Sonar Attack (LASA). It initially leverages the Lambertian model to simulate the formation of the SSS image, factoring the image to three parameters, then updates the parameters on the direction of gradients by the chain rule. Finally, the parameters regenerate the adversarial example to fool the classifier. To validate the performance of LASA, we constructed a diversified SSS image dataset containing three categories. On our dataset, LASA reduces the Top-1 accuracy of a well-trained ResNet-101 to 7.31%±0.21 (one-shot version) and 0.00% (iterative version), the success rate of targeted attack reaches 97.03±2.24, far beyond the performance of the existing state-of-the-art adversarial attack methods. Meanwhile, we show that the adversarial training using examples generated from LASA makes the classifier more robust. We expect that our methods can be applied as a benchmark of adversarial attacks on SSS images, motivating future research to design novel neural networks or defensive methods to resist real-world adversarial attacks on SSS images.}
}
@article{JIANG2023109429,
title = {Aggregated pyramid gating network for human pose estimation without pre-training},
journal = {Pattern Recognition},
volume = {138},
pages = {109429},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109429},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001309},
author = {Chenru Jiang and Kaizhu Huang and Shufei Zhang and Xinheng Wang and Jimin Xiao and Yannis Goulermas},
keywords = {Pyramid gating system, Stabilization, Human pose estimation},
abstract = {In this work, we propose a comprehensive aggregated residual gating structure, the Pyramid GAting Network (PGA-Net) for human pose estimation which can select, distill, and fuse semantic level and natural level information from multiple scales. In comparison, through utilizing multi-scale features, most existing state-of-the-art pose estimation methods are still limited in three aspects. First, multi-scale features contain massively redundant information, which is unfortunately not distilled by most existing approaches. Second, preferring deeper network structures to extract strong semantic features, the conventional methods often ignore original texture information fusion. Third, to attain a good parameter initialization, the current methods heavily rely on pre-training, which is very time-consuming or even unavailable. While better coping with the above problems, our proposed PGA-Net distills high-level semantic features and replenishes low-level original information to reinforce module representation capability. Meanwhile, PGA-Net demonstrates notable training stability and superior performance even without pre-training. Extensive experiments demonstrate that our method consistently outperforms previous approaches even without pre-training, enabling thus an end-to-end model training from scratch. In COCO benchmark, PGA-Net consistently achieves over 3% improvements than the baseline (without pre-training) under various model configurations.11The code is released at https://github.com/ssr0512/PGA-Net}
}
@article{LUO2023109376,
title = {Independent vector analysis: Model, applications, challenges},
journal = {Pattern Recognition},
volume = {138},
pages = {109376},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109376},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000778},
author = {Zhongqiang Luo},
keywords = {IVA, BSS, ICA, Source priori models, Unsupervised learning, Audio source separation},
abstract = {This paper overviews an appealing unsupervised learning method named independent vector analysis (IVA) for its promising applications, such as in audio/speech signal separation, medical signal processing, remote sensing, video/image processing, wireless communication processing, and so on. As a useful data-driven technique in blind source separation (BSS) field, IVA has played an increasingly vital role in dealing with the problems of convolutive mixture separation, multivariate latent variable analysis and multivariate data fusion. IVA extends the conventional independent component analysis (ICA) to multidimensional components, which can result in more available information utilization. Compared with ICA mechanism, IVA is not only to utilize the statistical independence of multivariate signals but also the statistical inner dependency of each multivariate signal. With this generalization, IVA can manipulate some prominent ill-pose issues faced in sensor receiving models and has the advantage to overcome the inherent random permutation ambiguity problem in joint BSS. Motivated by the flexible and versatile technology superiorities of IVA, this paper concentrates on reviewing the IVA model in details, associated methods briefly, and its potential applications as well as prospects. Moreover, some significant open problems about IVA challenges are also discussed in this paper.}
}
@article{WANG2023109360,
title = {Versatile recurrent neural network for wide types of video restoration},
journal = {Pattern Recognition},
volume = {138},
pages = {109360},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109360},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000614},
author = {Yadong Wang and Xiangzhi Bai},
keywords = {RNN, Video restoration, Versatile, Efficient},
abstract = {Video shooting of natural scenes often suffers from various serious degradation, such as motion blur, impact of atmospheric turbulence, random noise and resolution reduction, etc. Different from the maturity of image restoration research, video restoration is much more complicated so that it lacks effective general method. Here, we present a versatile recurrent neural network (VRNN) to handle wide types of video degradation and generate stable videos with ideal clarity. We complete the design of VRNN through deducing a general video restoration paradigm that reveals the importance of simultaneously utilizing past and future information for restoring current frame. Specifically, we propose a novel RNN cell in which hidden state flows in bidirections, enriching temporal information contained in the extracted features. Furthermore, a feature fusion module involves temporal and spatial attention processing is designed to refine features of neighbouring frames and help reconstruct current frame. Extensive experiments on well-known public datasets (including four different kinds of video restoration tasks, with a total of 35,666 videos and 515,774 frames) show that the proposed VRNN achieves 1–4 dB of PSNR increasing or several times less of computational complexity in all tasks against state-of-the-art methods, manifesting the versatile and efficient ability of proposed VRNN in wide types of video restoration.}
}
@article{ZHANG2023109442,
title = {Efficient large-scale oblique image matching based on cascade hashing and match data scheduling},
journal = {Pattern Recognition},
volume = {138},
pages = {109442},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109442},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001425},
author = {Qiyuan Zhang and Shunyi Zheng and Ce Zhang and Xiqi Wang and Rui Li},
keywords = {Oblique image matching, Feature point matching, SIFT, Cascade hashing, Match data scheduling, Structure from motion},
abstract = {In this paper, we design an efficient large-scale oblique image matching method. First, to reduce the number of redundant transmissions of match data, we propose a novel three-level buffer data scheduling (TLBDS) algorithm that considers the adjacency between images for match data scheduling from disk to graphics memory. Second, we adopt the epipolar constraint to filter the initial candidate points of cascade hashing matching, thereby significantly increasing the robustness of matching feature points. Comprehensive experiments are conducted on three oblique image datasets to test the efficiency and effectiveness of the proposed method. The experimental results show that our method can complete a match pair within 2.50∼2.64 ms, which not only is much faster than two open benchmark pipelines (i.e., OpenMVG and COLMAP) by 20.4∼97.0 times but also have higher efficiency than two state-of-the-art commercial software (i.e., Agisoft Metashape and Pix4Dmapper) by 10.4∼50.0 times.}
}
@article{LIU2023109368,
title = {Expression snippet transformer for robust video-based facial expression recognition},
journal = {Pattern Recognition},
volume = {138},
pages = {109368},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109368},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000699},
author = {Yuanyuan Liu and Wenbin Wang and Chuanxu Feng and Haoyu Zhang and Zhe Chen and Yibing Zhan},
keywords = {Video-based facial expression recognition, Snippet-based transformer, Attention-augmented snippet feature extractor, Shuffled snippet order prediction},
abstract = {Although Transformer can be powerful for modeling visual relations and describing complicated patterns, it could still perform unsatisfactorily for video-based facial expression recognition, since the expression movements in a video can be too small to reflect meaningful spatial-temporal relations. To this end, we propose to decompose the modeling of expression movements of a video into the modeling of a series of expression snippets, each of which contains a few frames, and then boost the Transformer’s ability for intra-snippet and inter-snippet visual modeling, respectively, obtaining the Expression snippet Transformer (EST). For intra-snippet modeling, we devise an attention-augmented snippet feature extractor to enhance the encoding of subtle facial movements of each snippet. For inter-snippet modeling, we introduce a shuffled snippet order prediction head and a corresponding loss to improve the modeling of subtle motion changes across subsequent snippets. The EST obtains state-of-the-art performance, demonstrating its superiority to other CNN-based methods. Our code and the trained model are available at https://github.com/DreamMr/EST}
}
@article{ZHAO2023109352,
title = {Improving generalization of double low-rank representation using Schatten-p norm},
journal = {Pattern Recognition},
volume = {138},
pages = {109352},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109352},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000535},
author = {Jiaoyan Zhao and Yongsheng Liang and Shuangyan Yi and Qiangqiang Shen and Xiaofeng Cao},
keywords = {Low-rank representation, Schatten- norm, Feature extraction, Subspace clustering},
abstract = {Low-rank representation reveals a highly-informative entailment of sparse matrices, where double low-rank representation (DLRR) presents an effective solution by adopting nuclear norm. However, it is a special constraint of Schatten-p norm with p=1 which equally treats all singular values, deviating from the optimal low-rank representation that considers p=0. Thus, this paper improves the DLRR generalization of DLRR by relaxing p=1 into 0<p≤1 to tighten the low-rank constraint of the Schatten-p norm. With such a relaxation, low-rank optimization is then accelerated, resulting in a lower bound on the calculation complexity. Experiments on unsupervised feature extraction and subspace clustering demonstrate that our low-rank optimization taking 0<p≤1 achieves a superior performance against state-of-the-art methods.}
}
@article{FRANCO2023109372,
title = {Under the hood of transformer networks for trajectory forecasting},
journal = {Pattern Recognition},
volume = {138},
pages = {109372},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109372},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000730},
author = {Luca Franco and Leonardo Placidi and Francesco Giuliari and Irtiza Hasan and Marco Cristani and Fabio Galasso},
keywords = {Trajectory forecasting, Human behavior, Transformer networks, BERT, Multi-modal future prediction},
abstract = {Transformer Networks have established themselves as the de-facto state-of-the-art for trajectory forecasting but there is currently no systematic study on their capability to model the motion patterns of people, without interactions with other individuals nor the social context. There is abundant literature on LSTMs, CNNs and GANs on this subject. However methods adopting Transformer techniques achieve great performances by complex models and a clear analysis of their adoption as plain sequence models is missing. This paper proposes the first in-depth study of Transformer Networks (TF) and the Bidirectional Transformers (BERT) for the forecasting of the individual motion of people, without bells and whistles. We conduct an exhaustive evaluation of the input/output representations, problem formulations and sequence modelling, including a novel analysis of their capability to predict multi-modal futures. Out of comparative evaluation on the ETH+UCY benchmark, both TF and BERT are top performers in predicting individual motions and remain within a narrow margin wrt more complex techniques, including both social interactions and scene contexts. Source code will be released for all conducted experiments.}
}
@article{FERNANDEZGARCIA2023109396,
title = {Assessing polygonal approximations: A new measurement and a comparative study},
journal = {Pattern Recognition},
volume = {138},
pages = {109396},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109396},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000973},
author = {Nicolás Luis Fernández-García and Luis Del-Moral Martínez and Ángel Carmona-Poyato and Francisco José Madrid-Cuevas and Rafael Medina-Carnicer},
keywords = {2D Closed curve, Contour, Polygonal approximation, Performance evaluation},
abstract = {Two proposals related to the evaluation of polygonal approximations are presented in this document. First, a new measurement, called normalized compression ratio and adjustment error (NCA), to provide a fair evaluation of the performance of the polygonal approximations of 2D closed curves is proposed. Second, a new methodology for evaluation of measurements for assessing polygonal approximations is also proposed. This methodology is based on the optimal quality curve concept, which can characterize the performance of the measurements. A simple visual analysis of the optimal quality curve allows possible drawbacks or weaknesses of the measurement to be detected. The new evaluation methodology is used to compare the performance of the proposed NCA and the most popular measurements, such as Rosin’s Merit, FOM or versions of FOM. Experiments show that NCA obtains the best results and, therefore, may be used to fairly evaluate the performance of polygonal approximations.}
}
@article{FANG2023109340,
title = {Robust image clustering via context-aware contrastive graph learning},
journal = {Pattern Recognition},
volume = {138},
pages = {109340},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109340},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000419},
author = {Uno Fang and Jianxin Li and Xuequan Lu and Ajmal Mian and Zhaoquan Gu},
keywords = {Supervised clustering, Graph convolution network, Contrastive graph learning, Graph view generation},
abstract = {Graph convolution networks (GCN) have recently become popular for image clustering. However, existing GCN-based image clustering techniques focus on learning image neighbourhoods which leads to poor reasoning on the cluster boundaries. To address this challenge, we propose a supervised image clustering approach based on contrastive graph learning (CGL). Our method generates an influential graph view (IGV) and a topological graph view (TGV) for each class to represent its global context from different viewpoints. These generated graph views are used to reason the inter-cluster relationships and intra-cluster boundaries from the local context of each node in a contrastive manner. Our method considers each class as a fully connected graph to explore its characteristics and strategically generate directional graph views. This enhances the transferability of the proposed approach to handle data with a similar structure. We conduct extensive experiments on open datasets such as LFW, CASIA-WebFace, and CIFAR-10 and show that our method outperforms state-of-the-art including deep GRAph Contrastive rEpresentation learning (GRACE), GraphCL, and Graph Contrastive Clustering (GCC).}
}
@article{ZHANG2023109402,
title = {Cross-task and cross-domain SAR target recognition: A meta-transfer learning approach},
journal = {Pattern Recognition},
volume = {138},
pages = {109402},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109402},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001036},
author = {Yukun Zhang and Xiansheng Guo and Henry Leung and Lin Li},
keywords = {Meta learning, Transfer learning, Domain confusion, Synthetic aperture radar (SAR)},
abstract = {Meta learning and transfer learning offer promising solutions to the problem of requiring large amounts of data in deep learning approaches for synthetic aperture radar (SAR) target recognition. To improve their performance further, we propose a novel Meta-transfer learning approach for cross-task and cross-domain SAR target recognition (MetraSAR). In the meta training phase, we train a robust meta learner with the human-like ability to master new knowledge quickly across tasks and domains. By designing the weighted classification loss with class weights, we conduct hard class mining that forces the meta learner to grow stronger. In addition to the external knowledge transfer across different tasks, we achieve the internal transfer across domains by using the domain confusion loss with a domain discriminator. To balance the two designed loss terms, we adopt the multi-gradient descent algorithm to optimize the meta learner adaptively. In the meta testing phase, the trained robust meta learner is transferred to solve the new task with few shot samples and a quick generalization. Extensive experiments on the moving and stationary target acquisition and recognition (MSTAR) dataset validate that MetraSAR has better performance than conventional SAR target recognition methods.}
}
@article{LEE2023109365,
title = {AFI-GAN: Improving feature interpolation of feature pyramid networks via adversarial training for object detection},
journal = {Pattern Recognition},
volume = {138},
pages = {109365},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109365},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000663},
author = {Seong-Ho Lee and Seung-Hwan Bae},
keywords = {Multi-scale feature representation, Object detection, Adversarial training, Feature up-sampling},
abstract = {Recent convolutional detectors learn strong semantic features by generating and combining multi-scale features via feature interpolation. However, simple interpolation incurs often noisy and blurred features. To resolve this, we propose a novel adversarially-trained interpolator which can substitute for the traditional interpolation effortlessly. In specific, we design AFI-GAN consisting of an AF interpolator and a feature patch discriminator. In addition, we present a progressive adversarial learning and AFI-GAN losses to generate multi-scale features for downstream detection tasks. However, we can also finetune the proposed AFI-GAN with the recent multi-scale detectors without the adversarial learning once a pre-trained AF interpolator is provided. We prove the effectiveness and flexibility of our AF interpolator, and achieve the better box and mask APs by 2.2% and 1.6% on average compared to using other interpolation. Moreover, we achieve an impressive detection score of 57.3% mAP on the MSCOCO dataset. Code is available at https://github.com/inhavl-shlee/AFI-GAN.}
}
@article{ZHANG2023109313,
title = {Weakly-supervised butterfly detection based on saliency map},
journal = {Pattern Recognition},
volume = {138},
pages = {109313},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109313},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000146},
author = {Ting Zhang and Muhammad Waqas and Yu Fang and Zhaoying Liu and Zahid Halim and Yujian Li and Sheng Chen},
keywords = {Butterfly detection, Saliency map, Class activation map, Weakly-supervised object detection},
abstract = {Given the actual needs for detecting multiple features of butterflies in natural ecosystems, this paper proposes a model of weakly-supervised butterfly detection based on a saliency map (WBD-SM) to enhance the accuracy of butterfly detection in the ecological environment as well as to overcome the difficulty of fine annotation. Our proposed model first extracts the features of different scales using the VGG16 without the fully connected layers as the backbone network. Next, the saliency maps of butterfly images are extracted using the deep supervision network with shortcut connections (DSS) used for the butterfly target location. The class activation maps of butterfly images are derived via the adversarial complementary learning (ACoL) network for butterfly target recognition. Then, the saliency and class activation maps are post-processed with conditional random fields, thereby obtaining the refined saliency maps of butterfly objects. Finally, the locations of the butterflies are acquired based on the saliency maps. Experimental results on the 20 categories of butterfly dataset collected in this paper indicate that the WBD-SM achieves a higher recognition accuracy than that of the VGG16 under different division ratios. At the same time, when the training set and test set are 8:2, our WBD-SM attains a 95.67% localization accuracy, which is 9.37% and 11.87% higher than the results of the DSS and ACoL, respectively. Compared with three state-of-the-art fully-supervised object detection networks, RefineDet, YOLOv3 and single-shot detection (SSD), the detection performance of our WBD-SM is better than RefineDet, and YOLOv3, and is almost the same as SSD.}
}
@article{JIA2023109357,
title = {Multi-dimensional multi-label classification: Towards encompassing heterogeneous label spaces and multi-label annotations},
journal = {Pattern Recognition},
volume = {138},
pages = {109357},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109357},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000584},
author = {Bin-Bin Jia and Min-Ling Zhang},
keywords = {Machine learning, Supervised learning, Multi-dimensional classification, Multi-label classification, Multi-dimensional multi-label classification},
abstract = {In traditional classification framework, the semantics of each object is usually characterized by annotating a single class label from one homogeneous label space. Nonetheless, objects with rich semantics naturally arise in real-world applications whose properties need to be characterized in a more sophisticated manner. In this paper, a new classification framework named Multi-Dimensional Multi-Label (MDML) classification is investigated which models objects with rich semantics by encompassing heterogeneous label spaces and multi-label annotations. Specifically, MDML generalizes the traditional classification framework by assuming a number of heterogeneous label spaces to characterize semantics from different dimensions, where each object is further annotated with multiple class labels from each heterogeneous label space. To learn from MDML training examples, a first attempt named CLIM is proposed based on an augmented stacking strategy. Firstly, CLIM induces a base multi-label predictive model w.r.t. each label space by maximizing the likelihood of the observed multiple class labels. Secondly, the thresholding predictions from all base models are used to augment the original feature space to yield stacked multi-label predictive models. The two-level models are refined alternately via empirical threshold tuning. Experiments on four real-world MDML data sets validate the effectiveness of CLIM in learning from training examples with heterogeneous label spaces and multi-label annotations.}
}
@article{YULIN2023109422,
title = {BEST: Building evidences from scattered templates for accurate contactless palmprint recognition},
journal = {Pattern Recognition},
volume = {138},
pages = {109422},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109422},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001231},
author = {Feng Yulin and Ajay Kumar},
keywords = {Contactless palmprint matching, Cross-sensor palmprint matching, Personal identification, Lightweight Convolutional Neural Networks,, Information security, Biometrics},
abstract = {Contactless palmprint identification offers significantly improved hygiene and user convenience, making it highly attractive for a range of civilian applications, especially during the current pandemic. However, the accurate recognition of contactless palmprint images can be highly challenging, attributed to the significant variations in the intra-class similarity and limitations of conventional palmprint feature descriptors under involuntary or contactless imaging variations. State-of-the-art completely contactless palmprint matching algorithms in the literature cannot adequately address these challenges and are not sufficiently accurate and fast enough for such real-world applications. This paper proposes a novel approach that adaptively locates the local palmprint regions with high similarities between their corresponding feature representations or templates to address these challenges. We consider spatial localization of such highly similar feature representations from multiple local regions and consolidate them to generate a more reliable match score. This paper presents reproducible and comparative experimental results, using within-database, cross-database, and cross-sensor performance evaluation, on four publicly available contactless palmprint datasets, including a sizeable contactless palmprint database from 600 different subjects. The proposed method achieves outperforming results compared with three state-of-the-art deep learning-based methods and five widely used conventional methods. In addition, the proposed method is also significantly faster than all state-of-the-art baseline methods.}
}
@article{SUN2023109285,
title = {Wse-MF: A weighting-based student exercise matrix factorization model},
journal = {Pattern Recognition},
volume = {138},
pages = {109285},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109285},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007646},
author = {Xia Sun and Bo Li and Richard Sutcliffe and Zhizezhang Gao and Wenying Kang and Jun Feng},
keywords = {Educational data mining, Personalized exercise prediction, Matrix factorization},
abstract = {Students who have been taught new ideas need to develop their skills by carrying out further work in their own time. This often consists of a series of exercises which must be completed. While students can choose exercises themselves from online sources, they will learn more quickly and easily if the exercises are specifically tailored to their needs. A good teacher will always aim to do this, but with the large groups of students who typically take advantage of open online courses, it may not be possible. Exercise prediction, working with large-scale matrix data, is a better way to address this challenge, and a key stage within such prediction is to calculate the probability that a student will answer a given question correctly. Therefore, this paper presents a novel approach called Weighting-based Student Exercise Matrix Factorization (Wse-MF) which combines student learning ability and exercise difficulty as prior weights. In order to learn how to complete the matrix, we apply an iterative optimization method that makes the approach practical for large-scale educational deployment. Compared with eight models in cognitive diagnosis and matrix factorization, our research results suggest that Wse-MF significantly outperforms the state-of-the-art on a range of real-world datasets in both prediction quality and time complexity. Moreover, we find that there is an optimal value of the latent factor K (the inner dimension of the factorization) for each dataset, which is related to the relationship between skills and exercises in that dataset. Similarly, the optimal value of hyperparameter c0 is linked to the ratio between exercises and students. Taken as a whole, we demonstrate improvements to matrix factorization within the context of educational data.}
}
@article{ELHARROUSS2023109361,
title = {Refined edge detection with cascaded and high-resolution convolutional network},
journal = {Pattern Recognition},
volume = {138},
pages = {109361},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109361},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000626},
author = {Omar Elharrouss and Youssef Hmamouche and Assia Kamal Idrissi and Btissam {El Khamlichi} and Amal {El Fallah-Seghrouchni}},
keywords = {Edge detection, Convolutional neural networks, Deep learning, Scale-representation, Backbone},
abstract = {Edge detection is represented as one of the most challenging tasks in computer vision, due to the complexity of detecting the edges or boundaries in real-world images that contains objects of different types and scales like trees, building as well as various backgrounds. Edge detection is represented also as a key task for many computer vision applications. Using a set of backbones as well as attention modules, deep-learning-based methods improved the detection of edges compared with traditional methods like Sobel or Canny. However, images of complex scenes still represent a challenge for these methods. Also, the detected edges using the existing approaches suffer from non-refined results with erroneous edges. In this paper, we attempted to overcome these challenges for refined edge detection using a cascaded and high-resolution network named (CHRNet). By maintaining the high resolution of edges during the training process, and conserving the resolution of the edge image during the network stage, sub-blocks are connected at every stage with the output of the previous layer. Also, after each layer, we use batch normalization layer with an active affine parameter as an erosion operation for the homogeneous region in the image. The proposed method is evaluated using the most challenging datasets including BSDS500, NYUD, and Multicue. The obtained results outperform the designed edge detection networks in terms of performance metrics and quality of output images.The code is available at: https://github.com/elharroussomar/chrnet/}
}
@article{CEVIKALP2023109385,
title = {From anomaly detection to open set recognition: Bridging the gap},
journal = {Pattern Recognition},
volume = {138},
pages = {109385},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109385},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000869},
author = {Hakan Cevikalp and Bedirhan Uzun and Yusuf Salk and Hasan Saribas and Okan Köpüklü},
keywords = {Anomaly detection, Open set recognition, Hypersphere classifier, Deep learning},
abstract = {The classifiers that return compact acceptance regions are crucial for the success in anomaly detection and open set recognition settings since we have to determine and reject the anomalies and samples coming from the unknown classes. This paper introduces novel methods that approximate the class acceptance regions with compact hypersphere models for anomaly detection and open set recognition. As opposed to the other deep hypersphere classifiers, we treat the hypersphere centers as learnable parameters and update them based on the changing deep feature representations. In addition, we propose novel loss terms that are more robust to the noisy labels within the outlier exposure and background datasets. The proposed methods bear similarity to the deep distance metric learning classifiers using the triplet loss function with the exception that the anchors are set to the hypersphere centers which are updated dynamically. The experimental results show that the proposed methods achieve the state-of-the-art accuracies on the majority of the tested datasets in the context of anomaly detection and open set recognition.}
}
@article{MOUSAVI2023109353,
title = {A generalized multi-aspect distance metric for mixed-type data clustering},
journal = {Pattern Recognition},
volume = {138},
pages = {109353},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109353},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000547},
author = {Elahe Mousavi and Mohammadreza Sehhati},
keywords = {Clustering, Mixed data, Ordinal and nominal attribute, Inter-dependency, Intra-attribute information, Mutual information},
abstract = {Distance calculation is straightforward when working with pure categorical or pure numerical data sets. Defining a unified distance to improve the clustering performance for a mixed data set composed of nominal, ordinal, and numerical attributes is very challenging due to the attributes’ different natures. In this study, we proposed a new measure of distance for a mixed-type data set that regards inter-attribute information and intra-attribute information depending on the type of attributes. In this regard, entropy and Jensen–Shannon divergence concepts were used to exploit the inter-attribute information of categorical-categorical and categorical-numerical attributes, respectively. Also, a modified version of Mahalanobis distance was proposed to consider the intra- and inter-attribute information of numerical attributes. We also introduced a unified framework based on mutual information to control attributes’ contribution to distance measurement. The proposed distance in conjunction with spectral clustering was extensively evaluated concerning various categorical, numerical, and mixed-type benchmark data sets, and the results demonstrated the efficacy of the proposed method.}
}
@article{KOPOREC2023109397,
title = {Human-centered deep compositional model for handling occlusions},
journal = {Pattern Recognition},
volume = {138},
pages = {109397},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109397},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000985},
author = {Gregor Koporec and Janez Perš},
keywords = {Convolutional neural networks, Hierarchical compositonal model, Instance segmentation, Occlusion handling, Discriminability, Generalizability, Interpretability, Domain knowledge},
abstract = {Despite their powerful discriminative abilities, Convolutional Neural Networks (CNNs) lack the properties of generative models. This leads to a decreased performance in environments where objects are poorly visible. Solving such a problem by adding more training samples can quickly lead to a combinatorial explosion, therefore the underlying architecture has to be changed instead. This work proposes a Human-Centered Deep Compositional model (HCDC) that combines low-level visual discrimination of a CNN and the high-level reasoning of a Hierarchical Compositional model (HCM). Defined as a transparent model, it can be optimized to real-world environments by adding compactly encoded domain knowledge from human studies and physical laws. The new FridgeNetv2 dataset and a mixture of publicly available datasets are used as a benchmark. The experimental results show the proposed model is explainable, has higher discriminative and generative power, and better handles the occlusion than the current state-of-the-art Mask-RCNN in instance segmentation tasks.}
}
@article{LI2023109381,
title = {Deep metric learning for few-shot image classification: A Review of recent developments},
journal = {Pattern Recognition},
volume = {138},
pages = {109381},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109381},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000821},
author = {Xiaoxu Li and Xiaochen Yang and Zhanyu Ma and Jing-Hao Xue},
keywords = {Few-shot learning, Metric learning, Image classification, Deep neural networks},
abstract = {Few-shot image classification is a challenging problem that aims to achieve the human level of recognition based only on a small number of training images. One main solution to few-shot image classification is deep metric learning. These methods, by classifying unseen samples according to their distances to few seen samples in an embedding space learned by powerful deep neural networks, can avoid overfitting to few training images in few-shot image classification and have achieved the state-of-the-art performance. In this paper, we provide an up-to-date review of deep metric learning methods for few-shot image classification from 2018 to 2022 and categorize them into three groups according to three stages of metric learning, namely learning feature embeddings, learning class representations, and learning distance measures. Under this taxonomy, we identify the trends of transitioning from learning task-agnostic features to task-specific features, from simple computation of prototypes to computing task-dependent prototypes or learning prototypes, from using analytical distance or similarity measures to learning similarities through convolutional or graph neural networks. Finally, we discuss the current challenges and future directions of few-shot deep metric learning from the perspectives of effectiveness, optimization and applicability, and summarize their applications to real-world computer vision tasks.}
}
@article{ZENG2023109337,
title = {Beyond OCR + VQA: Towards end-to-end reading and reasoning for robust and accurate textvqa},
journal = {Pattern Recognition},
volume = {138},
pages = {109337},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109337},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000389},
author = {Gangyan Zeng and Yuan Zhang and Yu Zhou and Xiaomeng Yang and Ning Jiang and Guoqing Zhao and Weiping Wang and Xu-Cheng Yin},
keywords = {Textvqa, End-to-end, Scene text reading, Scene text reasoning},
abstract = {Text-based visual question answering (TextVQA), which answers a visual question by considering both visual contents and scene texts, has attracted increasing attention recently. Most existing methods employ an optical character recognition (OCR) module as a pre-processor to read texts, then combine it with a visual question answering (VQA) framework. However, inaccurate OCR results may lead to cumulative error propagation, and the correlation between text reading and text-based reasoning is not fully exploited. In this work, we integrate OCR into the flow of TextVQA, targeting the mutual reinforcement of OCR and VQA tasks. Specifically, a visually enhanced text embedding module is proposed to predict semantic features from the visual information of texts, by which texts can be reasonably understood even without accurate recognition. Further, two elaborate schemes are developed to leverage contextual information in VQA to modify OCR results. The first scheme is a reading modification module that adaptively selects the answer results according to the contexts. Second, we propose an efficient end-to-end text reading and reasoning network, where the downstream VQA signal contributes to the optimization of text reading. Extensive experiments show that our method outperforms existing alternatives in terms of accuracy and robustness, whether ground truth OCR annotations are used or not.}
}
@article{ZHANG2023109373,
title = {A graph model-based multiscale feature fitting method for unsupervised anomaly detection},
journal = {Pattern Recognition},
volume = {138},
pages = {109373},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109373},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000742},
author = {Fanghui Zhang and Shichao Kan and Damin Zhang and Yigang Cen and Linna Zhang and Vladimir Mladenovic},
keywords = {Anomaly detection, Unsupervised learning, Graph model, Feature fitting representation},
abstract = {Anomaly detection and localization without prior knowledge is a challenging problem in industrial manufacturing due to the complexity and variety of anomaly types. Most of the existing methods have achieved considerable anomaly detection performance based on the distance between normal features and abnormal features. However, when the defect area is hard to distinguish from the background or the defect area is small, the distance between normal and abnormal features will be too close to detect anomaly areas. In addition, existing methods do not consider the influences of features in different layers with different anomaly sizes. In this paper, a graph model-based multiscale feature fitting method is proposed for unsupervised anomaly detection. Specifically, we build a graph model based on the K nearest neighbors of an anchor image. The feature fitting and anomaly scores of the anchor images in the graph vertices are calculated next. Finally, a weighted multiscale anomaly map matching method is proposed to detect and locate the anomaly regions of test images. Compared with the state-of-the-art methods, our proposed method achieves competitive improvement in anomaly detection and localization on the MVTec AD dataset, the two KolektorSDD datasets, and the mSTC dataset.}
}
@article{LEE2023109380,
title = {Robust spherical principal curves},
journal = {Pattern Recognition},
volume = {138},
pages = {109380},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109380},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300081X},
author = {Jongmin Lee and Hee-Seok Oh},
keywords = {Dimension reduction, Robustness, Measure of central tendency, Spherical domain},
abstract = {Principal curves are a nonlinear generalization of principal components and go through the mean of data lying in Euclidean space. In this paper, we propose L1-type and Huber-type principal curves through the median of data to robustify the principal curves for a dataset that may contain outliers. We further investigate the stationarity of the proposed robust principal curves on S2. Results from numerical experiments on S2 and S4, including real data analysis, manifest promising empirical features of the proposed method.}
}
@article{CHEN2023109369,
title = {Unsupervised person re-identification via multi-domain joint learning},
journal = {Pattern Recognition},
volume = {138},
pages = {109369},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109369},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000705},
author = {Feng Chen and Nian Wang and Jun Tang and Pu Yan and Jun Yu},
keywords = {Person re-identification, Data augmentation, Domain adaptation, Unsupervised learning},
abstract = {Deep learning techniques have achieved impressive progress in the task of person re-identification. However, how to generalize a learned model from the source domain to the target domain remains a long-standing challenge. Inspired by the fact that the enrichment of data diversity and the utilization of miscellaneous semantic features can lead to better generalization ability, we design a model that integrates a novel data augmentation method with a multi-label assignment strategy to achieve semantic features decoupling in the source domain. The pre-trained model is employed to extract several kinds of semantic features from the target dataset, and each kind of semantic features is regarded as a specific domain. We then cluster features of each domain and exploit the connection between different clustering results to perform self-distillation for generating more reliable pseudo labels. Finally, the obtained pseudo labels are used to fine-tune the pre-trained model to achieve model transfer from the source domain to the target one. Extensive experiments demonstrate that our approach outperforms some state-of-the-art methods by a clear margin and even surpass some supervised methods. Source code is available at: https://www.github.com/flychen321/MDJL.}
}
@article{XU2023109338,
title = {Computation-Efficient Knowledge Distillation via Uncertainty-Aware Mixup},
journal = {Pattern Recognition},
volume = {138},
pages = {109338},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109338},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000390},
author = {Guodong Xu and Ziwei Liu and Chen Change Loy},
keywords = {Knowledge distillation, Training cost},
abstract = {Knowledge distillation (KD) has emerged as an essential technique not only for model compression, but also other learning tasks such as continual learning. Given the richer application spectrum and potential online usage of KD, knowledge distillation efficiency becomes a pivotal component. In this work, we study this little-explored but important topic. Unlike previous works that focus solely on the accuracy of student network, we attempt to achieve a harder goal – to obtain a performance comparable to conventional KD with a lower computation cost during the transfer. To this end, we present UNcertainty-aware mIXup (UNIX), an effective approach that can reduce transfer cost by 20% to 30% and yet maintain comparable or achieve even better student performance than conventional KD. This is made possible via effective uncertainty sampling and a novel adaptive mixup approach that select informative samples dynamically over ample data and compact knowledge in these samples. We show that our approach inherently performs hard sample mining. We demonstrate the applicability of our approach to improve various existing KD approaches by reducing their queries to a teacher network. Extensive experiments are performed on CIFAR100 and ImageNet. Code and model are available at https://github.com/xuguodong03/UNIXKD.}
}
@article{ZHONG2023109427,
title = {Geometric algebra-based multiview interaction networks for 3D human motion prediction},
journal = {Pattern Recognition},
volume = {138},
pages = {109427},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109427},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001280},
author = {Jianqi Zhong and Wenming Cao},
keywords = {Human motion prediction, Geometric algebra, Motion generation},
abstract = {3D skeleton-based human motion prediction is an essential and challenging task for human-machine interactions, which aims to forecasts future poses given a history of their previous motions. Recent works based on Graph Neural Networks (GCNs) show promising performance for motion prediction due to the powerful ability of feature aggregation of GCNs. However, with the deep and multi-stage GCN model deployment, its feature extraction mechanism tends to result in feature similarity over all joints, and degrade the prediction performance. In addition, such a graph structure in recent works was still insufficient to process the high dimensional structural data in Euclidean space when inference through multi-layer networks. To solve the problem, we propose a novel Geometric Algebra-based Multi-view Interaction network (GA-MIN), which captures and aggregates motion features from two interactions: 1) global-interaction, which refactors various spectrum dependencies using geometric algebra-based structure, and 2) self-interaction, which leverage self-attention mechanism to capture compact representations. Extensive experiments are conducted on three public datasets: Human3.6M, CMU Mocap, and 3DPW, which prove that the proposed GA-MIN outperforms state-of-the-art methods on 3D Mean Per Joint Position Error (MPJPE) and Mean Angle Error (MAE) on average.}
}
@article{LUO2023109431,
title = {Infrared and visible image fusion based on Multi-State contextual hidden Markov Model},
journal = {Pattern Recognition},
volume = {138},
pages = {109431},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109431},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001322},
author = {Xiaoqing Luo and Yuting Jiang and Anqi Wang and Juan Wang and Zhancheng Zhang and Xiao-Jun Wu},
keywords = {Image fusion, non-subsampled Shearlet transform, contextual hidden Markov model, multi-state, soft context variable},
abstract = {In this paper, we propose a novel multi-state contextual hidden Markov model (MCHMM) in the non-subsampled Shearlet transform (NSST) domain for image fusion. The traditional two-state hidden Markov model divides the multi-scale coefficients only into large and small states, which can lead to an inaccurate statistical model and reduce the quality of the fusion result. Our method improves upon this by developing a multi-state model and a soft context variable to provide a fine-grained representation of the high-frequency subbands, resulting in improved fusion results. Additionally, the fusion of low-frequency subbands is performed on the difference of regional energy to ensure visual quality. Our experimental results on several datasets demonstrate that the proposed method outperforms other fusion methods in both subjective and objective evaluations.}
}
@article{DHAMANASKAR2023109358,
title = {Enhancing egocentric 3D pose estimation with third person views},
journal = {Pattern Recognition},
volume = {138},
pages = {109358},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109358},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000596},
author = {Ameya Dhamanaskar and Mariella Dimiccoli and Enric Corona and Albert Pumarola and Francesc Moreno-Noguer},
keywords = {3D pose estimation, Self-supervised learning, Egocentric vision},
abstract = {We propose a novel approach to enhance the 3D body pose estimation of a person computed from videos captured from a single wearable camera. The main technical contribution consists of leveraging high-level features linking first- and third-views in a joint embedding space. To learn such embedding space we introduce First2Third-Pose, a new paired synchronized dataset of nearly 2000 videos depicting human activities captured from both first- and third-view perspectives. We explicitly consider spatial- and motion-domain features, combined using a semi-Siamese architecture trained in a self-supervised fashion. Experimental results demonstrate that the joint multi-view embedded space learned with our dataset is useful to extract discriminatory features from arbitrary single-view egocentric videos, with no need to perform any sort of domain adaptation or knowledge of camera parameters. An extensive evaluation demonstrates that we achieve significant improvement in egocentric 3D body pose estimation performance on two unconstrained datasets, over three supervised state-of-the-art approaches. The collected dataset and pre-trained model are available for research purposes.11https://github.com/nudlesoup/First2Third-Pose}
}
@article{LIN2023109342,
title = {Multiview Jointly Sparse Discriminant Common Subspace Learning},
journal = {Pattern Recognition},
volume = {138},
pages = {109342},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109342},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000432},
author = {Yiling Lin and Zhihui Lai and Jie Zhou and Jiajun Wen and Heng Kong},
keywords = {Feature extraction, Small-class problem, Multiview classification, Discriminant common-space learning},
abstract = {Multiview data leads to the demand for classifying samples from various views, and the large gap between different views makes the classification task challenging. Recently, researchers have extended linear discriminant analysis (LDA) to multi-view scenarios. However, the extended methods are generally associated with the small-class problem, that is, the projection size is limited by the number of classes. In addition, they are sensitive to variations in images or outliers. To solve these problems, this study proposes a generalized robust multiview discriminant analysis (GRMDA) to obtain a linear transform for each view and for learning multiview jointly sparse discriminant common subspace. GRMDA aims to achieve both maximal between-class and minimal within-class variation for data from multiple views in a common space. Instead of formulating the ratio trace problem, we reformulate GRMDA inspired by maximum margin criterion (MMC) to address the small-class problem. Moreover, the proposed method achieves stronger robustness by reconstructing the within-class and between-class scatter terms from the definition of L2,1 norm. Furthermore, GRMDA ensures joint sparsity using the L2,1 norm-based regularization term. Additionally, we present an iterative algorithm, convergence proof, and complexity analysis. Experiments on six popular databases, that is, COIL100, USPS/MNIST, Extended Yale Face B, AR, BBCSport, and multiple feature datasets, were conducted to evaluate the performance of GRMDA against the state-of-the-art multiview methods. The experimental results demonstrate that the proposed method can achieve a significant performance with strong robustness and fast convergence.}
}
@article{LI2023109423,
title = {Dynamic graph structure learning for multivariate time series forecasting},
journal = {Pattern Recognition},
volume = {138},
pages = {109423},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109423},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001243},
author = {Zhuo Lin Li and Gao Wei Zhang and Jie Yu and Ling Yu Xu},
keywords = {Multivariate time series forecasting, Dynamic spatio-temporal dependencies, Graph neural networks, Long- and short-term patterns, Graph structure learning},
abstract = {Multivariate time series forecasting is a challenging task because the dynamic spatio-temporal dependencies between variables are a combination of multiple unknown association patterns. Existing graph neural networks typically model multivariate relationships with a predefined spatial graph or a learned fixed adjacency graph, which fails to handle the aforementioned challenges. In this study, we decompose association patterns into stable long-term and dynamic short-term patterns and propose a novel framework, named the static and dynamic graph learning network (SDGL), for modeling unknown patterns. Our approach infers two types of graph structures, from the data simultaneously: static and dynamic graphs. A static graph is developed to capture the fixed long-term pattern via node embedding, and we leverage graph regularity to control its learning direction. Dynamic graphs, which are time-varying matrices based on changing node-level features, are used to model dynamic dependencies over the short term. To effectively capture local dynamic patterns, we integrate the learned long-term pattern as an inductive bias. Experiments on six benchmark datasets show the state-of-the-art performance of our method. Analysis of the learned graphs reveals that the model succeeds in modeling dynamic spatio-temporal dependencies.}
}
@article{CHEN2023109386,
title = {RACL: A robust adaptive contrastive learning method for conversational satisfaction prediction},
journal = {Pattern Recognition},
volume = {138},
pages = {109386},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109386},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000870},
author = {Gang Chen and Xiangge Li and Shuaiyong Xiao and Chenghong Zhang and Xianghua Lu}
}
@article{MENSI2023109334,
title = {Detecting outliers from pairwise proximities: Proximity isolation forests},
journal = {Pattern Recognition},
volume = {138},
pages = {109334},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109334},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000353},
author = {Antonella Mensi and David M.J. Tax and Manuele Bicego},
keywords = {Random forest, Outlier detection, Isolation, Pairwise distances},
abstract = {Because outliers are very different from the rest of the data, it is natural to represent outliers by their distances to other objects. Furthermore, there are many scenarios in which only pairwise distances are known, and feature-based outlier detection methods cannot directly be applied. Considering these observations, and given the success of Isolation Forests for (feature-based) outlier detection, we propose Proximity Isolation Forest, a proximity-based extension. The methodology only requires a set of pairwise distances to work, making it suitable for different types of data. Analogously to Isolation Forest, outliers are detected via their early isolation in the trees; to encode the isolation we design nine training strategies, both random and optimized. We thoroughly evaluate the proposed approach on fifteen datasets, successfully assessing its robustness and suitability for the task; additionally we compare favourably to alternative proximity-based methods.}
}
@article{LIU2023109415,
title = {Accurate light field depth estimation under occlusion},
journal = {Pattern Recognition},
volume = {138},
pages = {109415},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109415},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001164},
author = {Yuxuan Liu and Mitko Aleksandrov and Zhihua Hu and Yan Meng and Li Zhang and Sisi Zlatanova and Haibin Ai and Pengjie Tao},
keywords = {Light field, Depth estimation, EPI, Multi-view depth maps integration, Occlusion handling},
abstract = {Epipolar plane images (EPIs) have advantages in light field depth estimation, but the current EPI-based methods only use one aspect of the information related to the line or its surroundings in EPIs. Moreover, most current methods merely extract the depth map of the target view, ignoring the depth information from other views. To fully utilize the available information, we first introduce a novel data cost by comprehensively utilizing the characteristics of pixel consistency on the line and region difference around the line in EPIs to improve the robustness against occlusion and noise. Then, we put forward a multi-view depth integration strategy that copes with weak texture and occlusion areas. Finally, an edging preserving filter is applied to further refine the depth map. Experiments on synthetic and real light field datasets show that the proposed method outperforms the state-of-the-art light field depth estimation algorithms, especially in the presence of occluded pixels.}
}
@article{LIU2023109418,
title = {Mitigate the classification ambiguity via localization-classification sequence in object detection},
journal = {Pattern Recognition},
volume = {138},
pages = {109418},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109418},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300119X},
author = {Chang Liu and Shaorong Xie and Xiaomao Li and Jiantao Gao and Weiping Xiao and Baojie Fan and Yan Peng},
keywords = {Object detection, Classification ambiguity, Refinement-aware classification},
abstract = {In anchor-based detectors, the confidence scores and label-assignment results for the classification task are determined by the unrefined anchors rather than the final-refined boxes, which causes classification ambiguity due to the lack of correlation between the classification and localization tasks. In this paper, we investigate the classification ambiguity thoroughly via extensive experiments, and present the localization-classification sequence detector (LCSDet) that performs localization and classification in order, bridging the gap between them. To achieve this, the refinement-aware (RA) classification branch and RA assignment are proposed in LCSDet. In inference, the RA classification branch rectifies the feature misalignment and directly classifies the refined anchors. During training, the RA assignment tackles the training instability, narrows the location-quality gap and assigns the refined anchors to ground-truth objects. Comprehensive experiments indicate that the LCSDet can effectively mitigate the classification ambiguity and achieve stable improvement across different baselines.}
}
@article{SUN2023109399,
title = {MUNet: Motion uncertainty-aware semi-supervised video object segmentation},
journal = {Pattern Recognition},
volume = {138},
pages = {109399},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109399},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001000},
author = {Jiadai Sun and Yuxin Mao and Yuchao Dai and Yiran Zhong and Jianyuan Wang},
keywords = {Video object segmentation, Uncertainty, Motion estimation, Self-supervised},
abstract = {The task of semi-supervised video object segmentation (VOS) has been greatly advanced and state-of-the-art performance has been made by dense matching-based methods. The recent methods leverage space-time memory (STM) networks and learn to retrieve relevant information from all available sources, where the past frames with object masks form an external memory and the current frame as the query is segmented using the mask information in the memory. However, when forming the memory and performing matching, these methods only exploit the appearance information while ignoring the motion information. In this paper, we advocate for the return of the motion information and propose a motion uncertainty-aware framework (MUNet) for semi-supervised VOS. First, we propose an implicit method to learn the spatial correspondences between neighboring frames, building upon a correlation cost volume. To handle the challenging cases of occlusion and textureless regions during constructing dense correspondences, we incorporate the uncertainty in dense matching and achieve motion uncertainty-aware feature representation. Second, we introduce a motion-aware spatial attention module to effectively fuse the motion features with the semantic features. Comprehensive experiments on challenging benchmarks show that using a small amount of data and combining it with powerful motion information can bring a significant performance boost. We achieve 76.5% J&F only using DAVIS17 for training22This result is initialized by the Mask-RCNN-ResNet50 weights pre-trained on COCO dataset. By initialization from ResNet50 pre-trained on ImageNet dataset, we can achieve 75.0% J&F, which is still the SOTA performance., which significantly outperforms the SOTA methods under the low-data protocol. The code and supplementary materials will be available at https://npucvr.github.io/MUNet.}
}
@article{ZHANG2023109378,
title = {MFSJMI: Multi-label feature selection considering join mutual information and interaction weight},
journal = {Pattern Recognition},
volume = {138},
pages = {109378},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109378},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000791},
author = {Ping Zhang and Guixia Liu and Jiazhi Song},
keywords = {Multi-label learning, Multi-label feature selection, Information theory, Underlying assumptions},
abstract = {Multi-label feature selection captures a reliable and informative feature subset from high-dimensional multi-label data, which plays an important role in pattern recognition. In conventional information-theoretical based multi-label feature selection methods, the high-order feature relevance between feature and label set is evaluated using low-order mutual information. However, existing methods do not establish the theoretical basis for the low-order approximation. To fill this gap, we first identify two underlying assumptions based on high-order label distribution: Label Independence Assumption (LIA) and Paired-label Independence Assumption (PIA). Second, we systematically analyze the strengths and weaknesses of two assumptions and introduce joint mutual information to satisfy more realistic label distribution. Furthermore, by decomposing joint mutual information, an interaction weight is proposed to consider multiple label correlations. Finally, a new method considering join mutual information and interaction weight is proposed. Comprehensive experiments demonstrate the effectiveness of the proposed method on various evaluation metrics.}
}
@article{REN2023109362,
title = {Surface normal and Gaussian weight constraints for indoor depth structure completion},
journal = {Pattern Recognition},
volume = {138},
pages = {109362},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109362},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000638},
author = {Dongran Ren and Meng Yang and Jiangfan Wu and Nanning Zheng},
keywords = {Depth structure completion, Surface normal, Gaussian weight, Convolution neural network, Markov random field, Kernel least-square method},
abstract = {Raw depth maps captured by depth sensors generally contain missing contents due to glossy, transparent, and sparsity problems. Recent methods well completed flat regions of raw depth maps; however, ignored the accuracy of depth structures. In this paper, an effective depth structure completion method is developed to infer missing depth structures. First, a raw depth map is divided into flat regions and depth structures based on a structure prediction network. Second, two local features including surface normals and Gaussian weights are extracted from a reference RGB image to impose constraints on flat regions and depth structures, separately. Third, a kernel least-square module is adopted to handle the texture-copy artifacts problem. Finally, an iterative optimization model is developed by embedding the two constraints into a Markov random field. The cost function of the model comprises three terms, which limit data fidelity between completed depth map and raw depth map, smoothness of flat regions, and accuracy of depth structures, respectively. The proposed method is evaluated on four indoor datasets including Matterport3D, RealSense, ScanNet, and NYUv2, and compared with eight recent baselines. Quantitative results demonstrate that RMSE and MAE of completed depth maps are considerably reduced by 22.0% and 45.3%, respectively. Visual results show the superiority in completing depth structures and suppressing texture-copy artifacts. Generalization test verify the effectiveness on unseen datasets.}
}
@article{NIU2023109382,
title = {Defense Against Adversarial Attacks with Efficient Frequency-Adaptive Compression and Reconstruction},
journal = {Pattern Recognition},
volume = {138},
pages = {109382},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109382},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000833},
author = {Zhong-Han Niu and Yu-Bin Yang},
keywords = {Deep neural networks, Adversarial defense, Adversarial robustness, Closed-set attack, Open-set attack},
abstract = {The increasing use of deep neural networks exposes themselves to adversarial attacks in the real world drawn from closed-set and open-set, which poses great threats to their application in safety-critical systems. Since adversarial attacks tend to mislead an original model by adding small perturbations into clean images, an intuitive idea of defensing adversarial attacks is eliminating perturbations as much as possible to mitigate attacking effects. However, such elimination-based strategies unfortunately fail to achieve satisfactory robustness. Aiming to investigate the intrinsic reasons for this phenomenon, systematic experiments are carried out in this paper to indicate that even a 20% residual perturbation can still preserve and exhibit attacking effects as strong as a full one. Our study also indicates that there are strong correlations between perturbations and legitimate images. Thus, breaking the correlation across multiple bands is more effective in mitigating attacking effects. Based on these findings, this paper proposes an efficient defense strategy called “Frequency-Adaptive Compression and rEconstruction (FACE)” to improve the robustness of the model to adversarial attacks. Specifically, low-frequency bands containing semantic information are compressed by a down-sampling operation, while the channel width of high-frequency bands is squeezed and further compressed by adding noise before the Tanh activating function. Meanwhile, attachment spaces of perturbations are also squeezed to the extent as much as possible. Finally, a clean output is obtained by upsampling together with expanded reconstruction. Experiments are extensively conducted on widely used datasets to demonstrate the effectiveness of the proposed method. For closed-set attacks, FACE outperforms the STOA elimination-based methods on ImageNet, achieving a 27.9% improvement. For the MNIST open-set attacks, it not only reduces the success rate of targeted attack by a large margin (from 100% to 24.7%), but also mitigates attacking effects with an FPR-95 value of 0.3.}
}
@article{ZHANG2023109354,
title = {Self-structured pyramid network with parallel spatial-channel attention for change detection in VHR remote sensed imagery},
journal = {Pattern Recognition},
volume = {138},
pages = {109354},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109354},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000559},
author = {Mingyang Zhang and Hanhong Zheng and Maoguo Gong and Yue Wu and Hao Li and Xiangming Jiang},
keywords = {Change detection, VHR remote sensing images, Feature pyramids, Attention mechanisms, Deep learning},
abstract = {Land cover change detection (CD) in very-high-resolution (VHR) images is still impeded by weak pattern separability and land cover complexity. To address these challenges, a self-structured pyramid network (S2PNet) with a parallel spatial-channel attention mechanism (PSAM) and a self-structured feature pyramid (SFP) is proposed for a finer annotation of changed land cover. The proposed PSAM refines the features of different levels in dual-branch coordinated by running parallel without mutual influence for a better recognition of varied objects, which can lead to less incorrectly detected land cover. And the SFP integrates the embedded multi-scale features to acquire an improved cognition over multi-scale objects, which can contribute to a more complete annotation over diverse objects. All-round experiments over several widely used open large-scale VHR CD data sets are carried out, which indicate the efficiency and effectiveness of the proposed method. Related comparisons suggest that the proposed method can achieve higher performance over several existing state-of-the-art CD methods. The source codes will be released at https://github.com/HaiXing-1998/S2PNet-CD.}
}
@article{ZHAO2023109374,
title = {Weight-guided class complementing for long-tailed image recognition},
journal = {Pattern Recognition},
volume = {138},
pages = {109374},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109374},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000754},
author = {Xinqiao Zhao and Jimin Xiao and Siyue Yu and Hui Li and Bingfeng Zhang},
keywords = {Image recognition, Long-tailed distribution, Gradient shift, Weight-guided method},
abstract = {Real-world data are often long-tailed distributed and have plenty classes. This characteristic leads to a significant performance drop for various models. One reason behind that is the gradient shift caused by unsampled classes in each training iteration. In this paper, we propose a Weight-Guided Class Complementing framework to address this issue. Specifically, this framework first complements the unsampled classes in each training iteration by using a dynamic updated data slot. Then, considering the over-fitting issue caused by class complementing, we utilize the classifier weights as learned knowledge and encourage the model to discover more class specific characteristics. Finally, we design a weight refining scheme to deal with the long-tailed bias existing in classifier weights. Experimental results show that our framework can be implemented upon different existing approaches effectively, achieving consistent improvements on various benchmarks with new state-of-the-art performances. Codes will be released.}
}
@article{LI2023109398,
title = {Human-related anomalous event detection via memory-augmented Wasserstein generative adversarial network with gradient penalty},
journal = {Pattern Recognition},
volume = {138},
pages = {109398},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109398},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000997},
author = {Nanjun Li and Faliang Chang and Chunsheng Liu},
keywords = {Human-related anomalous event detection, Video surveillance, Human skeleton trajectories, Wasserstein generative adversarial network with gradient penalty, Memory module},
abstract = {Timely detection of human-related anomaly in surveillance videos is a challenging task. Generally, the irregular human motion and action patterns can be regarded as abnormal human-related events. In this paper, we utilize the skeleton trajectories to learn the regularities of human motion and action in videos for anomaly detection. The skeleton trajectories are decomposed into global and local feature sequences, which are utilized to provide human motion and action information, respectively. Then, the global and local sequences are modeled as two separate sub-processes with our proposed Memory-augmented Wasserstein Generative Adversarial Network with Gradient Penalty (MemWGAN-GP). In each sub-process, the pre-trained MemWGAN-GP is employed to predict future feature sequences from corresponding input past sequences and reconstruct the input sequences simultaneously. The predicted and reconstructed feature sequences are compared with their groundtruth to identify anomalous sequences. The MemWGAN-GP integrates the autoencoder with a WGAN model to boost the reconstruction and prediction ability of the autoencoder. Besides, a memory module is employed in MemWGAN-GP to overcome high capacity of the autoencoder for anomalies reconstruction and prediction. Experimental results on four challenging datasets demonstrate advantages of the proposed method over other state-of-the-art algorithms.}
}
@article{WANG2023109377,
title = {Cascaded feature fusion with multi-level self-attention mechanism for object detection},
journal = {Pattern Recognition},
volume = {138},
pages = {109377},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109377},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300078X},
author = {Chuanxu Wang and Huiru Wang},
keywords = {Cascaded feature fusion, Multi-level self-attention mechanism, Space-channel feature correlation, Object detection},
abstract = {Object detection has been a challenging task due to the complexity and diversity of objects. The emergence of self-attention mechanism provides a new clue for feature fusion in object detection task. Most existing self-attention mechanisms focus on extracting the correlation between global and local information in space or among channels, however it remains problematic issues of how to effectively fuse all those features. To address the above problems, we propose a Pooling and Global feature Fusion Self-attention Mechanism (PGFSM) to capture multi-level correlations among a variety of features, so as to perform cascaded aggregations upon them. PGFSM consists of three parts: Spatial Self-attention Pooling Fusion Module (SSPFM), Channel Self-attention Pooling Fusion Module (CSPFM), and Spatial and Channel Global Self-attention Fusion Module (SCGSFM). SSPFM and CSPFM respectively carried out in space and channel, extract the global maximum pooling and global average pooling self-attention features; SCGSFM extracts the spatial and channel fused characteristic relationship in the global. Finally, the three fused feature relations are added on the original feature to achieve an enhanced trait representation. In test, our PGFSM is embedded into YOLOv4, YOLOv5, and EfficientDet network respectively, and evaluated in PASCAL VOC and MS COCO datasets. The experiment results show that the feature fusion self-attention mechanism improves the performance of object detection compared to each original framework and also the state-of-the-art modules, which proves the effectiveness of our method.}
}
@article{LI2023109366,
title = {A single-stage point cloud cleaning network for outlier removal and denoising},
journal = {Pattern Recognition},
volume = {138},
pages = {109366},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109366},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000675},
author = {Ying Li and Huankun Sheng},
keywords = {Point cloud cleaning, Denoising, Outlier removal, Neural networks},
abstract = {As a simple, flexible and effective representation for objects, 3D point cloud has attracted more and more attention in recent years. However, raw point clouds obtained from 3D scanners or image-based reconstruction techniques are often contaminated with noise and outliers, which hinders downstream tasks such as object classification, surface reconstruction, and so on. Therefore, point cloud cleaning, i.e., removing noisy points and outliers from raw point cloud, is a prior step of most geometry processing workflows. The exiting techniques for point cloud cleaning usually include two stages, that is, discarding outliers at first, and then denoising the resulting point cloud. This two-stage process usually requires two different models, which is cumbersome to train and use. To solve this problem, a novel data driven method, named SSPCN (single-stage point cloud cleaning network), is proposed in this paper. SSPCN can simultaneously remove outliers and denoise a point cloud in a single model. Specifically, SSPCN is consisted of adaptive downsampling module, feature compensation module, upsampling module and coordinate reconstruction module. Given a raw point cloud as input, the downsampling module is first used to obtain a prefiltered point cloud subset and learn initial features of the subset. The feature compensation module is then utilized to learn accurate features from initial features. Next, the upsampling module upsamples the features to restore the original size of the point cloud. Last, the coordinate reconstruction module generates a cleaned point cloud from upsampled features. SSPCN is validated both on synthetic and real scanned data. Extensive experiments demonstrate that SSPCN outperforms state-of-the-art point cloud cleaning techniques in terms of quantitative metric and visual quality.}
}
@article{WANG2023109350,
title = {Self-supervised clustering with assistance from off-the-shelf classifier},
journal = {Pattern Recognition},
volume = {138},
pages = {109350},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109350},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000511},
author = {Hanxuan Wang and Na Lu and Huan Luo and Qinyang Liu},
keywords = {Deep clustering, Classification, Self-supervised, Sample selection},
abstract = {Deep clustering outperforms conventional clustering by mutually promoting representation learning and cluster assignment. However, most existing deep clustering methods suffer from two major drawbacks. Firstly, most cluster assignment methods are highly dependent on the intermediate target distribution generated by a handcrafted nonlinear mapping function. Secondly, the clustering results can be easily guided towards wrong direction by the misassigned samples in each cluster. The existing deep clustering methods are incapable of discriminating such samples. These facts largely limit the possible performance that deep clustering methods can reach. To address these issues, a novel Self-Supervised Clustering (SSC) framework is constructed, which boosts the clustering performance by classification in an unsupervised manner. Fuzzy theory is used to score the membership of each sample to the clusters in terms of probability in each training epoch, which evaluates the intermediate clustering result certainty of each sample. The most reliable samples can be selected with the help of a sample selection method according to the membership and enhanced by data augmentation method. These augmented data are employed to fine-tune an off-the-shelf deep network classifier with the labels provided by the clustering in a self-supervised way. The classification results of the original dataset are used as the target distribution to guide the training process of the deep clustering model. The proposed framework can efficiently discriminate sample outliers and generate better target distribution with the assistance of the powerful classifier. Extensive experiments indicate that the proposed framework remarkably outperforms state-of-the-art deep clustering methods on four benchmark datasets.}
}