@article{ZHANG2023109235,
title = {Meta-hallucinating prototype for few-shot learning promotion},
journal = {Pattern Recognition},
volume = {136},
pages = {109235},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109235},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007142},
author = {Lei Zhang and Fei Zhou and Wei Wei and Yanning Zhang},
keywords = {Few-shot learning, Prototype hallucination, Meta-learning},
abstract = {An effective way for few-shot learning (FSL) is to establish a metric space where the distance between a query and the prototype of each class is computed for classification, and the key lies on hallucinating the appropriate prototypes for each class of the given FSL task. Most existing prototypical approaches hallucinate the class-wise prototype based on the given support samples with an equal contribution assumption, i.e., each support sample contributes equally to the corresponding prototype. However, due to the limited-data regime as well as the strict assumption, the hallucinated prototypes often deviate from the ideal ones that are determined by the sample distribution of each unseen class, and thus causing poor generalization performance. To mitigate this problem, we present a prototype meta-hallucination approach which shows two aspects of advantages. On one hand, instead of directly inferring the complicated sample distribution, it meta-learns to establish a difference distribution based generative model that infers the distribution of inter-sample difference and synthesizes new labeled samples through fusing the sampled inter-sample difference and each given support sample. This empowers us to augment the support set with more content-diverse samples and is beneficial to reduce the bias in prototype hallucination. On the other hand, we argue that each support sample may contribute no-equally to the ideal prototype that it belongs to and their relations vary with class characteristics. Following this, our approach meta-learns to dynamically re-weight all support samples in prototype hallucination, which makes it flexible to locate the ideal prototype for each unseen class based on its characteristics. Experiments on four FSL benchmark datasets show that our approach can effectively improve the performance of the prototypical baseline and outperform several state-of-the-art competitors with a clear margin.}
}
@article{JIANG2023109227,
title = {Deep hybrid model for single image dehazing and detail refinement},
journal = {Pattern Recognition},
volume = {136},
pages = {109227},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109227},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007063},
author = {Nanfeng Jiang and Kejian Hu and Ting Zhang and Weiling Chen and Yiwen Xu and Tiesong Zhao},
keywords = {Haze removal, Details refinement, Image processing},
abstract = {Deep learning technologies have been applied in Single Image Dehazing (SID) tasks successfully. However, most SID algorithms seldom consider to refine image details during dehazing. Therefore, there exist some detail-loss regions in dehazed results. To solve this issue, we design a deep hybrid network to improve dehazing performance and remedy the loss of details. Different from existing algorithms that usually ignore detail refinement and adopt a unified framework to remove haze, we propose to treat dehazing and detail refinement as two separate tasks, so that each task could be solved via different ways. Particularly, we design two sub-networks with a multi-term loss function. First, for removing haze effectively, we introduce the Squeeze-and-Excitation (SE) to design a haze residual attention sub-network, which is used to reconstruct the dehazed image. Second, as for remedying details, we take the previous dehazed image as the input to a detail refinement sub-network, where the image details can be enhanced via multi-scale contextual information aggregation. Through the joint training of two sub-network, the haze can be removed clearly and the image details can be preserved well. Moreover, the detail refinement sub-network can be detached into other existing dehazing methods to improve their model performances. Extensive experiments also verify the superiority of our proposed network against recently proposed state-of-the-arts.}
}
@article{YAN2023109267,
title = {Hybrid optimization with unconstrained variables on partial point cloud registration},
journal = {Pattern Recognition},
volume = {136},
pages = {109267},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109267},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007464},
author = {Yuanjie Yan and Junyi An and Jian Zhao and Furao Shen},
keywords = {Point cloud registration, Optimization},
abstract = {3D point cloud registration is a fundamental problem in computer vision (CV) and computer graphics (CG). Recently, a series of learning-based algorithms have been proposed to show the advantages in registration accuracy and inference speed. However, those learning-based methods usually ignore transformations with constrained rotations and translations in registration. In this paper, we propose a novel hybrid optimization method to solve the constrained rotational and translational transformations. A mapping function is introduced to deal with the restrained variables in optimization. Our method achieves superior performance on the Multi-View Partial Point dataset, which won the first place on the registration challenge in ICCV 2021. The method is also validated on the synthetic datasets ModelNet, ICL-NUIM, and the realistic 3DMatch dataset. We demonstrate that the global optimization methods still have great potential research for point cloud registration. The code is available at https://github.com/Dizzy-cell/HOUV.}
}
@article{WANG2023109259,
title = {TETFN: A text enhanced transformer fusion network for multimodal sentiment analysis},
journal = {Pattern Recognition},
volume = {136},
pages = {109259},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109259},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007385},
author = {Di Wang and Xutong Guo and Yumin Tian and Jinhui Liu and LiHuo He and Xuemei Luo},
keywords = {Multimodal sentiment analysis, Transformer, Text-oriented pairwise cross-modal mappings},
abstract = {Multimodal sentiment analysis (MSA), which aims to recognize sentiment expressed by speakers in videos utilizing textual, visual and acoustic cues, has attracted extensive research attention in recent years. However, textual, visual and acoustic modalities often contribute differently to sentiment analysis. In general, text contains more intuitive sentiment-related information and outperforms nonlinguistic modalities in MSA. Seeking a strategy to take advantage of this property to obtain a fusion representation containing more sentiment-related information and simultaneously preserving inter- and intra-modality relationships becomes a significant challenge. To this end, we propose a novel method named Text Enhanced Transformer Fusion Network (TETFN), which learns text-oriented pairwise cross-modal mappings for obtaining effective unified multimodal representations. In particular, it incorporates textual information in learning sentiment-related nonlinguistic representations through text-based multi-head attention. In addition to preserving consistency information by cross-modal mappings, it also retains the differentiated information among modalities through unimodal label prediction. Furthermore, the vision pre-trained model Vision-Transformer is utilized to extract visual features from the original videos to preserve both global and local information of a human face. Extensive experiments on benchmark datasets CMU-MOSI and CMU-MOSEI demonstrate the superior performance of the proposed TETFN over state-of-the-art methods.}
}
@article{HUANG2023109255,
title = {An ensemble hierarchical clustering algorithm based on merits at cluster and partition levels},
journal = {Pattern Recognition},
volume = {136},
pages = {109255},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109255},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007348},
author = {Qirui Huang and Rui Gao and Hoda Akhavan},
keywords = {Ensemble clustering, Cluster consensus, Hyper-cluster, Merit level, Robustness measure},
abstract = {Ensemble clustering has emerged as a combination of several basic clustering algorithms to achieve high quality final clustering. However, this technique is challenging due to the complexities in primary clusters such as overlapping, vagueness, instability and uncertainty. Typically, ensemble clustering uses all the primary clusters into partitions for consensus, where the merits of a cluster or a partition can be considered to improve the quality of the consensus. In general, the robustness of a partition may be poorly measured, while having some high-quality clusters. Inspired by the evaluation of cluster and partition, this paper proposes an ensemble hierarchical clustering algorithm based on the cluster consensus selection approach. Here, the selection of a subset of primary clusters from partitions based on their merit level is emphasized. Merit level is defined using the development of Normalized Mutual Information measure. Clusters of basic clustering algorithms that satisfy the predefined threshold of this measure are selected to participate in the final consensus. In addition, the consensus of the selected primary clusters to create the final clusters is performed based on the clusters clustering technique. In this technique, the selected primary clusters are re-clustered to create hyper-clusters. Finally, the final clusters are formed by assigning instances to hyper-clusters with the highest similarity. Here, an innovative criterion based on merit and cluster size for defining similarity is presented. The performance of the proposed algorithm has been proven by extensive experiments on real-world datasets from the UCI repository compared to state-of-the-art algorithms such as CPDM, ENMI, IDEA, CFTLC and SSCEN.}
}
@article{MIAO2023109210,
title = {On better detecting and leveraging noisy samples for learning with severe label noise},
journal = {Pattern Recognition},
volume = {136},
pages = {109210},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109210},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006896},
author = {Qing Miao and Xiaohe Wu and Chao Xu and Wangmeng Zuo and Zhaopeng Meng},
keywords = {Severe label noise, Lipschitz regularization, Adaptive modeling and detection of label noise, Semi-supervised learning},
abstract = {Despite the success of learning with noisy labels, existing approaches show limited performance when the noise level is extremely high, since deep neural networks (DNNs) are easily overfit to the training set with corrupted labels. In this paper, we introduce Lipschitz regularization to prevent the DNNs from over-fitting to noisy labels quickly. Meanwhile, to better detect and leverage the noisy samples, we propose a Lipschitz regularization based framework with a combination of adaptive modeling and detection module and improved semi-supervised learning. We propose to adaptively model the real distribution of the training set, and the implicit individual clean/noisy distribution, instead of parametric models. With Bayes’ rule, we then compute the posterior probability of a sample being clean, which provides a dynamic threshold for the detection of noisy labels. To reduce training instability caused by less labeled data with severe label noise, we improve the semi-supervised learning by combining the advantages of Mixup and FixMatch. It can not only increase the diversity of unlabeled samples, but also improve the generalization capability of the DNNs to avoid over-fitting. Experiments on several benchmarks demonstrate that our approach achieves comparable results with the state-of-the-art methods in the less-noisy environment, and obtains a substantial improvement (∼ 8% and ∼ 6% in accuracy on CIFAR-10 and CIFAR-100 respectively) with severe noise.}
}
@article{WU2023109231,
title = {SpatioTemporal focus for skeleton-based action recognition},
journal = {Pattern Recognition},
volume = {136},
pages = {109231},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109231},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007105},
author = {Liyu Wu and Can Zhang and Yuexian Zou},
keywords = {Action recognition, Skeleton topology, Graph convolutional network},
abstract = {Graph convolutional networks (GCNs) are widely adopted in skeleton-based action recognition due to their powerful ability to model data topology. We argue that the performance of recent proposed skeleton-based action recognition methods is limited by the following factors. First, the predefined graph structures are shared throughout the network, lacking the flexibility and capacity to model the multi-grain semantic information. Second, the relations among the global joints are not fully exploited by the graph local convolution, which may lose the implicit joint relevance. For instance, actions such as running and waving are performed by the co-movement of body parts and joints, e.g., legs and arms, however, they are located far away in physical connection. Inspired by the recent attention mechanism, we propose a multi-grain contextual focus module, termed MCF, to capture the action associated relation information from the body joints and parts. As a result, more explainable representations for different skeleton action sequences can be obtained by MCF. In this study, we follow the common practice that the dense sample strategy of the input skeleton sequences is adopted and this brings much redundancy since number of instances has nothing to do with actions. To reduce the redundancy, a temporal discrimination focus module, termed TDF, is developed to capture the local sensitive points of the temporal dynamics. MCF and TDF are integrated into the standard GCN network to form a unified architecture, named STF-Net. It is noted that STF-Net provides the capability to capture robust movement patterns from these skeleton topology structures, based on multi-grain context aggregation and temporal dependency. Extensive experimental results show that our STF-Net significantly achieves state-of-the-art results on three challenging benchmarks NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-Skeleton.}
}
@article{BI2023109194,
title = {Cross-modal hierarchical interaction network for RGB-D salient object detection},
journal = {Pattern Recognition},
volume = {136},
pages = {109194},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109194},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006732},
author = {Hongbo Bi and Ranwan Wu and Ziqi Liu and Huihui Zhu and Cong Zhang and Tian-Zhu Xiang},
keywords = {Saliency detection, Salient object detection, RGB-D, Feature fusion, Cross-modal interaction},
abstract = {How to effectively exchange and aggregate the information of multiple modalities (e.g. RGB image and depth map) is a big challenge in the RGB-D salient object detection community. To address this problem, in this paper, we propose a cross-modal Hierarchical Interaction Network (HINet), which boosts the salient object detection by excavating the cross-modal feature interaction and progressively multi-level feature fusion. To achieve it, we design two modules: cross-modal information exchange (CIE) module and multi-level information progressively guided fusion (PGF) module. Specifically, the CIE module is proposed to exchange the cross-modal features for learning the shared representations, as well as the beneficial feedback to facilitate the discriminative feature learning of different modalities. Besides, the PGF module is designed to aggregate the hierarchical features progressively with the reverse guidance mechanism, which employs the high-level feature fusion to guide the low-level feature fusion and thus improve the saliency detection performance. Extensive experiments show that our proposed model significantly outperforms the existing nine state-of-the-art models on five challenging benchmark datasets. Codes and results are available at: https://github.com/RanwanWu/HINet.}
}
@article{ZHANG2023109247,
title = {Coarse-to-fine feature representation based on deformable partition attention for melanoma identification},
journal = {Pattern Recognition},
volume = {136},
pages = {109247},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109247},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007269},
author = {Dong Zhang and Jing Yang and Shaoyi Du and Hongcheng Han and Yuyan Ge and Longfei Zhu and Ce Li and Meifeng Xu and Nanning Zheng},
keywords = {Histopathological image, Melanoma identification, Deformable convolution, Attention mechanism, Feature representation, Deep learning},
abstract = {In the histopathological melanoma image diagnosis system, manual identification of super-scale slides with dense cells is tedious, time-consuming, and subjective. To deal with this problem, we propose an automatic identification network based on the deformable partition attention to identify lots of dense slides as an assistant. A coarse-to-fine strategy is adopted in feature representation and qualitative identification to improve the identification accuracy of melanomas and nevi. First of all, because it is difficult to extract features in the lesion area with blurred boundaries and uneven distribution, we develop a deformable partition attention module, which integrates the advantage of the attention mechanism and deformable convolution. The module overcomes the limitation of rectangular convolution and gradually refines the channel and spatial features, which enriches feature representation by combining global and local features. Secondly, to address the problem of difficult convergence and poor recognition rate caused by the excessive non-aligned distance between benign-malignant and benign subcategories, we propose a progressive architecture via a coarse sub-network closely followed by a fine sub-network. Moreover, to further increase the inter-class differences and reduce the intra-class disparities, we propose a joint loss function to mine hard samples, which effectively improves the identification performance. Experimental results on the clinical dataset show that the proposed algorithm has higher sensitivity and specificity and outperforms state-of-the-art deep neural networks.}
}
@article{MADHU2023109153,
title = {ICC++: Explainable feature learning for art history using image compositions},
journal = {Pattern Recognition},
volume = {136},
pages = {109153},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109153},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200632X},
author = {Prathmesh Madhu and Tilman Marquart and Ronak Kosti and Dirk Suckow and Peter Bell and Andreas Maier and Vincent Christlein},
keywords = {Image/Scene compositions, Computer vision, Art history},
abstract = {Image compositions are helpful in the study of image structures and assist in discovering the semantics of the underlying scene portrayed across art forms and styles. With the digitization of artworks in recent years, thousands of images of a particular scene or narrative could potentially be linked together. However, manually linking this data with consistent objectiveness can be a highly challenging and time-consuming task. In this work, we present a novel approach called Image Composition Canvas (ICC++) to compare and retrieve images having similar compositional elements. ICC++ is an improvement over ICC, specializing in generating low and high-level features (compositional elements) motivated by Max Imdahl’s work. To this end, we present a rigorous quantitative and qualitative comparison of our approach with traditional and state-of-the-art (SOTA) methods showing that our proposed method outperforms all of them. In combination with deep features, our method outperforms the best deep learning-based method, opening the research direction for explainable machine learning for digital humanities. We will release the code and the data post-publication.}
}
@article{FU2023109263,
title = {AuxBranch: Binarization residual-aware network design via auxiliary branch search},
journal = {Pattern Recognition},
volume = {136},
pages = {109263},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109263},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007427},
author = {Siming Fu and Huanpeng Chu and Lu Yu and Bo Peng and Zheyang Li and Wenming Tan and Haoji Hu},
keywords = {Binary neural network, Binarization residual, Performance estimation indicator, Neural architecture search},
abstract = {While network binarization is a promising method in memory saving and speedup on hardware, it inevitably leads to binarization residual of intermediate features, resulting in performance capability degradation. To alleviate the above issue, we focus on the network topology design scheme to the more suitable network structure for the extreme-low-bit scenario. In this paper, we propose the baseline-auxiliary expanding network design method to compensate for the binarization residual of features via searching for auxiliary branches, denoted as AuxBranch. The intermediate feature maps are reasonably enhanced by combining baseline and auxiliary features, mimicking the corresponding feature output of the full-precision network. In addition, we devise a hybrid performance estimator (PE) with three elements of preliminary accuracy, feature similarity, and computational complexity. The PE jointly performs an efficient architecture search for binarization baseline and enables automatic computation complexity adjustment under diverse constraints. Extensive experiments show that our approach is superior in terms of accuracy and computational performance, and is plug-and-play for different network backbones and binarization policies. Our code is available at https://github.com/VipaiLab/AuxBranch.}
}
@article{CHEN2023109242,
title = {Position-aware and structure embedding networks for deep graph matching},
journal = {Pattern Recognition},
volume = {136},
pages = {109242},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109242},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200721X},
author = {Dongdong Chen and Yuxing Dai and Lichi Zhang and Zhihong Zhang and Edwin R. Hancock},
keywords = {Graph Matching, Graph Embedding, Deep Neural Network},
abstract = {Graph matching refers to the process of establishing node correspondences based on edge-to-edge constraints between graph nodes. This can be formulated as a combinatorial optimization problem under node permutation and pairwise consistency constraints. The main challenge of graph matching is to effectively find the correct match while reducing the ambiguities produced by similar nodes and edges. In this paper, we present a novel end-to-end neural framework that converts graph matching to a linear assignment problem in a high-dimensional space. This is combined with relative position information at the node level, and high-order structural arrangement information at the subgraph level. By capturing the relative position attributes of nodes between different graphs and the subgraph structural arrangement attributes, we can improve the performance of graph matching tasks, and establish reliable node-to-node correspondences. Our method can be generalized to any graph embedding setting, which can be used as components to deal with various graph matching problems answered with deep learning methods. We validate our method on several real-world tasks, by providing ablation studies to evaluate the generalization capability across different categories. We also compare state-of-the-art alternatives to demonstrate performance.}
}
@article{NIU2023109202,
title = {A multi-layer memory sharing network for video captioning},
journal = {Pattern Recognition},
volume = {136},
pages = {109202},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109202},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006811},
author = {Tian-Zi Niu and Shan-Shan Dong and Zhen-Duo Chen and Xin Luo and Zi Huang and Shanqing Guo and Xin-Shun Xu},
keywords = {Video captioning, Multi-layer network, Memory sharing, Enhanced gated recurrent unit},
abstract = {Over the past several years, video captioning has received much attention in computer vision and machine learning communities. Many models utilize an RNN-based decoder to generate sentences describing the content of a video. They have achieved much progress; however, few methods adopt a decoder with more than three layers because an RNN-based model with more layers may become hard to train, time-consuming or even deteriorate at a certain depth. To address the limitation, we propose a Multi-layer memory sharing Network, MesNet for short, which allows more layers to be stacked without compromising performance. In MesNet, we construct a novel memory sharing structure to strengthen the connections between layers and make the model easier to train. More specifically, we design an Enhanced Gated Recurrent Unit (En-GRU) and stack it to construct a deeper network. Unlike traditional RNN-based multi-layer networks, the memory states of all layers in MesNet are cross-used at each iteration to mimic the brain’s complex connections. Extensive experiments on MSVD and MSR-VTT demonstrate that our method performs well and outperforms some state-of-the-art methods significantly. Our code is available at https://github.com/nbbb/MesNet.}
}
@article{LAN2023109214,
title = {Coherence-aware context aggregator for fast video object segmentation},
journal = {Pattern Recognition},
volume = {136},
pages = {109214},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109214},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006938},
author = {Meng Lan and Jing Zhang and Zengmao Wang},
keywords = {Video object segmentation, Semi-supervised learning, Spatio-temporal representation, Context},
abstract = {Semi-supervised video object segmentation (VOS) is a highly challenging problem that has attracted much research attention in recent years. Temporal context plays an important role in VOS by providing object clues from the past frames. However, most of the prevailing methods directly use the predicted temporal results to guide the segmentation of the current frame, while ignoring the coherence of temporal context, which may be misleading and degrade the performance. In this paper, we propose a novel model named Coherence-aware Context Aggregator (CCA) for VOS, which consists of three modules. First, a coherence-aware module (CAM) is proposed to evaluate the coherence of the predicted result of the current frame and then fuses the coherent features to update the temporal context. CAM can determine whether the prediction is accurate, thus guiding the update of the temporal context and avoiding the introduction of erroneous information. Second, we devise a spatio-temporal context aggregation (STCA) module to aggregate the temporal context with the spatial feature of the current frame to learn a robust and discriminative target representation in the decoder part. Third, we design a refinement module to refine the coarse feature generated from the STCA module for more precise segmentation. Additionally, CCA uses a cropping strategy and takes small-size images as input, thus making it computationally efficient and achieving a real-time running speed. Extensive experiments on four challenging benchmarks show that CCA achieves a better trade-off between efficiency and accuracy compared to state-of-the-art methods. The code will be public.}
}
@article{CHENG2023109182,
title = {Adversarial training with distribution normalization and margin balance},
journal = {Pattern Recognition},
volume = {136},
pages = {109182},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109182},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006616},
author = {Zhen Cheng and Fei Zhu and Xu-Yao Zhang and Cheng-Lin Liu},
keywords = {Adversarial robustness, Adversarial training, Distribution normalization, Margin balance},
abstract = {Adversarial training is the most effective method to improve adversarial robustness. However, it does not explicitly regularize the feature space during training. Adversarial attacks usually move a sample iteratively along the direction which causes the steepest ascent of classification loss by crossing decision boundary. To alleviate this problem, we propose to regularize the distributions of different classes to increase the difficulty of finding an attacking direction. Specifically, we propose two strategies named Distribution Normalization (DN) and Margin Balance (MB) for adversarial training. The purpose of DN is to normalize the features of each class to have identical variance in every direction, in order to eliminate easy-to-attack intra-class directions. The purpose of MB is to balance the margins between different classes, making it harder to find confusing class directions (i.e., those with smaller margins) to attack. When integrated with adversarial training, our method can significantly improve adversarial robustness. Extensive experiments under white-box, black-box, and adaptive attacks demonstrate the effectiveness of our method over other state-of-the-art methods.}
}
@article{TANG2023109206,
title = {Training Compact DNNs with ℓ1/2 Regularization},
journal = {Pattern Recognition},
volume = {136},
pages = {109206},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109206},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006859},
author = {Anda Tang and Lingfeng Niu and Jianyu Miao and Peng Zhang},
keywords = {Deep neural networks, Model compression,  Quasi-norm, Non-Lipschitz regularization, Sparse optimization},
abstract = {Deep neural network(DNN) has achieved unprecedented success in many fields. However, its large model parameters which bring a great burden on storage and calculation hinder the development and application of DNNs. It is worthy of compressing the model to reduce the complexity of the DNN. Sparsity-inducing regularizer is one of the most common tools for compression. In this paper, we propose utilizing the ℓ1/2 quasi-norm to zero out weights of neural networks and compressing the networks automatically during the learning process. To our knowledge, it is the first work applying the non-Lipschitz continuous regularizer for the compression of DNNs. The resulting sparse optimization problem is solved by stochastic proximal gradient algorithm. For further convenience of calculation, an approximation of the threshold-form solution to the proximal operator with ℓ1/2 is given at the same time. Extensive experiments with various datasets and baselines demonstrate the advantages of our new method.}
}
@article{MATSUO2023109201,
title = {Deep attentive time warping},
journal = {Pattern Recognition},
volume = {136},
pages = {109201},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109201},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200680X},
author = {Shinnosuke Matsuo and Xiaomeng Wu and Gantugs Atarsaikhan and Akisato Kimura and Kunio Kashino and Brian Kenji Iwana and Seiichi Uchida},
keywords = {Dynamic time warping, Attention model, Metric learning, Time series classification, Online signature verification},
abstract = {Similarity measures for time series are important problems for time series classification. To handle the nonlinear time distortions, Dynamic Time Warping (DTW) has been widely used. However, DTW is not learnable and suffers from a trade-off between robustness against time distortion and discriminative power. In this paper, we propose a neural network model for task-adaptive time warping. Specifically, we use the attention model, called the bipartite attention model, to develop an explicit time warping mechanism with greater distortion invariance. Unlike other learnable models using DTW for warping, our model predicts all local correspondences between two time series and is trained based on metric learning, which enables it to learn the optimal data-dependent warping for the target task. We also propose to induce pre-training of our model by DTW to improve the discriminative power. Extensive experiments demonstrate the superior effectiveness of our model over DTW and its state-of-the-art performance in online signature verification.}
}
@article{LI2023109234,
title = {Learning spatiotemporal embedding with gated convolutional recurrent networks for translation initiation site prediction},
journal = {Pattern Recognition},
volume = {136},
pages = {109234},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109234},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007130},
author = {Weihua Li and Yanbu Guo and Bingyi Wang and Bei Yang},
keywords = {Translation initiation sites, Gated convolutional networks, Residual networks, Gated recurrent units},
abstract = {Accurately predicting translation initiation sites (TIS) from genomic sequences is crucial for understanding gene regulation and function. TIS prediction methods’ feature vectors are not discriminative enough to lead to unsatisfactory predictive results. In this work, we devise an efficient gated convolutional recurrent network (GCR-Net) with residual learning to dynamically extract dependency patterns of raw genomic sequences in an efficient fusion strategy and successfully improve the performance of the TIS prediction. GCR-Net mainly includes exponential gated convolutional residual networks (EGCRN) and bidirectional gated recurrent unit (Bi-GRU) networks. Particularly, we devise the novel EGCRN to extract multiple complex patterns of the spatial dimension from genomic sequences, where we design an exponential gated linear unit (EGLU) to reduce the vanishing gradient problem. Moreover, we combine EGLU with shortcut connections to develop the stacked gated mechanism based on convolutions that benefit information propagation across layers. Then, we use Bi-GRU with identity connections to learn long-term dependency patterns of the temporal dimension from genomic sequences. Besides, we evaluate our GCR-Net model on four TIS datasets, and experiments demonstrate that GCR-Net is an efficient deep learning-based TIS prediction tool and obtains superior performance compared to the baseline methods.}
}
@article{ZHANG2023109250,
title = {Learning to restore multiple image degradations simultaneously},
journal = {Pattern Recognition},
volume = {136},
pages = {109250},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109250},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007294},
author = {Le Zhang and Kevin Bronik and Bartłomiej W. Papież},
keywords = {Image restoration, Image quality, Multiple degradations, MRI},
abstract = {Image corruptions are common in the real world, for example images in the wild may come with unknown blur, bias field, noise, or other kinds of non-linear distributional shifts, thus hampering encoding methods and rendering downstream task unreliable. Image upgradation requires a complicated balance between high-level contextualised information and spatial specific details. Existing approaches to solving the problems are designed to focus on single corruption, which unavoidably results in poor performance when the acquisitions suffer from multiple degradations. In this study, we investigate the possibility of handling multiple degradations and enhancing the quality of images via deblurring, bias field correction, and denoising. To tackle the problems with propagating errors caused by independent learning, we propose a unified and scalable framework, which consists of three special decoders. Two decoders learn artifact attention from provided images thereby generating realistic individual artifact and multiple artifacts on single image; the third decoder is trained towards removing artifact on the synthetic image with multiple corruptions thereby generating high quality image. We additionally provide improvements over previous image degradation synthesis approaches by modelling multiple image degradations directly from data observations. We first create a toy MNIST dataset and investigate the properties of the proposed algorithm. We then use brain MRI datasets to demonstrate our method’s robustness, including both simulated (where necessary) and real-world artifacts. In addition, our method can be used for single/or multiple degradation(s) synthesis by implementing the learned degradation operators in a new domain from a given dataset. The code will be released upon acceptance of the paper.}
}
@article{DING2023109238,
title = {A Sampling-Based Density Peaks Clustering Algorithm for Large-Scale Data},
journal = {Pattern Recognition},
volume = {136},
pages = {109238},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109238},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007178},
author = {Shifei Ding and Chao Li and Xiao Xu and Ling Ding and Jian Zhang and Lili Guo and Tianhao Shi},
keywords = {Density peaks clustering, Sampling method, TI search strategy, Large-scale data},
abstract = {With the rapid development of information technology, massive amount of data is generated. How to discover useful information to support decision-making has become one of the focuses of scholar's research. Clustering is thought to be one of the main means to deal with large-scale data. Density peaks clustering (DPC) is an effective density-based clustering algorithm which is widely applied in numerous fields because of its satisfactory performance. However, the computational complexity of DPC is O(N2) which is not friendly to large-scale data. To solve this issue, a sampling-based density peaks clustering algorithm for large-scale data (SDPC) is proposed. Firstly, a sampling method is used to reduce the distance calculations. Secondly, approximate representatives are identified by an improved TI search strategy which further accelerates the clustering process. Afterwards, the approximate representatives are clustered by DPC. Finally, the remaining points are allocated to the same cluster as its nearest representatives. Experimental results on both synthetic datasets and real-world datasets illustrate that SDPC is more efficient than DPC, while its clustering performance maintains the same level as DPC.}
}
@article{WANG2023109266,
title = {AdaNS: Adaptive negative sampling for unsupervised graph representation learning},
journal = {Pattern Recognition},
volume = {136},
pages = {109266},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109266},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007452},
author = {Yu Wang and Liang Hu and Wanfu Gao and Xiaofeng Cao and Yi Chang},
keywords = {Graph representation learning, Negative sampling, Noise contrastive estimation},
abstract = {Recently, unsupervised graph representation learning has attracted considerable attention through effectively encoding graph-structured data without semantic annotations. To accelerate its training, noise contrastive estimation (NCE) samples uniformly negative examples to fit an unnormalized graph model. However, this uniform sampling strategy may easily lead to slow convergence, even the vanishing gradient problem. In this paper, we theoretically show that sampling those hard negatives close to the current anchor can relieve the above difficulties. With this finding, we then propose an Adaptive Negative Sampling strategy, namely AdaNS, which efficiently samples the hard negatives from the mixing distribution regarding the dimensional elements of the current node representation. Experiments show that our AdaNS sampling strategy applied on top of representative unsupervised models, e.g., DeepWalk, GraphSAGE, can outperform the existing negative sampling strategies in the tasks of node classification and visualization. This also further demonstrates that sampling those hard negatives can bring performance improvements for learning the node representations.}
}
@article{XIE2023109230,
title = {Scalable clustering by aggregating representatives in hierarchical groups},
journal = {Pattern Recognition},
volume = {136},
pages = {109230},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109230},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007099},
author = {Wen-Bo Xie and Zhen Liu and Debarati Das and Bin Chen and Jaideep Srivastava},
keywords = {Hierarchical clustering, Election tree, Representative node, Root},
abstract = {Appropriately handling the scalability of clustering is a long-standing challenge for the study of clustering techniques and is of fundamental interest to researchers in the community of data mining and knowledge discovery. In comparison to other clustering methods, hierarchical clustering demonstrates better interpretability of clustering results but poor scalability while handling large-scale data. Thus, more comprehensive studies on this problem need to be conducted. This paper develops a new scalable hierarchical clustering model called Election Tree, which can detect the most representative point for each sub-cluster via the process of node election in split data and adjust the members in sub-clusters by the operations of node merging and swap. Extensive experiments on real-world datasets reveal that the proposed computational framework has better clustering accuracy as opposed to the competing baseline methods. Meanwhile, with respect to the scalability tests on incremental synthetic datasets, the results show that the new model has a significantly lower time consumption than the state-of-the-art hierarchical clustering models such as PERCH, GRINCH, SCC and other classic baselines.}
}
@article{WANG2023109193,
title = {FP-DARTS: Fast parallel differentiable neural architecture search for image classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109193},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109193},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006720},
author = {Wenna Wang and Xiuwei Zhang and Hengfei Cui and Hanlin Yin and Yannnig Zhang},
keywords = {Neural architecture search, Computing overheads, Operator sub-sets, Two-parallel-path, Binary gate, Sigmoid function},
abstract = {Neural Architecture Search (NAS) has made remarkable progress in automatic machine learning. However, it still suffers massive computing overheads limiting its wide applications. In this paper, we present an efficient search method, Fast Parallel Differential Neural Architecture Search (FP-DARTS). The proposed method is carefully designed from three levels to construct and train the super-network. Firstly, at the operation-level, to reduce the computational burden, different from the standard DARTS search space (8 operations), we decompose the operation set into two non-overlapping operator sub-sets (4 operations for each). Adopting these two reduced search spaces, two over-parameterized sub-networks are constructed. Secondly, at the channel-level, the partially-connected strategy is adopted, where each sub-network only adopts partial channels. Then these two sub-networks construct a two-parallel-path super-network by addition. Thirdly, at the training-level, the binary gate is introduced to control whether a path participates in the super-network training. It may suffer an unfair issue when using softmax to select the best input for intermediate nodes across two operator sub-sets. To tackle this problem, the sigmoid function is introduced, which measures the performance of operations without compression. Extensive experiments demonstrate the effectiveness of the proposed algorithm. Specifically, FP-DARTS achieves 2.50% test error with only 0.08 GPU-days on CIFAR10, and a state-of-the-art top-1 error rate of 23.7% on ImageNet using only 2.44 GPU-days for search.}
}
@article{WANG2023109254,
title = {Neurodynamics-driven supervised feature selection},
journal = {Pattern Recognition},
volume = {136},
pages = {109254},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109254},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007336},
author = {Yadi Wang and Jun Wang and Dacheng Tao},
keywords = {Feature selection, Biconvex Optimization, Information-theoretic measures, Neurodynamic optimization},
abstract = {Feature selection is an important dimensionality reduction technique in machine learning, pattern recognition, image processing, and data mining. Most existing feature selection methods are greedy in nature thus are prone to sub-optimality. Though some feature selection methods based on global optimization of unsupervised redundancy may potentiate performance improvements, they may or may not be relevant to classification as the information on pairwise features with class labels is missing. In this paper, based on a supervised similarity measure, a biconvex optimization problem is formulated for holistic feature section with a quadratically weighted objective function subject to linear equality and nonnegativity constraints. In addition, an iteratively reweighted convex quadratic program is reformulated. A two-timescale duplex neurodynamic system is applied to solve the formulated biconvex optimization problem and a projection neural network is customized to solve the iteratively reweighted convex optimization problem. Experimental results of the proposed neurodynamics-based supervised feature selection are elaborated in comparison with several existing feature selection methods based on twenty benchmark datasets to substantiate the efficacy and superiority of the neurodynamics-based method for selecting informative features in classification.}
}
@article{MAHDIKHANLOU2023109217,
title = {3D hand pose estimation from a single RGB image by weighting the occlusion and classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109217},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109217},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006963},
author = {Khadijeh Mahdikhanlou and Hossein Ebrahimnezhad},
keywords = {3D hand pose estimation, Semantic segmentation, Occlusion weight, Hand pose classification},
abstract = {In this paper, a new framework for 3D hand pose estimation using a single RGB image is proposed. The framework is composed of two blocks. The first block formulates the hand pose estimation as a classification problem. Since the human hand can perform numerous poses, the classification network needs a huge number of parameters. So, we propose to classify hand poses based on three different aspects, including hand gesture, hand direction, and palm direction. In this way, the number of parameters will be significantly reduced. The motivation behind the classification block is that the model deals with the image as a whole and extracts global features. Furthermore, the output of the classification model is a valid pose that does not include any unexpected angle at joints. The second block estimates the 3D coordinates of the hand joints and focuses more on the details of the image pattern. RGB-based 3D hand pose estimation is an inherently ill-posed problem due to the lack of depth information in the 2D image. We propose to use the occlusion status of the hand joints to solve this problem. The occlusion status of the joints has been labeled manually. Some joints are partially occluded, and we propose to compute the extent of the occlusion by semantic segmentation. The existing methods in this field mostly used synthetic datasets. But all the models proposed in this paper are trained on more than 50 K real images. Extensive experiments on our new dataset and two other benchmark datasets show that the proposed method can achieve good performance. We also analyze the validity of the predicted poses, and the results show that the classification block increases the validity of the poses.}
}
@article{ZHANG2023109165,
title = {Weakly Supervised Instance Segmentation via Category-aware Centerness Learning with Localization Supervision},
journal = {Pattern Recognition},
volume = {136},
pages = {109165},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109165},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006446},
author = {Jiabin Zhang and Hu Su and Yonghao He and Wei Zou},
keywords = {Weakly supervised learning, Instance segmentation, Centerness, Coarse localization annotation},
abstract = {Deep convolutional neural networks (DCNN) trained with pixel-level segmentation masks achieve high performance in the task of instance segmentation. The difficulty of acquiring such annotation limits the application and popularization of the DCNN-based approaches. To address the issue, a weakly supervised approach is proposed in the paper which performs instance segmentation only with the supervision of bounding box or coarse localization annotation. A novel DCNN model is constructed which consists of two branches: the centerness branch and the segmentation branch. The former branch is to learn the semantically spatial importance over the areas of object instances under the localization supervision. Object proposals with exact boundaries are automatically generated and are then ranked under the guidance of the output of the centerness branch. The most matched instance proposal is assigned to each object, which is then used to supervise the segmentation branch. The losses are calculated by both the outputs of the two branches and the entire DCNN model is trained end-to-end. Experiments are extensively conducted to verify the effectiveness. With the supervision of precise bounding box annotation, our approach achieves state-of-the-art (SOTA) accuracy in the comparison with recent related works. And in the case of coarse localization annotation, our approach only deduces a slight reduction in accuracy, which significantly outperforms other approaches. The excellent performance demonstrates that our approach would be helpful to further alleviate the workload of image annotation while maintaining competitive accuracy.}
}
@article{DELMORAL2023109225,
title = {Pitfalls of assessing extracted hierarchies for multi-class classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109225},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109225},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200704X},
author = {Pablo {del Moral} and Sławomir Nowaczyk and Anita Sant’Anna and Sepideh Pashami},
keywords = {Hierarchical multi-class classification, Multi-class classification, Class hierarchies},
abstract = {Using hierarchies of classes is one of the standard methods to solve multi-class classification problems. In the literature, selecting the right hierarchy is considered to play a key role in improving classification performance. Although different methods have been proposed, there is still a lack of understanding of what makes a hierarchy good and what makes a method to extract hierarchies perform better or worse. To this effect, we analyze and compare some of the most popular approaches to extracting hierarchies. We identify some common pitfalls that may lead practitioners to make misleading conclusions about their methods. To address some of these problems, we demonstrate that using random hierarchies is an appropriate benchmark to assess how the hierarchy’s quality affects the classification performance. In particular, we show how the hierarchy’s quality can become irrelevant depending on the experimental setup: when using powerful enough classifiers, the final performance is not affected by the quality of the hierarchy. We also show how comparing the effect of the hierarchies against non-hierarchical approaches might incorrectly indicate their superiority. Our results confirm that datasets with a high number of classes generally present complex structures in how these classes relate to each other. In these datasets, the right hierarchy can dramatically improve classification performance.}
}
@article{BECKHAM2023109209,
title = {Visual question answering from another perspective: CLEVR mental rotation tests},
journal = {Pattern Recognition},
volume = {136},
pages = {109209},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109209},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006884},
author = {Christopher Beckham and Martin Weiss and Florian Golemo and Sina Honari and Derek Nowrouzezahrai and Christopher Pal},
keywords = {Deep learning, Computer vision, Visual question answering, Contrastive learning, Clevr},
abstract = {Different types of mental rotation tests have been used extensively in psychology to understand human visual reasoning and perception. Understanding what an object or visual scene would look like from another viewpoint is a challenging problem that is made even harder if it must be performed from a single image. We explore a controlled setting whereby questions are posed about the properties of a scene if that scene was observed from another viewpoint. To do this we have created a new version of the CLEVR dataset that we call CLEVR Mental Rotation Tests (CLEVR-MRT). Using CLEVR-MRT we examine standard methods, show how they fall short, then explore novel neural architectures that involve inferring volumetric representations of a scene. These volumes can be manipulated via camera-conditioned transformations to answer the question. We examine the efficacy of different model variants through rigorous ablations and demonstrate the efficacy of volumetric representations.}
}
@article{SHEHATA2023109197,
title = {Annotator-dependent uncertainty-aware estimation of gait relative attributes},
journal = {Pattern Recognition},
volume = {136},
pages = {109197},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109197},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006768},
author = {Allam Shehata and Yasushi Makihara and Daigo Muramatsu and Md Atiqur Rahman Ahad and Yasushi Yagi},
keywords = {Gait relative attribute, Relative label distribution, Relative score distribution, Annotator’s uncertainty, Transition matrix},
abstract = {In this paper, we describe an uncertainty-aware estimation framework for gait relative attributes. We specifically design a two-stream network model that takes a pair of gait videos as input. It then outputs a corresponding pair of Gaussian distributions of gait absolute attribute scores and annotator-dependent gait relative attribute label distributions. Moreover, we propose a differentiable annotator-independent uncertainty layer to estimate the gait relative attribute score distribution from the absolute distributions then map it to a relative attribute label distribution using the computation of cumulative distribution functions. Furthermore, we propose another annotator-dependent uncertainty layer to estimate the uncertainty on the gait relative attribute labels in terms of a set of trainable transition matrices. Finally, we design a joint loss function on the relative attribute label distribution to learn the model parameters. Experiments on two gait relative attribute datasets demonstrated the effectiveness of the proposed method against baselines in quantitative and qualitative evaluations.}
}
@article{CAO2023109262,
title = {Learning generalized visual odometry using position-aware optical flow and geometric bundle adjustment},
journal = {Pattern Recognition},
volume = {136},
pages = {109262},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109262},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007415},
author = {Yi-Jun Cao and Xian-Shi Zhang and Fu-Ya Luo and Peng Peng and Chuan Lin and Kai-Fu Yang and Yong-Jie Li},
keywords = {Visual odometry, Self-supervise learning, Optical flow, Monocular depth estimation, Joint learning, Generalization capability},
abstract = {Recent visual odometry (VO) methods incorporating geometric algorithm into deep-learning architecture have shown outstanding performance on the challenging monocular VO task. Despite encouraging results are shown, previous methods ignore the requirement of generalization capability under noisy environment and various scenes. To address this challenging issue, this work first proposes a novel optical flow network (PANet). Compared with previous methods that predict optical flow as a direct regression task, our PANet computes optical flow by predicting it into the discrete position space with optical flow probability volume, and then converting it to optical flow. Next, we improve the bundle adjustment module to fit the self-supervised training pipeline by introducing multiple sampling, ego-motion initialization, dynamic damping factor adjustment, and Jacobi matrix weighting. In addition, a novel normalized photometric loss function is advanced to improve the depth estimation accuracy. The experiments show that the proposed system not only achieves comparable performance with other state-of-the-art self-supervised learning-based methods on the KITTI dataset, but also significantly improves the generalization capability compared with geometry-based, learning-based and hybrid VO systems on the noisy KITTI and the challenging outdoor (KAIST) scenes.}
}
@article{CAO2023109213,
title = {Joint classification and prediction of random curves using heavy‐tailed process functional regression},
journal = {Pattern Recognition},
volume = {136},
pages = {109213},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109213},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006926},
author = {Chunzheng Cao and Xin Liu and Shuren Cao and Jian Qing Shi},
keywords = {Functional data analysis, Outliers, Heavy-tailed process, Bayesian estimation, MCMC},
abstract = {We propose a heavy-tailed process functional regression to jointly perform classification and prediction of time-varying functional data. We use two independent scale mixtures of Gaussian Processes to respectively model random effects and random errors, yielding robust inferences against both magnitude and shape outliers. We classify random curves by posterior predictive probabilities of class labels and offer a weighted prediction of future curve trends. A Bayesian estimation procedure is implemented through an MCMC sampling algorithm. The performance of classification and prediction of the proposed model is evaluated using simulated studies and some real data sets.}
}
@article{XUE2023109205,
title = {Local Linear Embedding with Adaptive Neighbors},
journal = {Pattern Recognition},
volume = {136},
pages = {109205},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109205},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006847},
author = {Jiaqi Xue and Bin Zhang and Qianyao Qiang},
keywords = {dimensionality reduction, Locally Linear Embedding, manifold learning, adaptive neighbor strategy},
abstract = {Dimensionality reduction is one of the most important techniques in the field of data mining. It embeds high-dimensional data into a low-dimensional vector space while keeping the main information as much as possible. Locally Linear Embedding (LLE) as a typical manifold learning algorithm computes neighborhood preserving embeddings of high-dimensional inputs. Based on the thought of LLE, we propose a novel unsupervised dimensionality reduction model called Local Linear Embedding with Adaptive Neighbors (LLEAN). To achieve a desirable dimensionality reduction result, we impose adaptive neighbor strategy and adopt a projection matrix to project data into an optimal subspace. The relationship between every pair-wise data is investigated to help reveal the data structure. Augmented Lagrangian Multiplier (ALM) is devised in optimization procedure to effectively solve the proposed objective function. Comprehensive experiments on toy data and benchmark datasets have been done and the results show that LLEAN outperforms other state-of-the-art dimensionality reduction methods.}
}
@article{LI2023109229,
title = {Understanding and combating robust overfitting via input loss landscape analysis and regularization},
journal = {Pattern Recognition},
volume = {136},
pages = {109229},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109229},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007087},
author = {Lin Li and Michael Spratling},
keywords = {Adversarial robustness, Adversarial training, Robust overfitting, Loss landscape analysis, Logit regularization},
abstract = {Adversarial training is widely used to improve the robustness of deep neural networks to adversarial attack. However, adversarial training is prone to overfitting, and the cause is far from clear. This work sheds light on the mechanisms underlying overfitting through analyzing the loss landscape w.r.t. the input. We find that robust overfitting results from standard training, specifically the minimization of the clean loss, and can be mitigated by regularization of the loss gradients. Moreover, we find that robust overfitting turns severer during adversarial training partially because the gradient regularization effect of adversarial training becomes weaker due to the increase in the loss landscape’s curvature. To improve robust generalization, we propose a new regularizer to smooth the loss landscape by penalizing the weighted logits variation along the adversarial direction. Our method significantly mitigates robust overfitting and achieves the highest robustness and efficiency compared to similar previous methods. Code is available at https://github.com/TreeLLi/Combating-RO-AdvLC.}
}
@article{SHU2023109257,
title = {ALVLS: Adaptive local variances-Based levelset framework for medical images segmentation},
journal = {Pattern Recognition},
volume = {136},
pages = {109257},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109257},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007361},
author = {Xiu Shu and Yunyun Yang and Jun Liu and Xiaojun Chang and Boying Wu},
keywords = {Image segmentation, Local fitting variance, Edge-based information, Level set framework},
abstract = {Medical image segmentation is a very challenging task, not only because the intensity of the medical image itself is not uniform, but also it may be accompanied by the impact of noise. Although mathematics, computer science, medicine, and other interdisciplinary fields have begun to study the problem of medical image segmentation, and have put forward a variety of segmentation algorithms, there is still much room for further improvement and enhancement. In the process of medical image collection and reconstruction, it is easy to produce intensity inhomogeneity and noises, as well as interference from other tissues, resulting in the difficulty of accurate segmentation. In this paper, we propose the adaptive local variances-based level set (ALVLS) model to segment medical images with intensity inhomogeneity and noises, including cardiac MR images, brain MR images, and breast ultrasound images. According to the variance difference information, the ALVLS model can adjust the effect of the area term adaptively. The local intensity variances are designed to optimize the ability to resist noise, which improves the segmentation accuracy of medical images. We also propose the two-layer level set model for segmenting left ventricles and left epicardium simultaneously. Experimental results for medical images and synthetic images show the desirable performance of the ALVLS model in accuracy, efficiency, and robustness to noise. In medical image competition, the Dice coefficient is used to calculate the similarity between the segmentation result and the ground truth. Thus we do comparisons with other methods and show that the Dice coefficient of the proposed method is higher than other testing methods.}
}
@article{COTOGNI2023109249,
title = {TreEnhance: A tree search method for low-light image enhancement},
journal = {Pattern Recognition},
volume = {136},
pages = {109249},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109249},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007282},
author = {Marco Cotogni and Claudio Cusano},
keywords = {Low-light image enhancement, Deep reinforcement learning, Automatic image retouching, Image processing, Tree search},
abstract = {In this paper we present TreEnhance, an automatic method for low-light image enhancement capable of improving the quality of digital images. The method combines tree search theory, and in particular the Monte Carlo Tree Search (MCTS) algorithm, with deep reinforcement learning. Given as input a low-light image, TreEnhance produces as output its enhanced version together with the sequence of image editing operations used to obtain it. During the training phase, the method repeatedly alternates two main phases: a generation phase, where a modified version of MCTS explores the space of image editing operations and selects the most promising sequence, and an optimization phase, where the parameters of a neural network, implementing the enhancement policy, are updated. Two different inference solutions are proposed for the enhancement of new images: one is based on MCTS and is more accurate but more time and memory consuming; the other directly applies the learned policy and is faster but slightly less precise. As a further contribution, we propose a guided search strategy that “reverses” the enhancement procedure that a photo editor applied to a given input image. Unlike other methods from the state of the art, TreEnhance does not pose any constraint on the image resolution and can be used in a variety of scenarios with minimal tuning. We tested the method on two datasets: the Low-Light dataset and the Adobe Five-K dataset obtaining good results from both a qualitative and a quantitative point of view.}
}
@article{YANG2023109245,
title = {HAMIL: Hierarchical aggregation-based multi-instance learning for microscopy image classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109245},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109245},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007245},
author = {Yang Yang and Yanlun Tu and Houchao Lei and Wei Long},
keywords = {Multi-instance learning, Biomedical image, Hierarchical aggregation},
abstract = {Multi-instance learning is common for computer vision tasks, especially in biomedical image processing. Traditional methods for multi-instance learning focus on designing feature aggregation methods and multi-instance classifiers, where the aggregation operation is performed either in the feature extraction or learning phase. As deep neural networks (DNNs) achieve great success in image processing via automatic feature learning, certain feature aggregation mechanisms need to be incorporated into common DNN architecture for multi-instance learning. Moreover, flexibility and reliability are crucial considerations to deal with varying quality and number of instances. In this study, we propose a hierarchical aggregation network for multi-instance learning, called HAMIL. The hierarchical aggregation protocol enables feature fusion in a defined order, and the simple convolutional aggregation units lead to an efficient and flexible architecture. We assess the model performance on two microscopy image classification tasks, namely protein subcellular localization using immunofluorescence images and gene annotation using spatial gene expression images. The experimental results show that HAMIL outperforms the state-of-the-art feature aggregation methods and the existing models for addressing these two tasks. The visualization analyses also demonstrate the ability of HAMIL to focus on high-quality instances.}
}
@article{LI2023109200,
title = {Progressive generation of 3D point clouds with hierarchical consistency},
journal = {Pattern Recognition},
volume = {136},
pages = {109200},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109200},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006793},
author = {Peipei Li and Xiyan Liu and Jizhou Huang and Deguo Xia and Jianzhong Yang and Zhen Lu},
keywords = {3D Point cloud generation, Point cloud analysis, Generative adversarial networks, Variational autoencoder, Hierarchical consistency},
abstract = {Generating 3D point cloud directly from latent prior (e.g., Gaussian distribution) plays a vital role in the representation learning and data augmentation in 3D vision tasks. Since point cloud is formed by irregular points, the generation process of point cloud requires rich semantic information, yet few studies are devoted to it. In this paper, we recast this generation task as a progressive learning problem to model the two-level hierarchy of distributions and address it by proposing a novel model called hierarchical consistency variational autoencoder (HC-VAE). This framework introduces a hierarchical consistent mechanism (HCM) to model the shape consistency and the pointwise representation consistency in a complementary manner. Specifically, we propose a stackable encoder-decoder framework and constrain the generation quality progressively to ensure that the underlying shape and fine-grained parts can be reconstructed with high fidelity. Additionally, given the progressively generated intermediate point cloud instances, a hierarchical-positive contrastive loss is introduced to learn the point-distribution-free instance representations to avoid explicitly parametrizing the distribution of points in a shape. In this way, our model suffices to generate diverse, high-resolution, and uniform point cloud instances. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art performance in point cloud generation.}
}
@article{LIU2023109261,
title = {A pyramid input augmented multi-scale CNN for GGO detection in 3D lung CT images},
journal = {Pattern Recognition},
volume = {136},
pages = {109261},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109261},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007403},
author = {Weihua Liu and Xiabi Liu and Xiongbiao Luo and Murong Wang and Guanghui Han and Xinming Zhao and Zheng Zhu},
keywords = {GGO detection, Multi-scale processing, 3D CT scans, Pyramid inputs},
abstract = {This paper proposes a new convolutional neural network (CNN) with multi-scale processing for detecting ground-glass opacity nodules (GGO) in 3D computed tomography (CT) images, which is referred to as PiaNet for short. PiaNet consists of a feature-extraction module and a prediction module. The former module is constructed by introducing pyramid multi-scale source connections into a contracting-expanding structure. Besides, a new multi-receptive-field convolution block (MRCB) is presented to fuse the convolutions with multiple kernels of varying sizes for capturing features in each scale of information better. The latter module includes a bounding-box regressor and a classifier that are employed to simultaneously recognize GGO nodules and estimate bounding boxes at multiple scales. To train the proposed PiaNet, a two-stage transfer learning strategy is developed. In the first stage, the feature-extraction module is embedded into a classifier network that is trained on a large data set of GGO and non-GGO patches, which are generated by performing data augmentation from a small number of annotated CT scans. In the second stage, the pretrained feature-extraction module is loaded into PiaNet, and then PiaNet is fine-tuned using the annotated CT scans. We evaluate the proposed PiaNet with the LIDC-IDRI dataset. The experimental results demonstrate that our method outperforms state-of-the-art counterparts, including the Subsolid CAD and Aidence systems and CPM-Net and S4ND and GA-SSD methods. PiaNet achieves a sensitivity of 93.6% with only one false positive per scan.}
}
@article{NING2023109216,
title = {Hyper-sausage coverage function neuron model and learning algorithm for image classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109216},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109216},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006951},
author = {Xin Ning and Weijuan Tian and Feng He and Xiao Bai and Le Sun and Weijun Li},
keywords = {Pattern recognition, Deep neural networks, Neuron model, Brain-inspired, Computer vision},
abstract = {Recently, deep neural networks (DNNs) promote mainly by network architectures and loss functions; however, the development of neuron models has been quite limited. In this study, inspired by the mechanism of human cognition, a hyper-sausage coverage function (HSCF) neuron model possessing a high flexible plasticity. Then, a novel cross-entropy and volume-coverage (CE_VC) loss is defined, which compresses the volume of the hyper-sausage to the hilt, and helps alleviate confusion among different classes, thus ensuring the intra-class compactness of the samples. Finally, a divisive iteration method is introduced, which considers each neuron model as a weak classifier, and iteratively increases the number of weak classifiers. Thus, the optimal number of the HSCF neuron is adaptively determined and an end-to-end learning framework is constructed. In particular, to improve the classification performance, the HSCF neuron can be applied to classical DNNs. Comprehensive experiments on eight datasets in several domains demonstrate the effectiveness of the proposed method. The proposed method exhibits the feasibility of boosting DNNs with neuron plasticity and provides a novel perspective for further developments in DNNs. The source code is available at https://github.com/Tough2011/HSCFNet.git .}
}
@article{ZHAI2023109167,
title = {Joint optimization of scoring and thresholding models for online multi-label classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109167},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109167},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200646X},
author = {Tingting Zhai and Hao Wang and Hongcheng Tang},
keywords = {online multi-label classification, online thresholding, adaptive thresholding, online learning},
abstract = {Existing online multi-label classification works cannot well handle the online label thresholding problem and lack regret analysis for their online algorithms. This paper proposes a novel framework of joint optimization of scoring and thresholding models for online multi-label classification, with the aim to overcome the above drawbacks. The key feature of our framework is that both scoring and thresholding models are included as important components of the online multi-label classifier and are incorporated into one online optimization problem. Based on this framework, we present two adaptive label thresholding algorithms and two fixed thresholding algorithms. For each type of algorithms, a first-order method and a second-order one are provided for updating the online multi-label classifier. Both methods enjoy a closed-form update. Our proposed algorithms are proved to achieve a sub-linear regret. Using Mercer kernels, two first-order algorithms can be extended to handle nonlinear multi-label prediction tasks. Experiments show the advantage of the adaptive and the fixed thresholding algorithms, in terms of various multi-label performance metrics.}
}
@article{SHAO2023109253,
title = {Twin SVM for conditional probability estimation in binary and multiclass classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109253},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109253},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007324},
author = {Yuan-Hai Shao and Xiao-Jing Lv and Ling-Wei Huang and Lan Bai},
keywords = {Support vector machine, Twin support vector machines, Conditional probability, Binary classification, Multiclass classification},
abstract = {In this paper, we estimate the conditional probability function by presenting a new twin SVM model (CPTWSVM) in binary and multiclass classification problems. The motivation of CPTWSVM is to implement the empirical risk minimization on training data, which is hard to realize in traditional twin SVMs. In each subproblem of CPTWSVM, it measures the empirical risk and outputs the corresponding probability estimate of each class, which eliminates the problems of inconsistent measurement in twin SVMs. Though an additional discriminant objective function is introduced, the optimization problem size of each subproblem is smaller than conditional probability SVM, and is solved by block decomposition algorithm efficiently. In addition, we extend CPTWSVM to multiclass classification by estimating the conditional probability of each class, and maintaining the above properties. Numerical experiments on benchmark and real application datasets demonstrate that CPTWSVM outputs the estimate of probability and the data projection well, resulting in better generalization ability than some leading TWSVMs communities, in terms of binary and multiclass classification.}
}
@article{VALVERDE2023109208,
title = {Region-wise loss for biomedical image segmentation},
journal = {Pattern Recognition},
volume = {136},
pages = {109208},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109208},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006872},
author = {Juan Miguel Valverde and Jussi Tohka},
keywords = {Deep learning, Segmentation, Medical imaging, Loss function},
abstract = {We propose Region-wise (RW) loss for biomedical image segmentation. Region-wise loss is versatile, can simultaneously account for class imbalance and pixel importance, and it can be easily implemented as the pixel-wise multiplication between the softmax output and a RW map. We show that, under the proposed RW loss framework, certain loss functions, such as Active Contour and Boundary loss, can be reformulated similarly with appropriate RW maps, thus revealing their underlying similarities and a new perspective to understand these loss functions. We investigate the observed optimization instability caused by certain RW maps, such as Boundary loss distance maps, and we introduce a mathematically-grounded principle to avoid such instability. This principle provides excellent adaptability to any dataset and practically ensures convergence without extra regularization terms or optimization tricks. Following this principle, we propose a simple version of boundary distance maps called rectified Region-wise (RRW) maps that, as we demonstrate in our experiments, achieve state-of-the-art performance with similar or better Dice coefficients and Hausdorff distances than Dice, Focal, weighted Cross entropy, and Boundary losses in three distinct segmentation tasks. We quantify the optimization instability provided by Boundary loss distance maps, and we empirically show that our RRW maps are stable to optimize. The code to run all our experiments is publicly available at: https://github.com/jmlipman/RegionWiseLoss.}
}
@article{QIAN2023109156,
title = {Weight matrix sharing for multi-label learning},
journal = {Pattern Recognition},
volume = {136},
pages = {109156},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109156},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006355},
author = {Kun Qian and Xue-Yang Min and Yusheng Cheng and Fan Min},
keywords = {Low-rank, Missing labels, Multi-label learning, Shared weight, Sparse},
abstract = {Multi-label learning on real-world data is a challenging task due to sparse labels, missing labels, and sparse structures. Some existing approaches are effective in addressing the former two issues. In this paper, we propose a shared weight matrix with low-rank and sparse regularization for multi-label learning (2SML) algorithm to address the issues simultaneously. First, two explicit correlation matrices are constructed from the feature matrix and label matrix. Second, we select informative labels by instance representativeness to learn implicit correlations. Third, a feature manifold and label manifold are employed to guide the shared weight learning process. Extensive experiments are undertaken on multiple benchmark datasets with and without missing labels. The results show that the proposed method outperforms the state-of-the-art methods.}
}
@article{2023109325,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {136},
pages = {109325},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(23)00026-2},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000262}
}
@article{DU2023109241,
title = {A new image decomposition approach using pixel-wise analysis sparsity model},
journal = {Pattern Recognition},
volume = {136},
pages = {109241},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109241},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007208},
author = {Shuangli Du and Yiguang Liu and Minghua Zhao and Zhenyu Xu and Jie Li and Zhenzhen You},
keywords = {Image decomposition, Rain streaks removal, Retinex theory, Pixel-wise analysis sparsity model, Synthesis sparsity model},
abstract = {Decomposing an image into two ‘simpler’ layers has been widely used in low-level vision tasks, such as image recovery and enhancement. It is an ill-posed problem since the number of unknowns are larger than the input. In this paper, a two-step strategy is introduced, including task-aware priors estimate and a decomposition model. A pixel-wise analysis sparsity model is proposed to regularize the separation layers, which supposes the transformed image generated with analysis operator is sparse. Unlike regularizing all pixels with one penalty weight, we try to estimate each pixel’s sparsity level with task-aware priors and to achieve pixel-wise sparse penalty. Additionally, one separation layer is regularized with both synthesis sparsity model and pixel-wise analysis sparsity model to exploit their complementary mechanisms. Unlike the analysis one utilizing image local features, the synthesis one exploits an over-complete dictionary and non-local similarity cues to provide flexible prior for regularizing the decomposition results. The proposed model is solved by an alternating optimization algorithm. We evaluate it with two applications, Retinex model and rain streaks removal. Extensive experiments on multiple enhancement datasets, many synthetic and real rainy images demonstrate that our method can remove imaging noise during Retinex decomposition, and can produce high fidelity deraining results. It achieves competing performance in terms of quantitative metrics and visual quality compared with the state-of-the-art methods.}
}
@article{TU2023109204,
title = {Relation-aware attention for video captioning via graph learning},
journal = {Pattern Recognition},
volume = {136},
pages = {109204},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109204},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006835},
author = {Yunbin Tu and Chang Zhou and Junjun Guo and Huafeng Li and Shengxiang Gao and Zhengtao Yu},
keywords = {Video captioning, Relation-aware attention, Graph learning},
abstract = {Video captioning often uses an attentive encoder-decoder as the baseline model. However, the conventional attention mechanism still remains two problems. First, the attended visual feature is often irrelevant to the target word state, because the attention process only uses the unidirectional flow from vision to linguistics, while lacking the reverse flow. Second, each attention result is independent, because it is computed only based on the previous word states while not considering the attention information from the past and future. This does not suit the attention habits of human beings. In this paper, we improve the conventional attention mechanism to a relation-aware attention mechanism. To this end, we propose two kinds of graph learning strategies, namely the linguistics-to-vision heterogeneous graph (HTG) and the vision-to-vision homogeneous graph (HMG). The HTG aims to enhance the inter-relation of attention by reversely modeling the relation of each word with respect to every attended visual feature, supporting proper semantic alignment in between. The HMG aims to enhance the intra-relation of attention by capturing the relations among all of the attended visual features, which can leverage the attention information from the past and future to guide the current attention process. Extensive experiments on two public datasets show that our proposed method not only significantly improves the baseline model, but also outperforms state-of-the-art methods.}
}
@article{GAUTAM2023109172,
title = {This looks More Like that: Enhancing Self-Explaining Models by Prototypical Relevance Propagation},
journal = {Pattern Recognition},
volume = {136},
pages = {109172},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109172},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006513},
author = {Srishti Gautam and Marina M.-C. Höhne and Stine Hansen and Robert Jenssen and Michael Kampffmeyer},
keywords = {Self-explaining models, Explainable AI, Deep learning, Spurious Correlation Detection},
abstract = {Current machine learning models have shown high efficiency in solving a wide variety of real-world problems. However, their black box character poses a major challenge for the comprehensibility and traceability of the underlying decision-making strategies. As a remedy, numerous post-hoc and self-explanation methods have been developed to interpret the models’ behavior. Those methods, in addition, enable the identification of artifacts that, inherent in the training data, can be erroneously learned by the model as class-relevant features. In this work, we provide a detailed case study of a representative for the state-of-the-art self-explaining network, ProtoPNet, in the presence of a spectrum of artifacts. Accordingly, we identify the main drawbacks of ProtoPNet, especially its coarse and spatially imprecise explanations. We address these limitations by introducing Prototypical Relevance Propagation (PRP), a novel method for generating more precise model-aware explanations. Furthermore, in order to obtain a clean, artifact-free dataset, we propose to use multi-view clustering strategies for segregating the artifact images using the PRP explanations, thereby suppressing the potential artifact learning in the models.}
}
@article{XIE2023109233,
title = {Multi-scale local-temporal similarity fusion for continuous sign language recognition},
journal = {Pattern Recognition},
volume = {136},
pages = {109233},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109233},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007129},
author = {Pan Xie and Zhi Cui and Yao Du and Mengyi Zhao and Jianwei Cui and Bin Wang and Xiaohui Hu},
keywords = {Sign language recognition, Temporal similarity, Content-aware feature selector, Position-aware convolution, Content-dependent aggregator},
abstract = {Continuous sign language recognition (cSLR) is a public significant task that transcribes a sign language video into an ordered gloss sequence. It is important to capture the fine-grained gloss-level details, since there is no explicit alignment between sign video frames and the corresponding glosses. Among the past works, one promising way is to adopt a one-dimensional convolutional network (1D-CNN) to temporally fuse the sequential frames. However, CNNs are agnostic to similarity or dissimilarity, and thus are unable to capture local consistent semantics within temporally neighboring frames. To address the issue, we propose to adaptively fuse local features via temporal similarity for this task. Specifically, we devise a Multi-scale Local-Temporal Similarity Fusion Network (mLTSF-Net) as follows: 1) In terms of a specific video frame, we firstly select its similar neighbours with multi-scale receptive regions to accommodate different lengths of glosses. 2) To ensure temporal consistency, we then use position-aware convolution to temporally convolve each scale of selected frames. 3) To obtain a local-temporally enhanced frame-wise representation, we finally fuse the results of different scales using a content-dependent aggregator. We train our model in an end-to-end fashion, and the experimental results on RWTH-PHOENIX-Weather 2014 datasets (RWTH) demonstrate that our model achieves competitive performance compared with several state-of-the-art models.}
}
@article{LI2023109196,
title = {Linear discriminant analysis with generalized kernel constraint for robust image classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109196},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109196},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006756},
author = {Shuyi Li and Hengmin Zhang and Ruijun Ma and Jianhang Zhou and Jie Wen and Bob Zhang},
keywords = {Linear discriminant analysis, Kernel constraint, Intra-class and inter-class distance, Separability, Image classification},
abstract = {Linear discriminant analysis (LDA) as a classical supervised dimensionality reduction method has shown powerful capability in various image classification tasks. The purpose of LDA seeks an optimal linear transformation that maps the original data to a low-dimensional space. Inspired by the fact that the kernel trick can capture the nonlinear similarity of features, we propose a novel generalized distance constraint dubbed intra-class and inter-class kernel constraint (IIKC). The proposed IIKC explicitly models the category kernel distance and focuses on helping the original LDA capture more discriminant features in order to further improve the separability and magnitude difference between nearby data points. Our proposed method with IIKC aims to achieve maximum category separability by minimizing the intra-class kernel distances as well as maximizing the inter-class kernel distance, simultaneously. Extensive experimental results on six publicly available benchmark databases illustrate that the LDA-based methods embedded with the proposed IIKC significantly improve the discrimination ability and achieve a better classification performance than the original and state-of-the-art LDA algorithms.}
}
@article{LI2023109215,
title = {Neural operator search},
journal = {Pattern Recognition},
volume = {136},
pages = {109215},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109215},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200694X},
author = {Wei Li and Shaogang Gong and Xiatian Zhu},
keywords = {Neural architecture search, Search space, Self-calibration operations, Dynamic convolution, Attention learning, Block design, Neural operation, Knowledge distillation},
abstract = {Existing neural architecture search (NAS) methods usually explore a limited feature-transformation-only search space, ignoring other advanced feature operations such as feature self-calibration by attention and dynamic convolutions. This disables the NAS algorithms to discover more advanced network architectures. We address this limitation by additionally exploiting feature self-calibration operations, resulting in a heterogeneous search space. To solve the challenges of operation heterogeneity and significantly larger search space, we formulate a neural operator search (NOS) method. NOS presents a novel heterogeneous residual block for integrating the heterogeneous operations in a unified structure, and an attention guided search strategy for facilitating the search process over a vast space. Extensive experiments show that NOS can search novel cell architectures with highly competitive performance on the CIFAR and ImageNet benchmarks.}
}
@article{ARUMUGAM2023109212,
title = {Interpreting denoising autoencoders with complex perturbation approach},
journal = {Pattern Recognition},
volume = {136},
pages = {109212},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109212},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006914},
author = {Dharanidharan Arumugam and Ravi Kiran},
keywords = {Complex step derivative approximation, Saliency maps, Trustworthiness, Pixel attributions, Sanity checks and deep neural networks (DNNs)},
abstract = {The goal of this study is to interpret denoising autoencoders by quantifying the importance of input pixel features for image reconstruction. The importance of pixel features is evaluated using the attributions of the pixel features to the latent variables of a denoising autoencoder used for image reconstruction. Pixel attributions are computed using a highly accurate and automatable perturbation approach and are plotted as saliency maps. Saliency maps highlight the contribution of the pixels for image reconstruction. The proposed approach produces more meaningful and understandable explanations than guided backpropagation and layer wise propagation methods. Three sanity checks are introduced to verify the fidelity of the generated saliency maps and also to elucidate the influence of inputs on the latent variables. The classification accuracy of images is significantly lowered when the most important pixel regions highlighted by the saliency maps are corrupted validating the proposed approach.}
}
@article{XIE2023109192,
title = {Laplacian Lp norm least squares twin support vector machine},
journal = {Pattern Recognition},
volume = {136},
pages = {109192},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109192},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006719},
author = {Xijiong Xie and Feixiang Sun and Jiangbo Qian and Lijun Guo and Rong Zhang and Xulun Ye and Zhijin Wang},
keywords = {Semi-supervised learning, Laplacian Lp norm least squares twin support vector machine, Lp norm graph regularization, Geometric information},
abstract = {Semi-supervised learning has become a hot learning framework, where large amounts of unlabeled data and small amounts of labeled data are available during the training process. The recently proposed Laplacian least squares twin support vector machine (Lap-LSTSVM) is an excellent tool to solve the semi-supervised classification problem. Motivated by the success of Lap-LSTSVM, in this paper, we propose a novel Laplacian Lp norm least squares twin support vector machine (Lap-LpLSTSVM). There are several advantages of our proposed method: (1) The performance of our proposed Lap-LpLSTSVM can be improved by the adjustability of the value of p. (2) The introduced Lp norm graph regularization term can efficiently exploit the geometric information embedded in the data. (3) An efficient iterative strategy is employed to solve the optimization problem. Besides, to demonstrate that our proposed method can make use of unlabeled data effectively, least squares twin support vector machine (LSTSVM) which only uses the same labeled data is used to compare with our proposed method. The experimental results on both synthetic and real-world datasets show that our proposed method outperforms other state-of-the-art methods and can also deal with noisy datasets.}
}
@article{FANG2023109248,
title = {A novel DAGAN for synthesizing garment images based on design attribute disentangled representation},
journal = {Pattern Recognition},
volume = {136},
pages = {109248},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109248},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007270},
author = {Naiyu Fang and Lemiao Qiu and Shuyou Zhang and Zili Wang and Kerui Hu and Kang Wang},
keywords = {DAGAN, Garment design attributes, Disentangled representation, Garment image synthesis, Online costume design},
abstract = {In online costume design, it is vital to preview the design effect rapidly by entangling design attributes from reference images. This paper proposes a novel method, named design attributes generative adversarial network (DAGAN) for synthesizing garment images based on design attribute disentangled representation. The garment style is disentangled into the shape, texture, shadow, and decoration design attributes. The shape mask, repeating texture region, Laplace image gradient, and local logo are leveraged as visual representations for clothing design attributes from reference images. Following the design sequence from global to local, GDA-Net and LDA-Net in DAGAN entangle global and local design attributes, respectively, in the latent space. Then, the desired garment image is synthesized to represent design intentions explicitly. The DA-dataset for clothing design attributes is released. Extensive experiments demonstrate that the DAGAN is robust to various instances of design attributes on Design Attributes dataset (DA-dataset), and that is superior to the cross-domain transfer models in entangling design attributes from reference images.}
}
@article{SALAZAR2023109240,
title = {A proxy learning curve for the Bayes classifier},
journal = {Pattern Recognition},
volume = {136},
pages = {109240},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109240},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007191},
author = {Addisson Salazar and Luis Vergara and Enrique Vidal},
keywords = {Classification, Parameter learning, Sample size, Training set size, Probability of error},
abstract = {In this paper, a theoretical learning curve is derived for the multi-class Bayes classifier. This curve fits general multivariate parametric models of the class-conditional probability density. The derivation uses a proxy approach based on analyzing the convergence of a statistic which is proportional to the posterior probability of the true class. By doing so, the curve depends only on the training set size and on the dimension of the feature vector; it does not depend on the model parameters. Essentially, the learning curve provides an estimate of the reduction in the excess of the probability of error that can be obtained by increasing the training set size. This makes it attractive in order to deal with the practical problems of defining appropriate training set sizes.}
}
@article{YUAN2023109228,
title = {An effective CNN and Transformer complementary network for medical image segmentation},
journal = {Pattern Recognition},
volume = {136},
pages = {109228},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109228},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007075},
author = {Feiniu Yuan and Zhengxiao Zhang and Zhijun Fang},
keywords = {Transformer, Medical image segmentation, Feature complementary module, Cross-domain fusion, Convolutional Neural Network},
abstract = {The Transformer network was originally proposed for natural language processing. Due to its powerful representation ability for long-range dependency, it has been extended for vision tasks in recent years. To fully utilize the advantages of Transformers and Convolutional Neural Networks (CNNs), we propose a CNN and Transformer Complementary Network (CTCNet) for medical image segmentation. We first design two encoders by Swin Transformers and Residual CNNs to produce complementary features in Transformer and CNN domains, respectively. Then we cross-wisely concatenate these complementary features to propose a Cross-domain Fusion Block (CFB) for effectively blending them. In addition, we compute the correlation between features from the CNN and Transformer domains, and apply channel attention to the self-attention features by Transformers for capturing dual attention information. We incorporate cross-domain fusion, feature correlation and dual attention together to propose a Feature Complementary Module (FCM) for improving the representation ability of features. Finally, we design a Swin Transformer decoder to further improve the representation ability of long-range dependencies, and propose to use skip connections between the Transformer decoded features and the complementary features for extracting spatial details, contextual semantics and long-range information. Skip connections are performed in different levels for enhancing multi-scale invariance. Experimental results show that our CTCNet significantly surpasses the state-of-the-art image segmentation models based on CNNs, Transformers, and even Transformer and CNN combined models designed for medical image segmentation. It achieves superior performance on different medical applications, including multi-organ segmentation and cardiac segmentation.}
}
@article{PENG2023109244,
title = {DIODE: Dilatable Incremental Object Detection},
journal = {Pattern Recognition},
volume = {136},
pages = {109244},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109244},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007233},
author = {Can Peng and Kun Zhao and Sam Maksoud and Tianren Wang and Brian C. Lovell},
keywords = {Incremental learning, Object detection},
abstract = {To accommodate rapid changes in the real world, the cognition system of humans is capable of continually learning concepts. On the contrary, conventional deep learning models lack this capability of preserving previously learned knowledge. When a neural network is fine-tuned to learn new tasks, its performance on previously trained tasks will significantly deteriorate. Many recent works on incremental object detection tackle this problem by introducing advanced regularization. Although these methods have shown promising results, the benefits are often short-lived after the first incremental step. Under multi-step incremental learning, the trade-off between old knowledge preserving and new task learning becomes progressively more severe. Thus, the performance of regularization-based incremental object detectors gradually decays for subsequent learning steps. In this paper, we aim to alleviate this performance decay on multi-step incremental detection tasks by proposing a dilatable incremental object detector (DIODE). For the task-shared parameters, our method adaptively penalizes the changes of important weights for previous tasks. At the same time, the structure of the model is dilated or expanded by a limited number of task-specific parameters to promote new task learning. Extensive experiments on PASCAL VOC and COCO datasets demonstrate substantial improvements over the state-of-the-art methods. Notably, compared with the state-of-the-art methods, our method achieves up to 6.0% performance improvement by increasing the number of parameters by just 1.2% for each newly learned task.}
}
@article{ONVUNGC2023109207,
title = {The Dahu graph-cut for interactive segmentation on 2D/3D images},
journal = {Pattern Recognition},
volume = {136},
pages = {109207},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109207},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006860},
author = {Minh {Ôn Vũ Ngọc} and Edwin Carlinet and Jonathan Fabrizio and Thierry Géraud},
keywords = {Vectorial Dahu pseudo-distance, Minimum barrier distance, Visual saliency, Object segmentation, Mathematical morphology},
abstract = {Interactive image segmentation is an important application in computer vision for selecting objects of interest in images. Several interactive segmentation methods are based on distance transform algorithms. However, the most known distance transform, geodesic distance, is sensitive to noise in the image and to seed placement. Recently, the Dahu pseudo-distance, a continuous version of the minimum barrier distance (MBD), is proved to be more powerful than the geodesic distance in noisy and blurred images. This paper presents a method for combining the Dahu pseudo-distance with edge information in a graph-cut optimization framework and leveraging each’s complementary strengths. Our method works efficiently on both 2D/3D images and videos. Results show that our method achieves better performance than other distance-based and graph-cut methods, thereby reducing the user’s efforts.}
}
@article{WANG2023109260,
title = {Joint depth map super-resolution method via deep hybrid-cross guidance filter},
journal = {Pattern Recognition},
volume = {136},
pages = {109260},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109260},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007397},
author = {Ke Wang and Lijun Zhao and Jinjing Zhang and Jialong Zhang and Anhong Wang and Huihui Bai},
keywords = {Joint image filter, Depth image, Super-resolution, Hybrid-cross guidance, Space-aware group-compensation},
abstract = {Nowadays color-guided Depth map Super-Resolution (DSR) methods mainly have three thorny problems: (1) joint DSR methods have serious detail and structure loss at very high sampling rate; (2) existing DSR networks have high computational complexity; (3) color-depth inconsistency makes it hard to fuse dual-modality features. To resolve these problems, we propose a joint hybrid-cross guidance filter method to progressively recover the quality of degraded Low-Resolution (LR) depth maps by exploiting color-depth consistency from multiple perspectives. Specifically, the proposed method leverages pyramid structure to extract multi-scale features from High-Resolution (HR) color image. At each scale, hybrid side window filter block is proposed to achieve high-efficiency color feature extraction after each down-sampling for HR color image. This block is also used to extract depth features from the LR depth map. Meanwhile, we propose a multi-perspective cross-guided fusion filter block to progressively fuse high-quality multi-scale structure information of color image with corresponding enhanced depth features. In this filter block, two kinds of space-aware group-compensation modules are introduced to capture various spatial features from different perspectives. Meanwhile, color-depth cross-attention module is proposed to extract color-depth consistency features for impactful boundary preservation. Comprehensively qualitative and quantitative experimental results have demonstrated that our method can achieve superior performances against a lot of state-of-the-art depth SR approaches in terms of mean absolute deviation and root mean square error on Middlebury, NYU-v2 and RGB-D-D datasets.}
}
@article{SHEN2023109236,
title = {Compact network embedding for fast node classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109236},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109236},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007154},
author = {Xiaobo Shen and Yew-Soon Ong and Zheng Mao and Shirui Pan and Weiwei Liu and Yuhui Zheng},
keywords = {Network embedding, Hashing, Compact representation, Graph},
abstract = {Network embedding has shown promising performance in real-world applications. The network embedding typically lies in a continuous vector space, where storage and computation costs are high, especially in large-scale applications. This paper proposes more compact representation to fulfill the gap. The proposed discrete network embedding (DNE) leverages hash code to represent node in Hamming space. The Hamming similarity between hash codes approximates the ground-truth similarity. The embedding and classifier are jointly learned to improve compactness and discrimination. The proposed multi-class classifier is further constrained to be discrete to expedite classification. In addition, this paper further extends DNE and proposes deep discrete attributed network embedding (DDANE) to learn compact deep embedding from more informative attributed network. From the perspective of generalized signal smoothing, the proposed DDANE trains an improved graph convolutional network autoencoder to effectively leverage node attribute and network structure. Extensive experiments on node classification demonstrate the proposed methods exhibit lower storage and computational complexity than state-of-the-art network embedding methods, and achieve satisfactory accuracy.}
}
@article{ZHAO2023109199,
title = {The neglected background cues can facilitate finger vein recognition},
journal = {Pattern Recognition},
volume = {136},
pages = {109199},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109199},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006781},
author = {Pengyang Zhao and Shuping Zhao and Jing-Hao Xue and Wenming Yang and Qingmin Liao},
keywords = {Finger vein recognition, Vein trait, Background cue, Intensity orientation vector, Binary feature learning},
abstract = {Recently, finger vein based biometric authentication has attracted considerable attention due to its high efficiency and high security. However, most existing finger vein representation methods focus on vein traits while ignoring background cues, although background cues also convey identity information specific to each individual. In this paper, we leverage background intensity variations in finger vein images as new features to enrich discriminative representation, and accordingly propose a new descriptor named Intensity Orientation Vector (IOV). IOV, scaleable to reflect characteristics of finger tissues, offers additional informative cues for finger vein representation. Furthermore, we propose a new learning scheme named Semantic Similarity Preserved Discrete Binary Feature Learning (SSP-DBFL) for finger vein recognition. Unlike the most bimodal binary feature representation methods, SSP-DBFL preserves high-level semantic similarity in a common Hamming space to exploit the consensus between vein traits and background cues. Specifically, given a finger vein image, we first extract the direction difference vectors (DDV) as the main vein traits and the IOV as the auxiliary background cues. Subsequently, we jointly learn projection functions from these two types of features in a supervised manner, converting the two features into discriminative binary codes with their semantic similarity preserved. Finally, the binary codes are pooled into histogram-based vectors for finger vein representation. Extensive experiments are conducted on five widely used finger vein databases and demonstrate the effectiveness of our proposed IOV and SSP-DBFL.}
}
@article{YANG2023109232,
title = {Detecting and grouping keypoints for multi-person pose estimation using instance-aware attention},
journal = {Pattern Recognition},
volume = {136},
pages = {109232},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109232},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007117},
author = {Sen Yang and Ze Feng and Zhicheng Wang and Yanjie Li and Shoukui Zhang and Zhibin Quan and Shu-tao Xia and Wankou Yang},
keywords = {Multi-person human pose estimation, Self-attention, Bottom-up, Transformer, Grouping, Keypoints association},
abstract = {Bottom-up human pose estimation models detect keypoints and learn associative information between keypoints, usually requiring human predefined offset fields or embeddings for keypoints grouping (clustering). In this paper, we present a brand new method that can entirely solve these problems based on Transformer, making the grouping process free of the human-defined associative signals. Specifically, the self-attention in vision Transformer measures feature similarity between any pair of locations, which provides a metric space to associate keypoints together into corresponding human instances. However, the naive attention patterns formed in Transformer are still not subjectively controlled, so there is no guarantee that the keypoints only attend to the instances to which they belong. To address it we propose a novel approach of supervising self-attention to be instance-aware, simultaneously accomplishing multi-person keypoint detection and clustering. By doing so, we can group the detected keypoints to their corresponding instances, according to the pairwise attention scores. An additional benefit of our method is that the instance segmentation results of any number of people can be directly obtained from the supervised attention matrix, thereby simplifying the pixel assignment pipeline. The qualitative and quantitative results on the COCO shows that, with a very simple architecture design, our method can achieve comparable performance against the CNN-based bottom-up counterparts with fewer parameters, which also demonstrate a promising way to control self-attention mechanism behavior for specific purposes.}
}
@article{WU2023109187,
title = {Pure graph-guided multi-view subspace clustering},
journal = {Pattern Recognition},
volume = {136},
pages = {109187},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109187},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006665},
author = {Hongjie Wu and Shudong Huang and Chenwei Tang and Yancheng Zhang and Jiancheng Lv},
keywords = {Multi-view learning, Subspace clustering, Graph learning, Pure graph},
abstract = {Multi-view subspace clustering approaches have shown outstanding performance in revealing similarity relationships and complex structures hidden in data. Despite the progress, previous multi-view clustering methods still face two challenges: (1) it is difficult to simultaneously achieve sparsity and connectivity of the affinity graph; (2) existing methods usually separate the graph learning step from the clustering process, which leads to unsatisfactory clustering performance as the final results critically rely on the learned graph. In this paper, we propose to achieve a structured consensus graph for multi-view subspace clustering by leveraging the sparsity and connectivity of each affinity graph. In the proposed method, the pure graph for each view is searched by finding the good neighbors. The multiple pure graphs are further fused into a consensus graph with a block-diagonal structure. That is, the consensus graph is enforced to contain exactly c connected components where c is the number of the clusters. Hence the label to each sample can be directly assigned since each connected component precisely corresponds to an individual cluster. As a result, the proposed model seamlessly accomplishes the subtasks including graph construction, pure graph learning (i.e., good neighbors searching), and cluster label allocation in a mutual reinforcement manner. Extensive experimental results demonstrate the superiority and reliability of our proposed method.}
}
@article{ZHANG2023109171,
title = {Lower bound estimation of recommendation error through user uncertainty modeling},
journal = {Pattern Recognition},
volume = {136},
pages = {109171},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109171},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006501},
author = {Heng-Ru Zhang and Ying Qiu and Ke-Lin Zhu and Fan Min},
keywords = {Magic barrier estimation, Mixture of exponential power, Recommender system, Uncertainty modeling},
abstract = {In machine learning, the Bayesian error is the lower bound of the prediction error induced by data distribution. In recommender systems, this is also known as the magic barrier (MGBR). MGBR estimation is an important issue because the recommended data frequently contain considerable uncertainties that are difficult to quantify. It is possible to determine the extent to which the recommendation algorithm can be optimized by obtaining the MGBR for a given dataset. MGBR estimation generally requires real user ratings that are not affected by external factors such as human emotions and living environment, which can be extremely difficult or even impossible to gather. Existing theoretical approaches based on simple models, such as Gaussian distributions, have limited estimation capabilities. In this paper, we propose a more sophisticated mixture of exponential power (MoEP) model, which enables adaptive parameter selection for intricate uncertainty. To fit the distribution of the real data, we constructed a flexible learning model that automatically adjusts super- or sub-Gaussian uncertainties using the MoEP components. To select parameters adaptively, we employed an expectation-maximization algorithm to infer the parameters of the components. To estimate the MGBR, we explored an approach for calculating the lower bound of the prediction error under the guidance of a probability model. Experiments on the four datasets validated the rationality of the proposed method. The results show that the MGBR estimated using the new model is marginally lower than the prediction error of state-of-the-art algorithms.}
}
@article{RUIZSANTAQUITERIA2023109252,
title = {Improving handgun detection through a combination of visual features and body pose-based data},
journal = {Pattern Recognition},
volume = {136},
pages = {109252},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109252},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007312},
author = {Jesus Ruiz-Santaquiteria and Alberto Velasco-Mata and Noelia Vallez and Oscar Deniz and Gloria Bueno},
keywords = {Handgun detection, Human pose estimation, CCTV Surveillance, Transformers, False positive filtering},
abstract = {Early detection of the presence of dangerous objects such as handguns in Closed-Circuit Television (CCTV) images is vital to reduce the potential damage. In this work, a novel method for automatic detection of handguns in CCTV-like images based on a combination architecture which leverages body pose estimation is proposed. Weapon appearance features along with body pose features are combined to perform robust detection in typical surveillance environments where appearance features alone are not sufficient (e.g., because the handgun may appear too small or dark). Both CNN and recent transformer-based architectures are applied for visual feature extraction. Experiments on multiple datasets show that this approach improves state-of-the-art pose-based handgun detectors. An ablation study is also performed to verify the contribution of the pose processing branch and the false positive filter.}
}
@article{ZHOU2023109203,
title = {Feature learning network with transformer for multi-label image classification},
journal = {Pattern Recognition},
volume = {136},
pages = {109203},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109203},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006823},
author = {Wei Zhou and Peng Dou and Tao Su and Haifeng Hu and Zhijie Zheng},
keywords = {Multi-label classification, Transformer, Multi-scale features, Spatial attention, Salient features, Feature suppression},
abstract = {The purpose of multi-label image classification task is to accurately assign a set of labels to the objects in images. Although promising results have been achieved, most of the existing methods cannot effectively learn multi-scale features, so it is difficult to identify small-scale objects from images. Besides, current attention-based methods tend to learn the most salient feature regions in images, but fail to excavate various potential useful features concealed by the most salient feature, thus limiting the further improvement of model performance. To address above issues, we propose a novel Feature Learning network based on Transformer to learn salient features and excavate potential useful features (FL-Tran). Specifically, in order to solve the problem that current methods are difficult to identify small-scale objects, we first present a novel multi-scale fusion module (MSFM) to align high-level features and low-level features to learn multi-scale features. Additionally, a spatial attention module (SAM) utilizing transformer encoder is introduced to capture salient object features in images to enhance the model performance. Furthermore, we devise a feature enhancement and suppression module (FESM) with the aim of excavating potential useful features concealed by the most salient features. By suppressing the most salient features obtained in current SAM layer, and then forcing subsequent SAM layer to excavate potential salient features in feature maps, FL-Tran model can learn various useful features more comprehensively. Extensive experiments on MS-COCO 2014, PASCAL VOC 2007, and NUS-WIDE datasets demonstrate that our proposed FL-Tran model outperforms current state-of-the-art methods.}
}
@article{CAI2023109195,
title = {Brain-like retinex: A biologically plausible retinex algorithm for low light image enhancement},
journal = {Pattern Recognition},
volume = {136},
pages = {109195},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109195},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006744},
author = {Rongtai Cai and Zekun Chen},
keywords = {Retinex, Low light image enhancement, Contour detection, Edge detection, Brain-inspired computation, Color constancy, Visual cortex, Retinal circuit},
abstract = {Retinex theory was first proposed by Land and McCann [1], where retinex is a portmanteau derived from the words of retina and cortex, implying that both the retina and cerebral cortex may participate in the perception of lightness and color. However, there are no recent reports on how the retina and visual cortex perform retinex decomposition. In this paper, we propose a biologically plausible solution to retinex decomposition. We develop an algorithm motivated by the primate’s retinal circuit to detect textural gradients, design an algorithm originating from the visual cortex to extract image contours, and thus split image edges into image contours and textural gradients. Then, we establish a variational model for retinex decomposition by using image contours and textural gradients to encode discontinuities in illumination and variations in reflectance, respectively. We also apply the proposed retinex model to low light image enhancement, high dynamic resolution image toning, and color constancy. Experiments show consistent superiority of the proposed algorithm. The code is available at Github.}
}
@article{SONG2023109198,
title = {Prior depth-based multi-view stereo network for online 3D model reconstruction},
journal = {Pattern Recognition},
volume = {136},
pages = {109198},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109198},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200677X},
author = {Soohwan Song and Khang Giang Truong and Daekyum Kim and Sungho Jo},
keywords = {Multi-view stereo, Deep learning, Online 3D reconstruction},
abstract = {This study addresses the online multi-view stereo (MVS) problem when reconstructing precise 3D models in real time. To solve this problem, most previous studies adopted a motion stereo approach that sequentially estimates depth maps from multiple localized images captured in a local time window. To compute the depth maps quickly, the motion stereo methods process down-sampled images or use a simplified algorithm for cost volume regularization; therefore, they generally produce reconstructed 3D models that are inaccurate. In this paper, we propose a novel online MVS method that accurately reconstructs high-resolution 3D models. This method infers prior depth information based on sequentially estimated depths and leverages it to estimate depth maps more precisely. The method constructs a cost volume by using the prior-depth-based visibility information and then fuses the prior depths into the cost volume. This approach significantly improves the stereo matching performance and completeness of the estimated depths. Extensive experiments showed that the proposed method outperforms other state-of-the-art MVS and motion stereo methods. In particular, it significantly improves the completeness of 3D models.}
}
@article{LIU2023109191,
title = {Automated lesion segmentation in fundus images with many-to-many reassembly of features},
journal = {Pattern Recognition},
volume = {136},
pages = {109191},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109191},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006707},
author = {Qing Liu and Haotian Liu and Wei Ke and Yixiong Liang},
keywords = {Feature reassembly, Upsampling operator, Downsampling operator, Lesion segmentation, Fundus image analysis},
abstract = {Existing CNN-based segmentation approaches have achieved remarkable progresses on segmenting objects in regular sizes. However, when migrating them to segment tiny retinal lesions, they encounter challenges. The feature reassembly operators that they adopt are prone to discard the subtle activations about tiny lesions and fail to capture long-term dependencies. This paper aims to solve these issues and proposes a novel Many-to-Many Reassembly of Features (M2MRF) for tiny lesion segmentation. Our proposed M2MRF reassembles features in a dimension-reduced feature space and simultaneously aggregates multiple features inside a large predefined region into multiple output features. In this way, subtle activations about small lesions can be maintained as much as possible and long-term spatial dependencies can be captured to further enhance the lesion features. Experimental results on two lesion segmentation benchmarks, i.e., DDR and IDRiD, show that 1) our M2MRF outperforms existing feature reassembly operators, and 2) equipped with our M2MRF, the HRNetV2 is able to achieve substantially better performances and generalisation ability than existing methods. Our code is made publicly available at https://github.com/CVIU-CSU/M2MRF-Lesion-Segmentation.}
}
@article{WU2023109211,
title = {Semi-supervised cross-modal hashing via modality-specific and cross-modal graph convolutional networks},
journal = {Pattern Recognition},
volume = {136},
pages = {109211},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109211},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006902},
author = {Fei Wu and Shuaishuai Li and Guangwei Gao and Yimu Ji and Xiao-Yuan Jing and Zhiguo Wan},
keywords = {Cross-modal hashing, semi-supervised learning, graph convolutional networks, modality-specific features, modality-shared features},
abstract = {Cross-modal hashing maps heterogeneous multimedia data into Hamming space for retrieving relevant samples across modalities, which has received great research interests due to its rapid retrieval and low storage cost. In real-world applications, due to high manual annotation cost of multi-media data, we can only make use of limited number of labeled data with rich unlabeled data. In recent years, several semi-supervised cross-modal hashing (SCH) methods have been presented. However, how to fully explore and jointly utilize the modality-specific (complementarity) and modality-shared (correlation) information for retrieval has not been well studied for existing SCH works. In this paper, we propose a novel SCH approach named Modality-specific and Cross-modal Graph Convolutional Networks (MCGCN). The network architecture contains two modality-specific channels and a cross-modal channel to learn modality-specific and shared representations for each modality, respectively. Graph convolutional network (GCN) is leveraged in these three channels to explore intra-modal and inter-modal similarity, and perform semantic information propagation from labeled data to unlabeled data. Modality-specific and shared representations for each modality are fused with attention scheme. To further reduce the modality gap, a discriminative model is designed, learning to classify the modality of representations, and network training is guided by adversarial scheme. Experiments on two widely used multi-modal datasets demonstrate MCGCN outperforms state-of-the-art semi-supervised/supervised cross-modal hashing methods.}
}
@article{AZIZ2023109183,
title = {Fast geometrical extraction of nearest neighbors from multi-dimensional data},
journal = {Pattern Recognition},
volume = {136},
pages = {109183},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109183},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006628},
author = {Yasir Aziz and Kashif Hussain Memon},
keywords = {Nearest neighbors, Classification, Hashing, Windowing operation},
abstract = {K-Nearest Neighbor (KNN) algorithm plays a significant role in various fields of data science and machine learning. Most variants of the KNN algorithm involve distance computations and a parameter (K) that represents the required number of neighbors. The recent research regarding distance computations and finding the optimal value of K have made neighborhood extraction a slow process. This research presents a fast geometrical approach for neighborhood extraction from multi-dimensional data. Instead of distance computations, the proposed algorithm creates a geometrical shape based on the number of features of data. This geometrical shape encompasses the reference data point and the neighboring points. The proposed algorithm's efficiency of time, classification, and hashing are evaluated and compared with existing state-of-the-art algorithms.}
}