@article{LIM2022108285,
title = {Protect, show, attend and tell: Empowering image captioning models with ownership protection},
journal = {Pattern Recognition},
volume = {122},
pages = {108285},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108285},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004659},
author = {Jian Han Lim and Chee Seng Chan and Kam Woh Ng and Lixin Fan and Qiang Yang},
keywords = {Image captioning, Ownership protection, Deep neural network, Recurrent neural network, Long short-term memory},
abstract = {By and large, existing Intellectual Property (IP) protection on deep neural networks typically i) focus on image classification task only, and ii) follow a standard digital watermarking framework that was conventionally used to protect the ownership of multimedia and video content. This paper demonstrates that the current digital watermarking framework is insufficient to protect image captioning tasks that are often regarded as one of the frontiers AI problems. As a remedy, this paper studies and proposes two different embedding schemes in the hidden memory state of a recurrent neural network to protect the image captioning model. From empirical points, we prove that a forged key will yield an unusable image captioning model, defeating the purpose of infringement. To the best of our knowledge, this work is the first to propose ownership protection on image captioning task. Also, extensive experiments show that the proposed method does not compromise the original image captioning performance on all common captioning metrics on Flickr30k and MS-COCO datasets, and at the same time it is able to withstand both removal and ambiguity attacks. Code is available at https://github.com/jianhanlim/ipr-imagecaptioning}
}
@article{BISWAS2022108301,
title = {Privacy-aware supervised classification: An informative subspace based multi-objective approach},
journal = {Pattern Recognition},
volume = {122},
pages = {108301},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108301},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004817},
author = {Chandan Biswas and Debasis Ganguly and Partha Sarathi Mukherjee and Ujjwal Bhattacharya and Yufang Hou},
keywords = {Privacy preserving representation learning, Informative subspace, Multi-objective learning, Defence against information stealing adversarial attacks},
abstract = {Sharing the raw or an abstract representation of a labelled dataset on cloud platforms can potentially expose sensitive information of the data to an adversary, e.g., in the case of an emotion classification task from text, an adversary-agnostic abstract representation of the text data may eventually lead an adversary to identify the demographics of the authors, such as their gender and age. In this paper, we propose a universal defense mechanism against such malicious attempts of stealing sensitive information from data shared on cloud platforms. More specifically, our proposed method employs an informative subspace based multi-objective approach to obtain a sensitive information aware encoding of the data representation. A number of experiments conducted on both standard text and image datasets demonstrate that our proposed approach is able to reduce the effectiveness of the adversarial task (i.e., in other words is able to better protect the sensitive information of the data) without significantly reducing the effectiveness of the primary task itself.}
}
@article{YAN2022108342,
title = {High dynamic range imaging via gradient-aware context aggregation network},
journal = {Pattern Recognition},
volume = {122},
pages = {108342},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108342},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005227},
author = {Qingsen Yan and Dong Gong and Javen Qinfeng Shi and Anton van {den Hengel} and Jinqiu Sun and Yu Zhu and Yanning Zhang},
keywords = {High dynamic range imaging, Deep learning, Exposure fusion, Ghosting artifacts, Image gradients},
abstract = {Obtaining a high dynamic range (HDR) image from multiple low dynamic range images with different exposures is an important step in various computer vision tasks. One of the ongoing challenges in the field is to generate HDR images without ghosting artifacts. Motivated by an observation that such artifacts are particularly noticeable in the gradient domain, in this paper, we propose an HDR imaging approach that aggregates the information from multiple LDR images with guidance from image gradient domain. The proposed method generates artifact-free images by integrating the image gradient information and the image context information in the pixel domain. The context information in a large area helps to reconstruct the contents contaminated by saturation and misalignments. Specifically, an additional gradient stream and the supervision in the gradient domain are applied to incorporate the gradient information in HDR imaging. To use the context information captured from a large area while preserving spatial resolution, we adopt dilated convolutions to extract multi-scale features with rich context information. Moreover, we build a new dataset containing 40 groups of real-world images from diverse scenes with ground truth to validate the proposed model. The samples in the proposed dataset include more challenging moving objects inducing misalignments. Extensive experimental results demonstrate that our proposed model outperforms previous methods on different datasets in terms of both quantitative measure and visual perception quality.}
}
@article{AKCAY2022108245,
title = {Towards automatic threat detection: A survey of advances of deep learning within X-ray security imaging},
journal = {Pattern Recognition},
volume = {122},
pages = {108245},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108245},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004258},
author = {Samet Akcay and Toby Breckon},
keywords = {Review, Survey, X-Ray security imaging, Deep learning},
abstract = {X-ray security screening is widely used to maintain aviation/transport security, and its significance poses a particular interest in automated screening systems. This paper aims to review computerised X-ray security imaging algorithms by taxonomising the field into conventional machine learning and contemporary deep learning applications. The first part briefly discusses the classical machine learning approaches utilised within X-ray security imaging, while the latter part thoroughly investigates the use of modern deep learning algorithms. The proposed taxonomy sub-categorises the use of deep learning approaches into supervised and unsupervised learning, with a particular focus on object classification, detection, segmentation and anomaly detection tasks. The paper further explores well-established X-ray datasets and provides a performance benchmark. Based on the current and future trends in deep learning, the paper finally presents a discussion and future directions for X-ray security imagery.}
}
@article{MA2022108297,
title = {Robust face alignment by dual-attentional spatial-aware capsule networks},
journal = {Pattern Recognition},
volume = {122},
pages = {108297},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108297},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004775},
author = {Jinyan Ma and Jing Li and Bo Du and Jia Wu and Jun Wan and Yafu Xiao},
keywords = {Face alignment, Hourglass capsule network, Adaptively local constrained dynamic routing, Capsule attention, Spatial attention},
abstract = {Face alignment in-the-wild still faces great challenges due to that i) partial occlusion blurs the inter-features spatial relations of faces and ii) traditional CNN makes the network more difficult to capture the spatial positional relations between landmarks. To address the issues above, we propose a face alignment algorithm named Dual-attentional Spatial-aware Capsule Network (DSCN). Firstly, the spatial-aware module builds a more accurate inter-features spatial constrained model with the hourglass capsule network (HGCaps) as the backbone, which can effectively enhance its robustness against occlusions. Then, two sorts of attention mechanisms, namely capsule attention and spatial attention, are added to the attention-guided module to make the network focus more on the advantageous features and suppress other unrelated ones for more effective feature recalibration. Our method achieves 1.08% failure rate on the COFW dataset, which is much lower than the current state-of-the-art algorithms. The mean error under 300W dataset and WFLW dataset are respectively 3.91% and 5.66%, which shows that DSCN is more robust to occlusion and outperforms state-of-the-art methods in the literature.}
}
@article{EELAHI2022108273,
title = {Online learnable keyframe extraction in videos and its application with semantic word vector in action recognition},
journal = {Pattern Recognition},
volume = {122},
pages = {108273},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108273},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004532},
author = {G M Mashrur {E Elahi} and Yee-Hong Yang},
keywords = {Online keyframes, Learnable threshold, Video summarization, Action recognition},
abstract = {Video processing has become a popular research direction in computer vision due to its various applications such as video summarization, action recognition, etc. Recently, deep learning-based methods have achieved impressive results in action recognition. However, these methods need to process a full video sequence to recognize the action, even though many of the frames in the video sequence are similar and non-essential to recognizing a particular action. Additionally, these non-essential frames increase the computational cost and can confuse a method in action recognition. Instead, the important frames called keyframes not only are helpful in recognizing an action but also can reduce the processing time of each video sequence in classification or in other applications, e.g. summarization. As well, current methods in video processing have not yet been demonstrated in an online fashion. Motivated by the above, we propose an online learnable module for keyframe extraction. This module can be used to select key shots in video and thus, can be applied to video summarization. The extracted keyframes can be used as input to any deep learning-based classification model to recognize action. We also propose a plugin module to use the semantic word vector as input along with keyframes and a novel train/test strategy for the classification models. To our best knowledge, this is the first time such an online module and train/test strategy have been proposed. The experimental results on many commonly used datasets in video summarization and in action recognition have demonstrated the effectiveness of the proposed module.}
}
@article{WANG2022108326,
title = {Support structure representation learning for sequential data clustering},
journal = {Pattern Recognition},
volume = {122},
pages = {108326},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108326},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005069},
author = {Xiumei Wang and Dingning Guo and Peitao Cheng},
keywords = {Sequential data, Clustering, Support structure representation},
abstract = {Sequential data clustering is a challenging task in data mining (e.g., motion recognition and video segmentation). For good performance in dealing with complex local correlation and high-dimensional structure of sequential data, representation based methods have become one of the hot topics for sequential data clustering, in which subspace clustering is a representative tool. Subspace clustering methods divide the sequence into disjoint segments according to a locally continuous and connected representation of raw data. Although the subspace clustering methods maintain the successive property of sequential data well, there exist redundant connections in the intersection of two subsequences, which will destroy the integrity of a cluster and easily cause the chained partition of the sequence. So it is necessary to learn a more specific structure representation of a sequence to preserves both sequential information and efficient connections. Besides, the representation that conducive to clustering should have sparsity and connectivity under some assumptions. To this end, we propose a novel method to learn the support structure representation of sequence, which can extract sufficient information about instances and get the compact structure of sequential data. Furthermore, a new subspace clustering method is proposed based on the representation based method. Theoretical analysis and experimental results show the effectiveness of the proposed method.}
}
@article{SHAO2022108261,
title = {Exploiting foreground and background separation for prohibited item detection in overlapping X-Ray images},
journal = {Pattern Recognition},
volume = {122},
pages = {108261},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108261},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004416},
author = {Fangtao Shao and Jing Liu and Peng Wu and Zhiwei Yang and Zhaoyang Wu},
keywords = {X-ray imagery, Object detection, Foreground and background separation (FBS), Recursive training},
abstract = {X-ray imagery security screening is an essential component of transportation and logistics. In recent years, some researchers have used computer vision algorithms to replace inefficient and tedious manual baggage inspection. However, X-ray images are complicated, and objects overlap with one another in a semi-transparent state, which underperforms the existing object detection frameworks. To solve the severe overlapping problem of X-ray images, we propose a foreground and background separation (FBS) X-ray prohibited item detection framework, which separates prohibited items from other items to exclude irrelevant information. First, we design a target foreground and use recursive training to adaptively approximate the real foreground. Thereafter, with the constraints of X-ray imaging characteristics, a decoder is employed to separate the prohibited items from other irrelevant items to obtain the foreground and background (FB). Finally, we use the attention module to make the detection framework focus more on the foreground. Our method is evaluated on a synthetic dataset with FB ground truth and two public datasets with only bounding box annotations. Extensive experimental results demonstrate that our method significantly outperforms state-of-the-art solutions. Furthermore, experiments are performed in the case where only a small number of images contain the FB ground truth. The results indicate that our method requires only a small number of FB ground truths to obtain a performance equivalent to that of all FB ground truths.}
}
@article{JEEVAN2022108308,
title = {An empirical study of the impact of masks on face recognition},
journal = {Pattern Recognition},
volume = {122},
pages = {108308},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108308},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100488X},
author = {Govind Jeevan and Geevar C. Zacharias and Madhu S. Nair and Jeny Rajan},
keywords = {Face recognition, Convolutional neural networks, Masked face, COVID-19},
abstract = {Face recognition has a wide range of applications like video surveillance, security, access control, etc. Over the past decade, the field of face recognition has matured and grown at par with the latest advancements in technology, particularly deep learning. Convolution Neural Networks have surpassed human accuracy in Face Recognition on popular evaluation tests such as LFW. However, most existing models evaluate their performance with an assumption of the availability of full facial information. The COVID-19 pandemic has laid forth challenges to this assumption, and to the performance of existing methods and leading-edge algorithms in the field of face recognition. This is in the wake of an explosive increase in the number of people wearing face masks. The reduced amount of facial information available to a recognition system from a masked face impacts their discrimination ability. In this context, we design and conduct a series of experiments comparing the masked face recognition performances of CNN architectures available in literature and exploring possible alterations in loss functions, architectures, and training methods that can enable existing methods to fully extract and leverage the limited facial information available in a masked face. We evaluate existing CNN-based face recognition systems for their performance against datasets composed entirely of masked faces, in contrast to the existing standard evaluations where masked or occluded faces are a rare occurrence. The study also presents evidence denoting an increased impact of network depth on performance compared to standard face recognition. Our observations indicate that substantial performance gains can be achieved by the introduction of masked faces in the training set. The study also inferred that various parameter settings determined suitable for standard face recognition are not ideal for masked face recognition. Through empirical analysis we derived new value recommendations for these parameters and settings.}
}
@article{CHEN2022108349,
title = {Multi-attention augmented network for single image super-resolution},
journal = {Pattern Recognition},
volume = {122},
pages = {108349},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108349},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100529X},
author = {Rui Chen and Heng Zhang and Jixin Liu},
keywords = {Super-resolution, Multi-scale U-net, pre-defined sparse kernels, Attention mechanism},
abstract = {How to improve the representational power of visual features extracted by deep convolutional neural networks is of crucial importance for high-quality image super-resolution. To address this issue, we propose a multi-attention augmented network, which mainly consists of content-, orientation- and position-aware modules. Specifically, we develop an attention augmented U-net structure to form the content-aware module in order to learn and combine multi-scale informative features within a large receptive field. To better reconstruct image details in different directions, we design a set of pre-defined sparse kernels to construct the orientation-aware module, which can extract more representative multi-orientation features and enhance the discriminative capacity in stacked convolutional stages. Then these extracted features are adaptively fused through channel attention mechanism. In upscale stage, the position-aware module adopts a novel self-attention to reweight the element-wise value of final low-resolution feature maps, for further suppressing the possible artifacts. Experimental results demonstrate that our method obtains better reconstruction accuracy and perceptual quality against state-of-the-art methods.}
}
@article{HAO2022108354,
title = {Gradient-Aligned convolution neural network},
journal = {Pattern Recognition},
volume = {122},
pages = {108354},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108354},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005343},
author = {You Hao and Ping Hu and Shirui Li and Jayaram K. Udupa and Yubing Tong and Hua Li},
keywords = {Gradient alignment, Rotation equivariant convolution, Rotation invariant neural network},
abstract = {Although Convolution Neural Networks (CNN) have achieved great success in many applications of computer vision in recent years, rotation invariance is still a difficult problem for CNN. Especially for some images, the content can appear in the image at any angle of rotation, such as medical images, microscopic images, remote sensing images and astronomical images. In this paper, we propose a novel convolution operation, called Gradient-Aligned Convolution (GAConv), which can help CNN achieve rotation invariance by replacing vanilla convolutions in CNN. GAConv is implemented with a prior pixel-level gradient alignment operation before regular convolution. With GAConv, Gradient-Aligned CNN (GACNN) can achieve rotation invariance without any data augmentation, feature-map augmentation, and filter enrichment. In GACNN, rotation invariance does not learn from the training set, but bases on the network model. Different from the vanilla CNN, GACNN will output invariant results for all rotated versions of an object, no matter whether the network is trained or not. This means that we only need to train the network with one canonical version of the object and all other rotated versions of this object should be recognized with the same accuracy. Classification experiments have been conducted to evaluate GACNN compared with some rotation invariant approaches. GACNN achieved the best results on the 360∘ rotated test set of MNIST-rotation, Plankton-sub-rotation, and Galaxy Zoo 2.}
}
@article{ALSHABI2022108309,
title = {ProCAN: Progressive growing channel attentive non-local network for lung nodule classification},
journal = {Pattern Recognition},
volume = {122},
pages = {108309},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108309},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004891},
author = {Mundher Al-Shabi and Kelvin Shak and Maxine Tan},
keywords = {Self-Attention, Non-local network, Nodule classification, Curriculum learning, Deep learning},
abstract = {Lung cancer classification in screening computed tomography (CT) scans is one of the most crucial tasks for early detection of this disease. Many lives can be saved if we are able to accurately classify malignant/cancerous lung nodules. Consequently, several deep learning based models have been proposed recently to classify lung nodules as malignant or benign. Nevertheless, the large variation in the size and heterogeneous appearance of the nodules makes this task an extremely challenging one. We propose a new Progressive Growing Channel Attentive Non-Local (ProCAN) network for lung nodule classification. The proposed method addresses this challenge from three different aspects. First, we enrich the Non-Local network by adding channel-wise attention capability to it. Second, we apply Curriculum Learning principles, whereby we first train our model on easy examples before hard ones. Third, as the classification task gets harder during the Curriculum learning, our model is progressively grown to increase its capability of handling the task at hand. We examined our proposed method on two different public datasets and compared its performance with state-of-the-art methods in the literature. The results show that the ProCAN model outperforms state-of-the-art methods and achieves an AUC of 98.05% and an accuracy of 95.28% on the LIDC-IDRI dataset. Moreover, we conducted extensive ablation studies to analyze the contribution and effects of each new component of our proposed method.}
}
@article{SARKAR2022108338,
title = {From soccer video to ball possession statistics},
journal = {Pattern Recognition},
volume = {122},
pages = {108338},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108338},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005185},
author = {Saikat Sarkar and Dipti Prasad Mukherjee and Amlan Chakrabarti},
keywords = {Ball possession statistics, Cost-flow network, Group similarity, Pass event detection, Soccer},
abstract = {Ball possession statistics in a soccer match is evaluated by counting the number of valid passes by both teams. The valid passes are determined by monitoring the start and end of a ball passing event initiated by a player. In this work, we map pass detection as detection of split and merge of nodes of a flow network. The players and ball represent nodes in the network. A group is formed by the objects (ball and players) which are spatially close to each other. Objects belonging to the same group are allowed to split or merge. We use this group relation to check if the objects split or merge in the sequence of frames. A constraint is added to the network to make sure that two objects can split only if the objects were previously merged. Flow through the split or merge node of the network denotes a ball pass event. Additional nodes like appear and disappear are added to the network to map the possibility that new objects could appear or old objects may disappear to and from the frame. The minimum cost path in the flow network provides the solution for valid pass events. Experimental evaluation shows that our proposal is at least 4% better in estimating ball possession statistics and 8% better in pass detection of a soccer match seen in a broadcast video than that of competitive methods.}
}
@article{LIN2022108300,
title = {Atom correlation based graph propagation for scene graph generation},
journal = {Pattern Recognition},
volume = {122},
pages = {108300},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108300},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004805},
author = {Bingqian Lin and Yi Zhu and Xiaodan Liang},
keywords = {Scene graph generation, Long-tailed distribution, Knowledge graph, Atom correlation, Category space},
abstract = {Long-tailed distribution in the dataset is one of the major problems of the scene graph generation task. Previous methods attempt to alleviate this by introducing human commonsense knowledge in the form of statistical correlations between object pairs. However, the reasoning path they used is usually composable and the prior knowledge they employed is generally image-specific, making the knowledge learning less flexible, stable and holistic. In this paper, we propose Atom Correlation Based Graph Propagation (AC-GP) for the scene graph generation task. Specifically, diverse atom correlations between objects and their relationships are explored by separating relationships to form new semantic nodes and decomposing the compound reasoning paths. Based on these atom correlations, the knowledge graphs are introduced for the feature enhancement by information propagating in the global category space. By exploiting atom correlations, the introduced prior knowledge can be more common and easy to learn. Moreover, propagating the knowledge in the global category space enables the model aware of more comprehensive and holistic knowledge. As a result, the model capacity and stability can be effectively improved to mine infrequent and missed relationships. Experimental results on two benchmark datasets: Visual Relation Detection (VRD) and Visual Genome (VG) show the superiority of the proposed AC-GP over strong baseline methods.}
}
@article{YANG2022108311,
title = {Nonconvex 3D array image data recovery and pattern recognition under tensor framework},
journal = {Pattern Recognition},
volume = {122},
pages = {108311},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108311},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100491X},
author = {Ming Yang and Qilun Luo and Wen Li and Mingqing Xiao},
keywords = {Tensor completion, Tensor robust principle component analysis, T-SVD, Tensor nuclear norm, Weighted tensor schatten- quasi-norm},
abstract = {In this paper, we present a weighted tensor Schatten-p quasi-norm (0<p<1) regularizer for 3D array datasets in order to recover the low-rank part and the sparse part, respectively. Corresponding algorithms associated with augmented Lagrangian multipliers are established and the constructed sequence converges to the desirable Karush-Kuhn-Tucker (KKT) point, which is mathematically validated in detail. Although the proposed weighted tensor Schatten-p quasi-norm is non-convex, it appears not only to less penalize the singular values but also to be effective in capturing the low-rank property. The main findings in this paper are the appropriate choice of p depends on specific tasks: low-rank data set recovery usually requires relatively large value of p, while sparse data set recovery needs relatively small value of p. And the weights chosen in our tensor Schatten-p quasi-norm are inversely to the singular values exponentially for promoting the sensitivity to different singular values. Experimental results for video inpainting (tensor completion), image recovery and salient object detection (tensor robust principal component analysis) have been shown that the proposed approach outperforms various latest approaches in literature.}
}
@article{CUI2022108296,
title = {Coarse-to-fine pseudo supervision guided meta-task optimization for few-shot object classification},
journal = {Pattern Recognition},
volume = {122},
pages = {108296},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108296},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004763},
author = {Yawen Cui and Qing Liao and Dewen Hu and Wei An and Li Liu},
keywords = {Unsupervised few-shot learning, Meta-learning, Clustering, Object classification},
abstract = {Few-Shot Learning (FSL) is a challenging and practical learning pattern, aiming to solve a target task which has only a few labeled examples. Currently, the field of FSL has made great progress, but largely in the supervised setting, where a large auxiliary labeled dataset is required for offline training. However, the unsupervised FSL (UFSL) problem where the auxiliary dataset is fully unlabeled has been seldom investigated despite of its significant value. This paper focuses on the more general and challenging UFSL problem and presents a novel method named Coarse-to-Fine Pseudo Supervision-guided Meta-Learning (C2FPS-ML) for unsupervised few-shot object classification. It first obtains prior knowledge from an unlabeled auxiliary dataset during unsupervised meta-training, and then use the prior knowledge to assist the downstream few-shot classification task. Coarse-to-Fine Pseudo Supervisions in C2FPS-ML aim to optimize meta-task sampling process in unsupervised meta-training stage which is one of the dominant factors for improving the performance of meta-learning based FSL algorithms. Human can learn new concepts progressively or hierarchically following the coarse-to-fine manners. By simulating this human’s behaviour, we develop two versions of C2FPS-ML for two different scenarios: one is natural object dataset and another one is other kinds of dataset (e.g., handwritten character dataset). For natural object dataset scenario, we propose to exploit the potential hierarchical semantics of the unlabeled auxiliary dataset to build a tree-like structure of visual concepts. For another scenario, progressive pseudo supervision is obtained by forming clusters in different similarity aspects and is represented by a pyramid-like structure. The obtained structure is applied as the supervision to construct meta-tasks in meta-training stage, and prior knowledge from the unlabeled auxiliary dataset is learned from the coarse-grained level to the fine-grained level. The proposed method sets the new state of the art on the gold-standard miniImageNet and achieves remarkable results on Omniglot while simultaneously increases efficiency.}
}
@article{MALHOTRA2022108243,
title = {Multi-task driven explainable diagnosis of COVID-19 using chest X-ray images},
journal = {Pattern Recognition},
volume = {122},
pages = {108243},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108243},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004246},
author = {Aakarsh Malhotra and Surbhi Mittal and Puspita Majumdar and Saheb Chhabra and Kartik Thakral and Mayank Vatsa and Richa Singh and Santanu Chaudhury and Ashwin Pudrod and Anjali Agrawal},
keywords = {X-Ray, COVID-19, Detection, Diagnostics, Deep learning, Explainable artificial intelligence, Multi-task learning},
abstract = {With increasing number of COVID-19 cases globally, all the countries are ramping up the testing numbers. While the RT-PCR kits are available in sufficient quantity in several countries, others are facing challenges with limited availability of testing kits and processing centers in remote areas. This has motivated researchers to find alternate methods of testing which are reliable, easily accessible and faster. Chest X-Ray is one of the modalities that is gaining acceptance as a screening modality. Towards this direction, the paper has two primary contributions. Firstly, we present the COVID-19 Multi-Task Network (COMiT-Net) which is an automated end-to-end network for COVID-19 screening. The proposed network not only predicts whether the CXR has COVID-19 features present or not, it also performs semantic segmentation of the regions of interest to make the model explainable. Secondly, with the help of medical professionals, we manually annotate the lung regions and semantic segmentation of COVID19 symptoms in CXRs taken from the ChestXray-14, CheXpert, and a consolidated COVID-19 dataset. These annotations will be released to the research community. Experiments performed with more than 2500 frontal CXR images show that at 90% specificity, the proposed COMiT-Net yields 96.80% sensitivity.}
}
@article{CHASANI2022108272,
title = {The UU-test for statistical modeling of unimodal data},
journal = {Pattern Recognition},
volume = {122},
pages = {108272},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108272},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004520},
author = {Paraskevi Chasani and Aristidis Likas},
keywords = {Unimodal data, Unimodality test, Statistical modeling, Uniform mixture model},
abstract = {Deciding on the unimodality of a dataset is an important problem in data analysis and statistical modeling. It allows to obtain knowledge about the structure of the dataset, i.e. whether data points have been generated by a probability distribution with a single or more than one peaks. Such knowledge is very useful for several data analysis problems, such as for deciding on the number of clusters and determining unimodal projections. We propose a technique called UU-test (Unimodal Uniform test) to decide on the unimodality of a one-dimensional dataset. The method operates on the empirical cumulative density function (ecdf) of the dataset. It attempts to build a piecewise linear approximation of the ecdf that is unimodal and models the data sufficiently in the sense that the data corresponding to each linear segment follows the uniform distribution. A unique feature of this approach is that in the case of unimodality, it also provides a statistical model of the data in the form of a Uniform Mixture Model. We present experimental results in order to assess the ability of the method to decide on unimodality and perform comparisons with the well-known dip-test approach. In addition, in the case of unimodal datasets we evaluate the Uniform Mixture Models provided by the proposed method using the test set log-likelihood and the two-sample Kolmogorov-Smirnov (KS) test.}
}
@article{SINGH2022108284,
title = {Context extraction module for deep convolutional neural networks},
journal = {Pattern Recognition},
volume = {122},
pages = {108284},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108284},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004647},
author = {Pravendra Singh and Pratik Mazumder and Vinay P. Namboodiri},
keywords = {Contextual pointwise convolution, Convolutional neural network (CNN), Image classification, Deep learning},
abstract = {Convolutional layers convolve the input feature maps to generate valuable output features, and they help deep learning methods significantly in solving complex problems. In order to tackle problems efficiently, deep learning solutions should ensure that the parameters of the model do not increase significantly with the complexity of the problem. Pointwise convolutions are primarily used for parameter reduction in many deep learning architectures. They are convolutional filters of kernel size 1×1. The pointwise convolution, however, ignores the spatial information around the points it is processing. This design is by choice, in order to reduce the overall parameters and computations. However, we hypothesize that this shortcoming of pointwise convolution has a significant impact on network performance. We propose a novel alternative design for pointwise convolution, which uses spatial information from the input efficiently. Our approach extracts spatial context information from the input at two scales and further refines the extracted context based on the channel importance. Finally, we add the refined context to the output of the pointwise convolution. This is the first work that improves pointwise convolution by incorporating context information. Our design significantly improves the performance of the networks without substantially increasing the number of parameters and computations. We perform experiments on coarse/fine-grained image classification, few-shot fine-grained classification, and on object detection. We further perform various ablation experiments to validate the significance of the different components used in our design. Lastly, we show experimentally that our proposed technique can be combined with existing state-of-the-art network performance improvement approaches to further improve the network performance.}
}
@article{ZHU2022108312,
title = {Learning multiscale hierarchical attention for video summarization},
journal = {Pattern Recognition},
volume = {122},
pages = {108312},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108312},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004921},
author = {Wencheng Zhu and Jiwen Lu and Yucheng Han and Jie Zhou},
keywords = {Video summarization, Hierarchical structure, Attention models, Multiscale temporal representation, Two-stream framework},
abstract = {In this paper, we propose a multiscale hierarchical attention approach for supervised video summarization. Different from most existing supervised methods which employ bidirectional long short-term memory networks, our method exploits the underlying hierarchical structure of video sequences and learns both the short-range and long-range temporal representations via a intra-block and a inter-block attention. Specifically, we first separate each video sequence into blocks of equal length and employ the intra-block and inter-block attention to learn local and global information, respectively. Then, we integrate the frame-level, block-level, and video-level representations for the frame-level importance score prediction. Next, we conduct shot segmentation and compute shot-level importance scores. Finally, we perform key shot selection to produce video summaries. Moreover, we extend our method into a two-stream framework, where appearance and motion information is leveraged. Experimental results on the SumMe and TVSum datasets validate the effectiveness of our method against state-of-the-art methods.}
}
@article{YANG2022108260,
title = {Bayesian compression for dynamically expandable networks},
journal = {Pattern Recognition},
volume = {122},
pages = {108260},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108260},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004404},
author = {Yang Yang and Bo Chen and Hongwei Liu},
keywords = {Bayesian compression, DEN, Continual learning, Selective retraining, Dynamically expands network, Semantic drift},
abstract = {This paper develops Bayesian Compression for Dynamically Expandable Network (BCDEN), which can learn a compact model structure with preserving the accuracy in a continual learning scenarios. Dynamically Expandable Network (DEN) is efficiently trained by performing selective retraining, dynamically expands network capacity with only the necessary number of units, and effectively prevents semantic drift by duplicating and timestamping units in an online manner. Overcoming conventional DEN only giving point estimates, we providing the Bayesian inference under the principle framework. We validate our BCDEN on multiple public datasets under continual learning setting, on which it can outperform existing continual learning methods on a variety of tasks, and with the state-of-the-art compression results, while still maintaining comparable performance.}
}
@article{WU2022108214,
title = {A multimodal attention fusion network with a dynamic vocabulary for TextVQA},
journal = {Pattern Recognition},
volume = {122},
pages = {108214},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108214},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003952},
author = {Jiajia Wu and Jun Du and Fengren Wang and Chen Yang and Xinzhe Jiang and Jinshui Hu and Bing Yin and Jianshu Zhang and Lirong Dai},
keywords = {Dynamic vocabulary, Attention map, Multimodal fusion, ST-VQA},
abstract = {Visual question answering (VQA) is a well-known problem in computer vision. Recently, Text-based VQA tasks are getting more and more attention because text information is very important for image understanding. The key to this task is to make good use of text information in the image. In this work, we propose an attention-based encoder-decoder network that combines the multimodal information of visual, linguistic, and location features together. By using the attention mechanism to focus on key features to the question, our multimodal feature fusion can provide more accurate information to improve the performance. Furthermore, we present a decoder with attention map loss, which can not only predict complex answers but also deal with a dynamic vocabulary to reduce the decoding space. Compared with softmax-based cross entropy loss which can only handle a fixed-length vocabulary, the attention map loss significantly improves the accuracy and efficiency. Our method achieved the first place of all three tasks in the ICDAR2019 robust reading challenge on scene text visual question answering (ST-VQA).}
}
@article{LIU2022108341,
title = {Weakly Supervised Segmentation of COVID19 Infection with Scribble Annotation on CT Images},
journal = {Pattern Recognition},
volume = {122},
pages = {108341},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108341},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005215},
author = {Xiaoming Liu and Quan Yuan and Yaozong Gao and Kelei He and Shuo Wang and Xiao Tang and Jinshan Tang and Dinggang Shen},
keywords = {COVID-19, infection segmentation, weakly supervised learning, transformation consistency, uncertainty},
abstract = {Segmentation of infections from CT scans is important for accurate diagnosis and follow-up in tackling the COVID-19. Although the convolutional neural network has great potential to automate the segmentation task, most existing deep learning-based infection segmentation methods require fully annotated ground-truth labels for training, which is time-consuming and labor-intensive. This paper proposed a novel weakly supervised segmentation method for COVID-19 infections in CT slices, which only requires scribble supervision and is enhanced with the uncertainty-aware self-ensembling and transformation-consistent techniques. Specifically, to deal with the difficulty caused by the shortage of supervision, an uncertainty-aware mean teacher is incorporated into the scribble-based segmentation method, encouraging the segmentation predictions to be consistent under different perturbations for an input image. This mean teacher model can guide the student model to be trained using information in images without requiring manual annotations. On the other hand, considering the output of the mean teacher contains both correct and unreliable predictions, equally treating each prediction in the teacher model may degrade the performance of the student network. To alleviate this problem, the pixel level uncertainty measure on the predictions of the teacher model is calculated, and then the student model is only guided by reliable predictions from the teacher model. To further regularize the network, a transformation-consistent strategy is also incorporated, which requires the prediction to follow the same transformation if a transform is performed on an input image of the network. The proposed method has been evaluated on two public datasets and one local dataset. The experimental results demonstrate that the proposed method is more effective than other weakly supervised methods and achieves similar performance as those fully supervised.}
}
@article{ZHU2022108325,
title = {Reasoning structural relation for occlusion-robust facial landmark localization},
journal = {Pattern Recognition},
volume = {122},
pages = {108325},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108325},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005057},
author = {Congcong Zhu and Xiaoqiang Li and Jide Li and Songmin Dai and Weiqin Tong},
keywords = {Facial landmark localization, Relational reasoning, Long short-distance dependency, Biometrics},
abstract = {In facial landmark localization tasks, various occlusions heavily degrade the localization accuracy due to the partial observability of facial features. This paper proposes a structural relation network (SRN) for occlusion-robust landmark localization. Unlike most existing methods that simply exploit the shape constraint, the proposed SRN aims to capture the structural relations among different facial components. These relations can be considered a more powerful shape constraint against occlusion. To achieve this, a hierarchical structural relation module (HSRM) is designed to hierarchically reason the structural relations that represent both long- and short-distance spatial dependencies. Compared with existing network architectures,the HSRM can efficiently model the spatial relations by leveraging its geometry-aware network architecture, which reduces the semantic ambiguity caused by occlusion. Moreover, the SRN augments the training data by synthesizing occluded faces. To further extend our SRN for occluded video data, we formulate the occluded face synthesis as a Markov decision process (MDP). Specifically, it plans the movement of the dynamic occlusion based on an accumulated reward associated with the performance degradation of the pre-trained SRN. This procedure augments hard samples for robust facial landmark tracking. Extensive experimental results indicate that the proposed method achieves outstanding performance on occluded and masked faces. Code is available at https://github.com/zhuccly/SRN}
}
@article{CHEN2022108337,
title = {Kernelized support tensor train machines},
journal = {Pattern Recognition},
volume = {122},
pages = {108337},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108337},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005173},
author = {Cong Chen and Kim Batselier and Wenjian Yu and Ngai Wong},
keywords = {Image classification, Tensor, Support tensor machine},
abstract = {Tensor, a multi-dimensional data structure, has been exploited recently in the machine learning community. Traditional machine learning approaches are vector- or matrix-based, and cannot handle tensorial data directly. In this paper, we propose a tensor train (TT)-based kernel technique for the first time, and apply it to the conventional support vector machine (SVM) for high-dimensional image classification with very small number of training samples. Specifically, we propose a kernelized support tensor train machine that accepts tensorial input and preserves the intrinsic kernel property. The main contributions are threefold. First, we propose a TT-based feature mapping procedure that maintains the TT structure in the feature space. Second, we demonstrate two ways to construct the TT-based kernel function while considering consistency with the TT inner product and preservation of information. Third, we show that it is possible to apply different kernel functions on different data modes. In principle, our method tensorizes the standard SVM on its input structure and kernel mapping scheme. This reduces the storage and computation complexity of kernel matrix construction from exponential to polynomial. The validity proof and computation complexity of the proposed TT-based kernel functions are provided elaborately. Extensive experiments are performed on high-dimensional fMRI and color images datasets, which demonstrates the superiority of the proposed scheme compared with the state-of-the-art techniques.}
}
@article{MALTOUDOGLOU2022108271,
title = {Well-calibrated confidence measures for multi-label text classification with a large number of labels},
journal = {Pattern Recognition},
volume = {122},
pages = {108271},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108271},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004519},
author = {Lysimachos Maltoudoglou and Andreas Paisios and Ladislav Lenc and Jiří Martínek and Pavel Král and Harris Papadopoulos},
keywords = {Text classification, Multi-label, Word2vec, Bert, Conformal prediction, Label powerset, Computational efficiency, Nonconformity measure, Confidence measure},
abstract = {We extend our previous work on Inductive Conformal Prediction (ICP) for multi-label text classification and present a novel approach for addressing the computational inefficiency of the Label Powerset (LP) ICP, arrising when dealing with a high number of unique labels. We present experimental results using the original and the proposed efficient LP-ICP on two English and one Czech language data-sets. Specifically, we apply the LP-ICP on three deep Artificial Neural Network (ANN) classifiers of two types: one based on contextualised (bert) and two on non-contextualised (word2vec) word-embeddings. In the LP-ICP setting we assign nonconformity scores to label-sets from which the corresponding p-values and prediction-sets are determined. Our approach deals with the increased computational burden of LP by eliminating from consideration a significant number of label-sets that will surely have p-values below the specified significance level. This reduces dramatically the computational complexity of the approach while fully respecting the standard CP guarantees. Our experimental results show that the contextualised-based classifier surpasses the non-contextualised-based ones and obtains state-of-the-art performance for all data-sets examined. The good performance of the underlying classifiers is carried on to their ICP counterparts without any significant accuracy loss, but with the added benefits of ICP, i.e. the confidence information encapsulated in the prediction sets. We experimentally demonstrate that the resulting prediction sets can be tight enough to be practically useful even though the set of all possible label-sets contains more than 1e+16 combinations. Additionally, the empirical error rates of the obtained prediction-sets confirm that our outputs are well-calibrated.}
}
@article{LIU2022108294,
title = {Multi-label sampling based on local label imbalance},
journal = {Pattern Recognition},
volume = {122},
pages = {108294},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108294},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100474X},
author = {Bin Liu and Konstantinos Blekas and Grigorios Tsoumakas},
keywords = {Multi-label learning, Class imbalance, Oversampling and undersampling, Local label imbalance, Ensemble methods},
abstract = {Class imbalance is an inherent characteristic of multi-label data that hinders most multi-label learning methods. One efficient and flexible strategy to deal with this problem is to employ sampling techniques before training a multi-label learning model. Although existing multi-label sampling approaches alleviate the global imbalance of multi-label datasets, it is actually the imbalance level within the local neighbourhood of minority class examples that plays a key role in performance degradation. To address this issue, we propose a novel measure to assess the local label imbalance of multi-label datasets, as well as two multi-label sampling approaches, namely Multi-Label Synthetic Oversampling based on Local label imbalance (MLSOL) and Multi-Label Undersampling based on Local label imbalance (MLUL). By considering all informative labels, MLSOL creates more diverse and better labeled synthetic instances for difficult examples, while MLUL eliminates instances that are harmful to their local region. Experimental results on 13 multi-label datasets demonstrate the effectiveness of the proposed measure and sampling approaches for a variety of evaluation metrics, particularly in the case of an ensemble of classifiers trained on repeated samples of the original data.}
}
@article{YANG2022108295,
title = {Exploring rich intermediate representations for reconstructing 3D shapes from 2D images},
journal = {Pattern Recognition},
volume = {122},
pages = {108295},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108295},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004751},
author = {Yang Yang and Junwei Han and Dingwen Zhang and Qi Tian},
keywords = {3D Reconstruction, Shape transformation, Intermediate representations},
abstract = {Recovering 3D voxelized shapes with fine details from single-view 2D images is an extremely challenging and ill-conditioned problem. Most of the existing methods learn the 3D reconstruction process by encoding the 3D shapes and the 2D images into the same low-dimensional latent vector, which lacks the capacity to capture detailed features in the surface of the 3D object shapes. To address this issue, we propose to explore rich intermediate representation for 3D shape reconstruction by using a newly designed network architecture. We first use a two-steam network to infer the depth map and the topology-specific mean shape from the given 2D image, which forms the intermediate representation prediction branch. The intermediate representations capture the global spatial structure and the visible surface geometric structure, which are important for reconstructing high-quality 3D shapes. Based on the obtained intermediate representation, a novel shape transformation network is then proposed to reconstruct the fine details of the whole 3D object shapes. The experimental results on the challenging ShapeNet and Pix3D datasets show that our approach outperforms the existing state-of-the-art methods.}
}
@article{SINGH2022108307,
title = {Feature wise normalization: An effective way of normalizing data},
journal = {Pattern Recognition},
volume = {122},
pages = {108307},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108307},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004878},
author = {Dalwinder Singh and Birmohan Singh},
keywords = {Data normalization, -nearest neighbor classification, Machine learning, Metaheuristic optimization, Naive bayes classification, Neural networks, Support vector machines},
abstract = {This paper presents a novel Feature Wise Normalization approach for the effective normalization of data. In this approach, each feature is normalized independently with one of the methods from the pool of normalization methods. It is in contrast to the conventional approach which normalizes the data with one method only and as a result, yields suboptimal performance. Additionally, generalization and superiority among normalization methods are also not ensured owing to different machine learning mechanisms for solving classification tasks. The proposed approach benefits from the collective response of multiple methods to normalize the data better as individual features become a normalization unit. The selection of methods is a combinatorial problem that can be solved with optimization algorithms. For this purpose, Antlion optimization is considered that combines the search of methods with the fine-tuning of classifier parameters. Twelve methods are used to create the pool beside the original scale, and the obtained data is evaluated on four learning algorithms. Experiments are performed on 18 benchmark datasets to show the efficacy of the proposed approach in contrast to conventional normalization.}
}
@article{WEI2022108335,
title = {Neighborhood preserving embedding on Grassmann manifold for image-set analysis},
journal = {Pattern Recognition},
volume = {122},
pages = {108335},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108335},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100515X},
author = {Dong Wei and Xiaobo Shen and Quansen Sun and Xizhan Gao and Zhenwen Ren},
keywords = {Neighborhood preserving embedding, Dimensionality reduction, Grassmann manifold, Twin learning},
abstract = {Modeling image sets as points on Grassmann manifold has attracted increasing interests in computer vision community and has been applied to many applications. However, such approaches have suffered from the limitation that high computational cost on Grassmann manifold must be involved, especially high-dimensional ones. In this paper, we propose an unsupervised robust dimensionality reduction algorithm for Grassmann manifold based on Neighborhood Preserving Embedding (GNPE). We first introduce two strategies to construct the coefficients-based similarity graph to eliminate the effects of errors. Then, a projection is learned from the high-dimensional Grassmann manifold to the relative low-dimensional one with more discriminative capability, where the local neighborhood structure is well preserved. To address the issue that the estimated similarity graph is unreliable with noise and outliers, we further propose a unified learning framework which performs similarity learning and projection learning simultaneously. By leveraging the interactions between these two essential tasks, we can capture accurate structures and learn discriminative projections. The proposed method can be optimized by an efficient iterative algorithm. Experiments on various image set classification and clustering tasks clearly show that our model achieves consistent improvements in terms of both effectiveness and efficiency.}
}
@article{MORALES2022108283,
title = {SetMargin loss applied to deep keystroke biometrics with circle packing interpretation},
journal = {Pattern Recognition},
volume = {122},
pages = {108283},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108283},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004635},
author = {Aythami Morales and Julian Fierrez and Alejandro Acien and Ruben Tolosana and Ignacio Serna},
keywords = {Keystroke biometrics, Circle packing, Deep learning, DML},
abstract = {This work presents a new deep learning approach for keystroke biometrics based on a novel Distance Metric Learning method (DML). DML maps input data into a learned representation space that reveals a “semantic” structure based on distances. In this work, we propose a novel DML method specifically designed to address the challenges associated to free-text keystroke identification where the classes used in learning and inference are disjoint. The proposed SetMargin Loss (SM-L) extends traditional DML approaches with a learning process guided by pairs of sets instead of pairs of samples, as done traditionally. The proposed learning strategy allows to enlarge inter-class distances while maintaining the intra-class structure of keystroke dynamics. We analyze the resulting representation space using the mathematical problem known as Circle Packing, which provides neighbourhood structures with a theoretical maximum inter-class distance. We finally prove experimentally the effectiveness of the proposed approach on a challenging task: keystroke biometric identification over a large set of 78,000 subjects. Our method achieves state-of-the-art accuracy on a comparison performed with the best existing approaches.}
}
@article{WANG2022108230,
title = {Deep neighbor-aware embedding for node clustering in attributed graphs},
journal = {Pattern Recognition},
volume = {122},
pages = {108230},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108230},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004118},
author = {Chun Wang and Shirui Pan and Celina P. Yu and Ruiqi Hu and Guodong Long and Chengqi Zhang},
keywords = {Attributed graph, Node clustering, Graph attention network, Graph convolutional network, Network representation},
abstract = {Node clustering aims to partition the vertices in a graph into multiple groups or communities. Existing studies have mostly focused on developing deep learning approaches to learn a latent representation of nodes, based on which simple clustering methods like k-means are applied. These two-step frameworks for node clustering are difficult to manipulate and usually lead to suboptimal performance, mainly because the graph embedding is not goal-directed, i.e., designed for the specific clustering task. In this paper, we propose a clustering-directed deep learning approach, Deep Neighbor-aware Embedded Node Clustering (DNENC for short) for clustering graph data. Our method focuses on attributed graphs to sufficiently explore the two sides of information in graphs. It encodes the topological structure and node content in a graph into a compact representation via a neighbor-aware graph autoencoder, which progressively absorbs information from neighbors via a convolutional or attentional encoder. Multiple neighbor-aware encoders are stacked to build a deep architecture followed by an inner-product decoder for reconstructing the graph structure. Furthermore, soft labels are generated to supervise a self-training process, which iteratively refines the node clustering results. The self-training process is jointly learned and optimized with the graph embedding in a unified framework, to benefit both components mutually. Experimental results compared with state-of-the-art algorithms demonstrate the good performance of our framework.}
}
@article{CHRISTMAS2022108340,
title = {Non-stationary, online variational Bayesian learning, with circular variables},
journal = {Pattern Recognition},
volume = {122},
pages = {108340},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108340},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005203},
author = {J. Christmas},
keywords = {Online learning/processing, Variational methods, Bayes procedures},
abstract = {We introduce an online variational Bayesian model for tracking changes in a non-stationary, multivariate, temporal signal, using as an example the changing frequency and amplitude of a noisy sinusoidal signal over time. The model incorporates each observation as it arrives and then discards it, and places priors over precision hyperparameters to ensure that (i) the posterior probability distributions do not become overly tight, which would impede its ability to recognise and track changes, and (ii) no values in the system are able to continuously increase and hence exceed the numerical representation of the programming language. It is thus able to perform truly online processing for an infinitely long set of observations. Only a single round of updates in the variational Bayesian scheme per observation is used, and the complexity of the algorithm is constant in time. The proposed method is demonstrated on a large number of synthetic datasets, comparing the results from the full model (with precision hyperparameters as variables with priors) with those from the base model where the precision hyperparameters are fixed values. The full model is also demonstrated on a set of real climate data.}
}
@article{JIANG2022108324,
title = {Two-step domain adaptation for underwater image enhancement},
journal = {Pattern Recognition},
volume = {122},
pages = {108324},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108324},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005045},
author = {Qun Jiang and Yunfeng Zhang and Fangxun Bao and Xiuyang Zhao and Caiming Zhang and Peide Liu},
keywords = {Underwater image enhancement, Transfer learning, Domain adaptation, Cycle-consistent adversarial network},
abstract = {In recent years, underwater image enhancement methods based on deep learning have achieved remarkable results. Since the images obtained in complex underwater scenarios lack a ground truth, these algorithms mainly train models on underwater images synthesized from in-air images. Synthesized underwater images are different from real-world underwater images; this difference leads to the limited generalizability of the training model when enhancing real-world underwater images. In this work, we present an underwater image enhancement method that does not require training on synthetic underwater images and eliminates the dependence on underwater ground-truth images. Specifically, a novel domain adaptation framework for real-world underwater image enhancement inspired by transfer learning is presented; it transfers in-air image dehazing to real-world underwater image enhancement. The experimental results on different real-world underwater scenes indicate that the proposed method produces visually satisfactory results.}
}
@article{YANG2022108348,
title = {Multi-scale spatial-spectral fusion based on multi-input fusion calculation and coordinate attention for hyperspectral image classification},
journal = {Pattern Recognition},
volume = {122},
pages = {108348},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108348},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005288},
author = {Lina Yang and Fengqi Zhang and Patrick Shen-Pei Wang and Xichun Li and Zuqiang Meng},
keywords = {Hyperspectral image(HSI), Multi-scale fusion, Fusion calculation, Coordinate attention, Image patch, 3D convolution},
abstract = {Recently, the deep learning method that integrates image features has gradually become a hot development trend in hyperspectral image classification. However, these studies did not fully consider the fusion of image features, and did not remove the interference to the classification process caused by the difference in the size of the objects. These factors hinder the further improvement of the classification effect. To eliminate these drawbacks, this paper proposes a more effective fusion scheme (MSF-MIF), which realizes the fusion from the perspective of location characteristics and channel characteristics through 3D convolution and spatial feature concatenation. In view of the size discrepancy of the objects to be classified, this method extracts features from several input patches of different scales and uses the novel calculation method proposed to fuse them, which minimizes the interference caused by size differences. In addition, this research also tried to quote the coordinate attention structure for the first time that combines spatial and spectral attention features to further improve the classification performance. Experimental results on three commonly used data sets prove that this framework has achieved a breakthrough in classification accuracy.}
}
@article{ZHONG2022108336,
title = {A cascade reconstruction model with generalization ability evaluation for anomaly detection in videos},
journal = {Pattern Recognition},
volume = {122},
pages = {108336},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108336},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005161},
author = {Yuanhong Zhong and Xia Chen and Jinyang Jiang and Fan Ren},
keywords = {Anomaly detection, pixel reconstruction, optical flow prediction, generalization ability evaluation},
abstract = {Anomaly detection plays an important role in surveillance video since it maintains public safety efficiently with low cost. In current works, anomaly detection methods based on reconstruction with deep learning has been extensively studied for the powerful representation capacity. These methods use convolutional neural networks to learn model for describing normality at training and detect anomalies according to reconstruction error at testing. However, excessive representation capacity of neural networks will also bring disadvantages to anomaly detection when it is powerful enough to reconstruct abnormal information. For this reason, we proposed two solutions; firstly, a cascade model which conducts pixel reconstruction followed by optical flow prediction is designed. The conversion from frame to optical flow learns the correlation between object appearance and motion, while pixel reconstruction enlarges the optical flow prediction error to conduct effective anomaly detection. Secondly, the generalization ability evaluation based on pseudo-anomaly is proposed, which is used to evaluate the ability of model to represent anomaly, thus selecting an optimal model for anomaly detection. The selected model achieves AUC 88.9% on Avenue, 82.6% on Ped1, 97.7% on Ped2, and 70.7% on ShanghaiTech datasets. Extensive ablation experiments have verified the effectiveness of our method. Code will be released at https://github.com/Xia-Chen/Cascade_Reconstruction.}
}
@article{GHOSH2022108279,
title = {A black-box adversarial attack strategy with adjustable sparsity and generalizability for deep image classifiers},
journal = {Pattern Recognition},
volume = {122},
pages = {108279},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108279},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004593},
author = {Arka Ghosh and Sankha Subhra Mullick and Shounak Datta and Swagatam Das and Asit Kr. Das and Rammohan Mallipeddi},
keywords = {Adversarial attack, Black-box attack, Convolutional image classifier, Differential evolution, Sparse universal attack},
abstract = {Constructing adversarial perturbations for deep neural networks is an important direction of research. Crafting image-dependent adversarial perturbations using white-box feedback has hitherto been the norm for such adversarial attacks. However, black-box attacks are much more practical for real-world applications. Universal perturbations applicable across multiple images are gaining popularity due to their innate generalizability. There have also been efforts to restrict the perturbations to a few pixels in the image. This helps to retain visual similarity with the original images making such attacks hard to detect. This paper marks an important step that combines all these directions of research. We propose the DEceit algorithm for constructing effective universal pixel-restricted perturbations using only black-box feedback from the target network. We conduct empirical investigations using the ImageNet validation set on the state-of-the-art deep neural classifiers by varying the number of pixels to be perturbed from a meager 10 pixels to as high as all pixels in the image. We find that perturbing only about 10% of the pixels in an image using DEceit achieves a commendable and highly transferable Fooling Rate while retaining the visual quality. We further demonstrate that DEceit can be successfully applied to image-dependent attacks as well. In both sets of experiments, we outperform several state-of-the-art methods.}
}
@article{KUMAR2022108255,
title = {SARS-Net: COVID-19 detection from chest x-rays by combining graph convolutional network and convolutional neural network},
journal = {Pattern Recognition},
volume = {122},
pages = {108255},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108255},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004350},
author = {Aayush Kumar and Ayush R Tripathi and Suresh Chandra Satapathy and Yu-Dong Zhang},
keywords = {Convolutional neural network, Graph convolutional network, COVID-19 detection, Chest X-ray, Deep learning},
abstract = {COVID-19 has emerged as one of the deadliest pandemics that has ever crept on humanity. Screening tests are currently the most reliable and accurate steps in detecting severe acute respiratory syndrome coronavirus in a patient, and the most used is RT-PCR testing. Various researchers and early studies implied that visual indicators (abnormalities) in a patient's Chest X-Ray (CXR) or computed tomography (CT) imaging were a valuable characteristic of a COVID-19 patient that can be leveraged to find out virus in a vast population. Motivated by various contributions to open-source community to tackle COVID-19 pandemic, we introduce SARS-Net, a CADx system combining Graph Convolutional Networks and Convolutional Neural Networks for detecting abnormalities in a patient's CXR images for presence of COVID-19 infection in a patient. In this paper, we introduce and evaluate the performance of a custom-made deep learning architecture SARS-Net, to classify and detect the Chest X-ray images for COVID-19 diagnosis. Quantitative analysis shows that the proposed model achieves more accuracy than previously mentioned state-of-the-art methods. It was found that our proposed model achieved an accuracy of 97.60% and a sensitivity of 92.90% on the validation set.}
}
@article{BAI2022108166,
title = {Learning-based resilience guarantee for multi-UAV collaborative QoS management},
journal = {Pattern Recognition},
volume = {122},
pages = {108166},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108166},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003538},
author = {Chengchao Bai and Peng Yan and Xiaoqiang Yu and Jifeng Guo},
keywords = {Unmanned business, Communication service, Multi-UAV, Deep reinforcement learning, QoS-aware, System resilience},
abstract = {Unmanned and intelligent technologies are the future development trend in the business field. It is of great significance for the connotation analysis and application characterization of massive interactive data. Particularly, during major epidemics or disasters, how to provide business services safely and securely is crucial. Specifically, providing users with resilient and guaranteed communication services is a challenging business task when the communication facilities are damaged. Unmanned aerial vehicles (UAVs), with flexible deployment and high maneuverability, can be used to serve as aerial base stations (BSs) to establish emergency networks. However, it is challenging to control multiple UAVs to provide efficient and fair communication quality of service (QoS) to users due to their limited communication service capabilities. In this paper, we propose a learning-based resilience guarantee framework for multi-UAV collaborative QoS management. We formulate this problem as a partial observable Markov decision process and solve it with proximal policy optimization (PPO), which is a policy-based deep reinforcement learning method. A centralized training and decentralized execution paradigm is used, where the experience collected by all UAVs is used to train the shared control policy. Each UAV takes actions based on the partial environment information it observes. In addition, the design of the reward function considers the average and variance of the communication QoS of all users. Extensive simulations are conducted for performance evaluation. The simulation results indicate that (1) the trained policies can adapt to different scenarios and provide resilient and guaranteed communication QoS to users, (2) increasing the number of UAVs can compensate for the lack of service capabilities of UAVs, (3) when UAVs have local communication service capabilities, the policies trained with PPO have better performance compared with the policies trained with other algorithms.}
}
@article{MANUELVARGAS2022108310,
title = {Unimodal regularisation based on beta distribution for deep ordinal regression},
journal = {Pattern Recognition},
volume = {122},
pages = {108310},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108310},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004908},
author = {Víctor {Manuel Vargas} and Pedro Antonio Gutiérrez and César Hervás-Martínez},
keywords = {Ordinal regression, Unimodal distribution, Convolutional network, Beta distribution, Stick-breaking},
abstract = {Currently, the use of deep learning for solving ordinal classification problems, where categories follow a natural order, has not received much attention. In this paper, we propose an unimodal regularisation based on the beta distribution applied to the cross-entropy loss. This regularisation encourages the distribution of the labels to be a soft unimodal distribution, more appropriate for ordinal problems. Given that the beta distribution has two parameters that must be adjusted, a method to automatically determine them is proposed. The regularised loss function is used to train a deep neural network model with an ordinal scheme in the output layer. The results obtained are statistically analysed and show that the combination of these methods increases the performance in ordinal problems. Moreover, the proposed beta distribution performs better than other distributions proposed in previous works, achieving also a reduced computational cost.}
}
@article{SHI2022108351,
title = {Loss functions for pose guided person image generation},
journal = {Pattern Recognition},
volume = {122},
pages = {108351},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108351},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005318},
author = {Haoyue Shi and Le Wang and Nanning Zheng and Gang Hua and Wei Tang},
keywords = {Person image generation, Loss function analysis, Structure similarity index},
abstract = {Pose guided person image generation aims to transform a source person image to a target pose. It is an ill-posed problem as we often need to generate pixels that are invisible in the source image. Recent works focus on designing new architectures of deep neural networks and show promising performance. However, they simply adopt loss functions widely used in generic image generation tasks, e.g., adversarial loss, L1-norm loss, perceptual loss, and style loss, which fail to consider the unique structural patterns of a person. In addition, it remains unclear how each individual loss and their combinations impact the generated person images. The goal of this paper is to have a comprehensive study of loss functions for pose guided person image generation. After revisiting these generic loss functions, we consider the structural similarity (SSIM) index as a loss function since it is widely used as the evaluation metric and can capture the perceptual quality of generated images. In addition, motivated by the observation that a person can be divided into part regions with homogeneous pixel values or texture, we extend the SSIM loss into a novel Part-based SSIM (PSSIM) loss to explicitly account for the articulated body structure. A new PSSIM metric is then proposed naturally to access the quality of generated person images. In order to have a deep investigation of loss functions, we conduct extensive experiments including single-loss analysis, multi-loss combination analysis, optimal loss combination search, and comparison with state-of-the-art methods. Both quantitative and qualitative results indicate that (1) using different loss functions significantly impacts the generated person images, (2) the combination of adversarial loss, perceptual loss, and PSSIM loss is the optimal choice for person image generation, and (3) the proposed PSSIM loss is complementary to prior losses and helps improve the performance of state-of-the art methods. We have made the source code publicly available at https://github.com/shyern/Pose-Transfer-pSSIM.git.}
}
@article{YANG2022108241,
title = {Data synthesis method preserving correlation of features},
journal = {Pattern Recognition},
volume = {122},
pages = {108241},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108241},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004222},
author = {Wonseok Yang and Woochul Nam},
keywords = {Data synthesis, Correlation, Artificial dataset, Random noise},
abstract = {Abundant data are essential for improving the performance of machine learning algorithms. Thus, if only limited data are available, data synthesis can be used to enlarge datasets. Data synthesis methods based on the covariance matrix are useful because of their fast data synthesis capabilities. However, artificial datasets generated via classical techniques show statistical discrepancies when compared to original datasets. To address this problem, we developed a new data synthesis method that preserves the correlation (between features) observed in the original dataset. This preservation was realized by considering not only the correlation but also the random noises used in data synthesis process. This method was applied to various biosignals (i.e., electrocortiography, electromyogram, and electrocardiogram), wherein data points are insufficient. Several classifiers (i.e., convolutional neural network, support vector machine, and k-nearest neighbor) were used to verify that the classification accuracy can be improved by the proposed data synthesis method.}
}
@article{CINA2022108306,
title = {A black-box adversarial attack for poisoning clustering},
journal = {Pattern Recognition},
volume = {122},
pages = {108306},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108306},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004866},
author = {Antonio Emanuele Cinà and Alessandro Torcinovich and Marcello Pelillo},
keywords = {Adversarial learning, Unsupervised learning, Clustering, Robustness evaluation, Machine learning security},
abstract = {Clustering algorithms play a fundamental role as tools in decision-making and sensible automation processes. Due to the widespread use of these applications, a robustness analysis of this family of algorithms against adversarial noise has become imperative. To the best of our knowledge, however, only a few works have currently addressed this problem. In an attempt to fill this gap, in this work, we propose a black-box adversarial attack for crafting adversarial samples to test the robustness of clustering algorithms. We formulate the problem as a constrained minimization program, general in its structure and customizable by the attacker according to her capability constraints. We do not assume any information about the internal structure of the victim clustering algorithm, and we allow the attacker to query it as a service only. In the absence of any derivative information, we perform the optimization with a custom approach inspired by the Abstract Genetic Algorithm (AGA). In the experimental part, we demonstrate the sensibility of different single and ensemble clustering algorithms against our crafted adversarial samples on different scenarios. Furthermore, we perform a comparison of our algorithm with a state-of-the-art approach showing that we are able to reach or even outperform its performance. Finally, to highlight the general nature of the generated noise, we show that our attacks are transferable even against supervised algorithms such as SVMs, random forests and neural networks.}
}
@article{YENDURI2022108282,
title = {Fine-grained action recognition using dynamic kernels},
journal = {Pattern Recognition},
volume = {122},
pages = {108282},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108282},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004623},
author = {Sravani Yenduri and Nazil Perveen and Vishnu Chalavadi and Krishna Mohan C},
keywords = {Fine-grained action recognition, Spatio-temporal features, Gaussian mixture model, Dynamic kernels},
abstract = {Fine-grained action recognition involves comparison of similar actions of variable-length size consisting of subtle interactions between human and specific objects. Hence, we propose a dynamic kernel-based approach to handle the variable-length patterns for effective recognition of fine-grained actions. Initially, we extract local spatio-temporal features for each video to capture appearance and motion information effectively. An action-independent Gaussian mixture model (AIGMM) is trained on the extracted features of all fine-grained actions to analyze spatio-temporal information and preserve the local similarities among fine-grained actions. Then, the statistics of AIGMM, namely, mean, covariance, and posteriors are used to build the kernels for finding the similarity between any two fine-grained actions by mapping statistics to kernel feature space. We demonstrate the effectiveness of proposed approach using three dynamic kernels i.e., GMM mean interval kernel, supervector kernel, intermediate matching kernel on four varieties of fine-grained action datasets, namely, MERL, JIGSAWS, KSCGR, and MPII cooking2}
}
@article{LI2022108203,
title = {Biological eagle eye-based method for change detection in water scenes},
journal = {Pattern Recognition},
volume = {122},
pages = {108203},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108203},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003654},
author = {Xuan Li and Haibin Duan and Jingchun Li and Yimin Deng and Fei-Yue Wang},
keywords = {Change detection, Eagle eye, Synthetic boat sequence, Synthetic dataset, Unmanned aerial vehicle},
abstract = {Change detection (CD) is an important vision task for autonomous landing of unmanned aerial vehicles (UAV) on water. High-density photoreceptors and lateral inhibition mechanisms have inspired a novel biologic computational method based on structure and properties in eagle eyes as proposed for change detection. We call this method “STabCD,” which ensures spatiotemporal distribution consistency to achieve foreground acquisition, noise reduction, and background adaptability. Therefore, our proposed model responds strongly to object information and suppresses noise and wave textures. Then, we present a cloning method to simulate water scenes and collect a new synthetic dataset (called “Synthetic Boat Sequence”) for UAV vision research. Besides, we utilize synthetic datasets and corresponding real datasets to conduct change detection experiments. The experimental results indicate that: 1) the STabCD model achieves the best results in real or synthetic water landing scenes; and 2) change detection models for UAV can be quantitatively analyzed and tested under challenging synthetic scenarios.}
}
@article{JIN2022108323,
title = {Feature flow: In-network feature flow estimation for video object detection},
journal = {Pattern Recognition},
volume = {122},
pages = {108323},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108323},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005033},
author = {Ruibing Jin and Guosheng Lin and Changyun Wen and Jianliang Wang and Fayao Liu},
keywords = {Video object detection, Feature flow, Object detection, Video analysis, Deep convolutional neural network (DCNN)},
abstract = {Optical flow, which expresses pixel displacement, is widely used in many computer vision tasks to provide pixel-level motion information. However, with the remarkable progress of the convolutional neural network, recent state-of-the-art approaches are proposed to solve problems directly on feature-level. Since the displacement of feature vector is not consistent with the pixel displacement, a common approach is to forward optical flow to a neural network and fine-tune this network on the task dataset. With this method, they expect the fine-tuned network to produce tensors encoding feature-level motion information. In this paper, we rethink about this de facto paradigm and analyze its drawbacks in the video object detection task. To mitigate these issues, we propose a novel network (IFF-Net) with an In-network Feature Flow estimation module (IFF module) for video object detection. Without resorting to pre-training on any additional dataset, our IFF module is able to directly produce feature flow which indicates the feature displacement. Our IFF module consists of a shallow module, which shares the features with the detection branches. This compact design enables our IFF-Net to accurately detect objects, while maintaining a fast inference speed. Furthermore, we propose a transformation residual loss (TRL) based on self-supervision, which further improves the performance of our IFF-Net. Our IFF-Net outperforms existing methods and achieves new state-of-the-art performance on ImageNet VID.}
}
@article{WANG2022108347,
title = {Coarse-to-fine-grained method for image splicing region detection},
journal = {Pattern Recognition},
volume = {122},
pages = {108347},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108347},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005276},
author = {Xiaofeng Wang and Yan Wang and Jinjin Lei and Bin Li and Qin Wang and Jianru Xue},
keywords = {Image splicing detection, CFA interpolation algorithm, Forensics features, Texture strength features, Edges smoothing},
abstract = {In this study, we aim to improve the accuracy of image splicing detection. We propose a progressive image splicing detection method that can detect the position and shape of spliced region. Because image splicing is likely to destroy or change the consistent correlation pattern introduced by color filter array (CFA) interpolation process, we first used a covariance matrix to reconstruct the R, G and B channels of image and utilized the inconsistencies of the CFA interpolation pattern to extract forensics feature. Then, these forensics features were used to perform coarse-grained detection, and texture strength features were used to perform fine-grained detection. Finally, an edge smoothing method was applied to realize precise localization. As compared to the state-of-the-art CFA-based image splicing detection methods, the proposed method has a high-level detection accuracy and strong robustness against content-preserving manipulations and JPEG compression.}
}
@article{HUANG2022108359,
title = {Discriminative unimodal feature selection and fusion for RGB-D salient object detection},
journal = {Pattern Recognition},
volume = {122},
pages = {108359},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108359},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005392},
author = {Nianchang Huang and Yongjiang Luo and Qiang Zhang and Jungong Han},
keywords = {RGB-D salient object detection, Discriminative unimodal feature selection, Semantic information, Multi-scale cross-modal feature fusion},
abstract = {Most existing RGB-D salient object detectors make use of the complementary information of RGB-D images to overcome the challenging scenarios, e.g., low contrast, clutter backgrounds. However, these models generally neglect the fact that one of the input images may be poor in quality. This will adversely affect the discriminative ability of cross-modal features when the two channels are fused directly. To address this issue, a novel end-to-end RGB-D salient object detection model is proposed in this paper. At the core of our model is a Semantic-Guided Modality-Weight Map Generation (SG-MWMG) sub-network, producing modality-weight maps to indicate which regions on both modalities are high-quality regions, given input RGB-D images and the guidance of their semantic information. Based on it, a Bi-directional Multi-scale Cross-modal Feature Fusion (Bi-MCFF) module is presented, where the interactions of the features across different modalities and scales are exploited by using a novel bi-directional structure for better capturing cross-scale and cross-modal complementary information. The experimental results on several benchmark datasets verify the effectiveness and superiority of the proposed method over some state-of-the-art methods.}
}
@article{LIU2022108237,
title = {Zero-shot learning via a specific rank-controlled semantic autoencoder},
journal = {Pattern Recognition},
volume = {122},
pages = {108237},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108237},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004180},
author = {Yang Liu and Xinbo Gao and Jungong Han and Li Liu and Ling Shao},
keywords = {Zero-shot learning, Rank, Domain shift, Autoencoder},
abstract = {Existing embedding zero-shot learning models usually learn a projection function from the visual feature space to the semantic embedding space, e.g. attribute space or word vector space. However, the projection learned based on seen samples may not generalize well to unseen classes, which is known as the projection domain shift problem in ZSL. To address this issue, we propose a method named Low-rank Semantic Autoencoder (LSA) to consider the low-rank structure of seen samples to maintain the sparse feature of reconstruction error, which can further improve zero-shot learning capability. Moreover, to obtain a more robust projection for unseen classes, we propose a Specific Rank-controlled Semantic Autoencoder (SRSA) to accurately control of the projection’s rank. Extensive experiments on six benchmarks demonstrate the superiority of the proposed models over most existing embedding ZSL models under the standard zero-shot setting and the more realistic generalized zero-shot setting.}
}
@article{LIU2022108293,
title = {Who is closer: A computational method for domain gap evaluation},
journal = {Pattern Recognition},
volume = {122},
pages = {108293},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108293},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004738},
author = {Xiaobin Liu and Shiliang Zhang},
keywords = {Domain gap evaluation, CNN, Domain adaptive learning},
abstract = {Domain gaps between different datasets limit the generalization ability of CNN models. Precise evaluation on the domain gap has potential to assist the promotion of CNN generalization ability. This paper proposes a computational framework to evaluate gaps between different domains, e.g., judging which one of source domains is closer to the target domain. Our model is based on the observation that, given a well-trained classifier on the source domain, the entropy of its classification scores of the output layer can be used as an indicator of the domain gap. For instance, smaller domain gap generally corresponds to smaller entropy of classification scores. To further boost the discriminative power in distinguishing domain gaps, a novel training strategy is proposed to supervise the model to produce smaller entropy on one source domain and larger entropy on other source domains. This supervision leads to an efficient and discriminative domain gap evaluation model. Extensive experiments on multiple datasets including faces, vehicles, fashions, and persons, etc. show that our method can reasonably measure domain gaps. We further conduct experiments on domain adaptive person ReID task and our method is adopted to pre-trained model selection, pre-trained model fusion, source dataset fusion, and source dataset selection. As shown in the experiments, our method substantially boosts the ReID accuracy. To the best of our knowledge, this is an original work focusing on computational domain gap evaluation. Our code is available at https://github.com/liu-xb/DomainGapEvaluation.}
}
@article{HE2022108280,
title = {Hyperspectral super-resolution via coupled tensor ring factorization},
journal = {Pattern Recognition},
volume = {122},
pages = {108280},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108280},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100460X},
author = {Wei He and Yong Chen and Naoto Yokoya and Chao Li and Qibin Zhao},
keywords = {Coupled tensor ring decomposition, Super-resolution, Hyperspectral, Multispectral},
abstract = {Hyperspectral super-resolution (HSR) fuses a low-resolution hyperspectral image (HSI) and a high-resolution multispectral image (MSI) to obtain a high-resolution HSI (HR-HSI). In this paper, we propose a new model called coupled tensor ring factorization (CTRF) for HSR. The proposed CTRF approach simultaneously learns the tensor ring core tensors of the HR-HSI from a pair of HSI and MSI. The CTRF model can separately exploit the low-rank property of each class (Section 3.3), which has not been explored in previous coupled tensor models. Meanwhile, the model inherits the simple representation of coupled matrix/canonical polyadic factorization and flexible low-rank exploration of coupled Tucker factorization. We further introduce spectral nuclear norm regularization to explore the global spectral low-rank property. The experiments demonstrated the advantage of the proposed nuclear norm regularized CTRF model compared to previous matrix/tensor and deep learning methods.}
}
@article{PATIL2022108350,
title = {Multi‐frame based adversarial learning approach for video surveillance},
journal = {Pattern Recognition},
volume = {122},
pages = {108350},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108350},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005306},
author = {Prashant W. Patil and Akshay Dudhane and Sachin Chaudhary and Subrahmanyam Murala},
keywords = {Temporal sampling, Multi-scale adversarial learning, Foreground-background segmentation and video surveillance},
abstract = {Foreground-background segmentation (FBS) is one of the prime tasks for automated video-based applications like traffic analysis and surveillance. The different practical scenarios like weather degraded videos, irregular moving objects, dynamic background, etc., make FBS a challenging task. The existing FBS algorithms mainly depend on one of the three different factors, namely (1) complicated training process, (2) additionally trained modules for other applications, or (3) neglect the inter-frame spatio-temporal structural dependencies. In this paper, a novel multi-frame-based adversarial learning network is proposed with multi-scale inception and residual module for FBS. As, FBS is a temporal enlightenment-based problem, a temporal encoding mechanism with decreasing variable intervals is proposed for the input frame selection. The proposed network comprises multi-scale inception and residual connection-based dense modules to learn prominent features of the foreground object(s). Also, feedback of the estimated foreground map of previous frame is utilized to exhibit more temporal consistency. Learning of the network is concentrated in different ways like cross-data, disjoint, and global training-testing for FBS. The qualitative and quantitative experimental analysis of the proposed approach is done on three benchmark datasets for FBS. Experimental analysis on three benchmark datasets proves the significance of the proposed approach as compared to state-of-the-art FBS approaches.}
}
@article{NEOGI2022108236,
title = {Factored Latent-Dynamic Conditional Random Fields for single and multi-label sequence modeling},
journal = {Pattern Recognition},
volume = {122},
pages = {108236},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108236},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004179},
author = {Satyajit Neogi and Justin Dauwels},
keywords = {Conditional Random Fields, Sequence labeling, Multi-task learning, Latent-Dynamic models, Probabilistic graphical models},
abstract = {Conditional Random Fields (CRF) are frequently applied for labeling and segmenting sequence data. Morency et al. (2007) introduced hidden state variables in a labeled CRF structure in order to model the latent dynamics within class labels, thus improving the labeling performance. Such a model is known as Latent-Dynamic CRF (LDCRF). We present Factored LDCRF (FLDCRF), a structure that allows multiple latent dynamics of the class labels to interact with each other. Including such latent-dynamic interactions leads to improved labeling performance on single-label and multi-label sequence modeling experiments across two different datasets, viz., UCI gesture phase data and UCI opportunity data. FLDCRF outperforms all state-of-the-art sequence models, viz., CRF, LDCRF, LSTM, LSTM-CRF, Factorial CRF, Coupled CRF and a multi-label LSTM model across experiments in this paper. In addition, FLDCRF offers easier model selection and is more consistent across validation and test data than LSTM models. FLDCRF is also much faster to train compared to LSTM, even without a GPU. FLDCRF outshines the best LSTM model by ∼4% on a single-label task on the UCI gesture phase data and outperforms LSTM models by ∼2% on average on the multi-label sequence tagging experiment on the UCI opportunity data.}
}
@article{DESHPANDE2022108289,
title = {AI-Based human audio processing for COVID-19: A comprehensive overview},
journal = {Pattern Recognition},
volume = {122},
pages = {108289},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108289},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004696},
author = {Gauri Deshpande and Anton Batliner and Björn W. Schuller},
keywords = {COVID-19, Digital health, Audio processing, Computational paralinguistics},
abstract = {The Coronavirus (COVID-19) pandemic impelled several research efforts, from collecting COVID-19 patients’ data to screening them for virus detection. Some COVID-19 symptoms are related to the functioning of the respiratory system that influences speech production; this suggests research on identifying markers of COVID-19 in speech and other human generated audio signals. In this article, we give an overview of research on human audio signals using ‘Artificial Intelligence’ techniques to screen, diagnose, monitor, and spread the awareness about COVID-19. This overview will be useful for developing automated systems that can help in the context of COVID-19, using non-obtrusive and easy to use bio-signals conveyed in human non-speech and speech audio productions.}
}
@article{GUO2022108334,
title = {Graph Clustering via Variational Graph Embedding},
journal = {Pattern Recognition},
volume = {122},
pages = {108334},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108334},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005148},
author = {Lin Guo and Qun Dai},
keywords = {Graph convolution neural network, Variational graph embedding, Graph clustering, Variational graph auto-encoder},
abstract = {Graph clustering based on embedding aims to divide nodes with higher similarity into several mutually disjoint groups, but it is not a trivial task to maximumly embed the graph structure and node attributes into the low dimensional feature space. Furthermore, most of the current advanced methods of graph nodes clustering adopt the strategy of separating graph embedding technology and clustering algorithm, and ignore the potential relationship between them. Therefore, we propose an innovative end-to-end graph clustering framework with joint strategy to handle the complex problem in a non-Euclidean space. In terms of learning the graph embedding, we propose a new variational graph auto-encoder algorithm based on the Graph Convolution Network (GCN), which takes into account the boosting influence of joint generative model of graph structure and node attributes on the embedding output. On the basis of embedding representation, we implement a self-training mechanism through the construction of auxiliary distribution to further enhance the prediction of node categories, thereby realizing the unsupervised clustering mode. In addition, the loss contribution of each cluster is normalized to prevent large clusters from distorting the embedding space. Extensive experiments on real-world graph datasets validate our design and demonstrate that our algorithm has highly competitive in graph clustering over state-of-the-art methods.}
}
@article{REHMAN2022108305,
title = {Divide well to merge better: A novel clustering algorithm},
journal = {Pattern Recognition},
volume = {122},
pages = {108305},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108305},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004854},
author = {Atiq Ur Rehman and Samir Brahim Belhaouari},
keywords = {Clustering, Data projection, Joint probability density estimation, Non-parametric techniques},
abstract = {In this paper, a novel non-parametric clustering algorithm which is based on the concept of divide-and-merge is proposed. The proposed algorithm is based on two primary phases, after data cleaning: (i) the Division phase and (ii) the Merging phase. In the initial phase of division, the data is divided into an optimized number of small sub-clusters utilizing all the dimensions of the data. In the second phase of merging, the small sub-clusters obtained as a result of division are merged according to an advanced statistical metric to form the actual clusters in the data. The proposed algorithm has the following merits: (i) ability to discover both convex and non-convex shaped clusters, (ii) ability to discover clusters different in densities, (iii) ability to detect and remove outliers/noise in the data (iv) easily tunable or fixed hyperparameters (v) and its usability for high dimensional data. The proposed algorithm is extensively tested on 20 benchmark datasets including both, the synthetic and the real datasets and is found better/competing to the existing state-of-the-art parametric and non-parametric clustering algorithms.}
}
@article{HASANINASAB2022108253,
title = {Efficient COVID-19 testing via contextual model based compressive sensing},
journal = {Pattern Recognition},
volume = {122},
pages = {108253},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108253},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004337},
author = {Mehdi Hasaninasab and Mohammad Khansari},
keywords = {COVID-19, Graph signal model, Group testing, Model-based compressive sensing},
abstract = {The COVID-19 pandemic is threatening billions of people's life all over the world. As of March 6, 2021, covid-19 has confirmed in 115,653,459 people worldwide. It has also a devastating effect on businesses and social activities. Since there is still no definite cure for this disease, extensive testing is the most critical issue to determine the trend of illness, appropriate medical treatment, and make social distancing policies. Besides, testing more people in a shorter time helps to contain the contagion. The PCR-based methods are the most popular tests which take about an hour to make the output result. Obviously, it makes the number of tests highly limited and consequently, hurts the efficiency of pandemic control. In this paper, we propose a new approach to identify affected individuals with a considerably reduced No. of tests. Intuitively, saving time and resources is the main advantage of our approach. We use contextual information to make a graph-based model to be used in model-based compressive sensing (CS). Our proposed model makes the testing with fewer tests required compared to traditional testing methods and even group testing. We embed contextual information such as age, underlying disease, symptoms (i.e. cough, fever, fatigue, loss of consciousness), and social contacts into a graph-based model. This model is used in model-based CS to minimize the required test. We take advantage of Discrete Graph Signal Processing on Graph (DSPG) to generate the model. Our contextual model makes CS more efficient in both the number of samples and the recovery quality. Moreover, it can be applied in the case that group testing is not applicable due to its severe dependency on sparsity. Experimental results show that the overall testing speed (individuals per test ratio) increases more than 15 times compared to the individual testing with the error of less than 5% which is dramatically lower than that of traditional compressive sensing.}
}
@article{CHAI2022108281,
title = {A Novel Quasi-Newton Method for Composite Convex Minimization},
journal = {Pattern Recognition},
volume = {122},
pages = {108281},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108281},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004611},
author = {W.H. Chai and S.S. Ho and H.C. Quek},
keywords = {non-smooth, proximal mapping, quasi-Newton},
abstract = {A fast parallelable Jacobi iteration type optimization method for non-smooth convex composite optimization is presented. Traditional gradient-based techniques cannot solve the problem. Smooth approximate functions are attempted to be used as a replacement of those non-smooth terms without compromising the accuracy. Recently, proximal mapping concept has been introduced into this field. Techniques which utilize proximal average based proximal gradient have been used to solve the problem. The state-of-art methods only utilize first-order information of the smooth approximate function. We integrate both first and second-order techniques to use both first and second-order information to boost the convergence speed. A convergence rate with a lower bound of O(1k2) is achieved by the proposed method and a super-linear convergence is enjoyed when there is proper second-order information. In experiments, the proposed method converges significantly better than the state of art methods which enjoy O(1k) convergence.}
}
@article{WAN2022108358,
title = {Revisiting image captioning via maximum discrepancy competition},
journal = {Pattern Recognition},
volume = {122},
pages = {108358},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108358},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005380},
author = {Boyang Wan and Wenhui Jiang and Yu-Ming Fang and Minwei Zhu and Qin Li and Yang Liu},
keywords = {Image captioning, Model comparison, Attention mechanism},
abstract = {Image captioning is a hot research topic bridging computer vision and natural language processing during the past several decades. It has achieved great progress with the help of large-scale datasets and deep learning techniques. Though the variety of image captioning models (ICMs), the performance of ICMs have got stuck in a bottleneck judging from the publicly published results. Considering the marginal performance gains brought by recent ICMs, we raise the following question: “what about the performances of the recent ICMs achieve on in-the-wild images? To clarify this question, we compare existing ICMs by evaluating their generalization ability. Specifically, we propose a novel method based on maximum discrepancy competition to diagnose existing ICMs. Firstly, we establish a new test set containing only informative images selected by adopting maximum discrepancy competition on the existing ICMs, from an arbitrary large-scale raw image set. Secondly, a small-scale and low-cost subjective annotation experiment is conducted on the new test set. Thirdly, we rank the generalization ability of the existing ICMs by comparing their performances on the new test set. Finally, the keys of different ICMs are demonstrated based on a detailed analysis of experimental results. Our analysis yields several interesting findings, including that 1) Using simultaneously low- and high-level object features may be an effective tool to boost the generalization ability for the Transformer based ICMs. 2) Self-attention mechanism may provide better modelling ability for inter- and intra-modal data than other attention-based mechanisms. 3) Constructing an ICM with a multistage language decoder may be a promising way to improve its performance.}
}
@article{CHU2022108240,
title = {Learning panoptic segmentation through feature discriminability},
journal = {Pattern Recognition},
volume = {122},
pages = {108240},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108240},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004210},
author = {Tao Chu and Wenjie Cai and Qiong Liu},
keywords = {Panoptic segmentation, Feature discriminability, Region refinement},
abstract = {Panoptic segmentation has attracted increasing attention as a joint task of semantic and instance segmentation. However, previous works have not noticed that the different requirements for semantic and instance segmentation can lead to conflict of feature discriminability. Instance segmentation mainly focuses on the central area of each instance in things regions, while semantic segmentation focuses on the whole region of a specific class. To resolve it, we propose: 1) a Dual-FPN framework which separates the shared Feature Pyramid Network (FPN) in previous works to reduce the conflict of receptive field and meet different requirements of the two tasks; 2) a Region Refinement Module which leverages the prediction of semantic segmentation to refine the result of instance segmentation and resolves the conflict between the things regions and the stuff regions. Experimental results on Cityscapes dataset and Mapillary Vistas dataset show that our proposed method can improve the result of both things and stuff and obtain state-of-the-art performance.}
}
@article{DAI2022108249,
title = {Deep image prior based defense against adversarial examples},
journal = {Pattern Recognition},
volume = {122},
pages = {108249},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108249},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004295},
author = {Tao Dai and Yan Feng and Bin Chen and Jian Lu and Shu-Tao Xia},
keywords = {Deep neural network, Adversarial example, Image prior, Defense},
abstract = {Recently, deep neural networks (DNNs) have shown serious vulnerability to adversarial examples with imperceptible perturbation to clean images. To counter this issue, many powerful defensive methods (e.g., ComDefend) focus on rectifying the adversarial examples with well-trained models from a large training dataset (e.g., clean-adversarial image pairs). However, such methods rely heavily on the learned external priors from an external large training dataset, while neglecting the rich image internal priors of the input itself, thus limiting the generalization of the defense models against the adversarial examples with biased image statistics from the external training dataset. Motivated by deep image prior that can capture rich image statistics from a single image, we propose an effective Deep Image Prior Driven Defense (DIPDefend) method against adversarial examples. With a DIP generator to fit the target/adversarial input, we find that our image reconstruction exhibits quite interesting learning preference from a feature learning perspectives, i.e., the early stage primarily learns the robust features resistant to adversarial perturbation, followed by learning non-robust features that are sensitive to adversarial perturbation. Besides, we develop an adaptive stopping strategy that adapts our method to diverse images. In this way, the proposed model obtains a unique defender for each individual adversarial input, thus being robust to various attackers. Experimental results demonstrate the superiority of our method over the state-of-the-art defense methods against white-box and black-box adversarial attacks.}
}
@article{SHI2022108316,
title = {Explainable scale distillation for hyperspectral image classification},
journal = {Pattern Recognition},
volume = {122},
pages = {108316},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108316},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004969},
author = {Cheng Shi and Li Fang and Zhiyong Lv and Minghua Zhao},
keywords = {Hyperspectral image classification, Knowledge distillation, Scale distillation, Explainable scale network},
abstract = {The land-covers within an observed remote sensing scene are usually of different scales; therefore, the ensemble of multi-scale information is a commonly used strategy to achieve more accurate scene interpretation; however, this process suffers from being time-consuming. In terms of this issue, this paper proposes a scale distillation network to explore the possibility that single-scale classification network can achieve the same (or even better) classification performance compared with multi-scale one. The proposed scale distillation network consists of a cumbersome multi-scale teacher network and a lightweight single-scale student network. The former is trained for multi-scale information learning, and the latter improves the classification accuracy by accepting the knowledge from the multi-scale teacher network and its true label. The experimental results show the advantages of scale distillation on hyperspectral image classification. The single-scale student network can even achieve higher evaluation accuracy than the multi-scale teacher network. In addition, a faithful explainable scale network is designed to visually explain the trained scale distillation network. The traditional deep neural network is a black-box and lacks interpretability. The explanation of the trained network can explore more hidden information from the predictions. We visually explain the prediction results of scale distillation network, and the results show that the explainable scale network can more precisely analyze the relationship between the learned scale features and the land-cover categories. Moreover, the possible application of the explainable scale network on classification is further discussed in this study.}
}
@article{ZHANG2022108292,
title = {Generalizable model-agnostic semantic segmentation via target-specific normalization},
journal = {Pattern Recognition},
volume = {122},
pages = {108292},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108292},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004726},
author = {Jian Zhang and Lei Qi and Yinghuan Shi and Yang Gao},
keywords = {Domain generalization, Semantic segmentation, Model-agnostic learning, Target-specific normalization},
abstract = {Semantic segmentation in a supervised learning manner has achieved significant progress in recent years. However, its performance usually drops dramatically due to the data-distribution discrepancy between seen and unseen domains when we directly deploy the trained model to segment the images of unseen (or new coming) domains. To this end, we propose a novel domain generalization framework for the generalizable semantic segmentation task, which enhances the generalization ability of the model from two different views, including the training paradigm and the test strategy. Concretely, we exploit the model-agnostic learning to simulate the domain shift problem, which deals with the domain generalization from the training scheme perspective. Besides, considering the data-distribution discrepancy between seen source and unseen target domains, we develop the target-specific normalization scheme to enhance the generalization ability. Furthermore, when images come one by one in the test stage, we design the image-based memory bank (Image Bank in short) with style-based selection policy to select similar images to obtain more accurate statistics of normalization. Extensive experiments highlight that the proposed method produces state-of-the-art performance for the domain generalization of semantic segmentation on multiple benchmark segmentation datasets, i.e., Cityscapes, Mapillary.}
}
@article{ELKHAYATI2022108288,
title = {Segmentation of Handwritten Arabic Graphemes Using a Directed Convolutional Neural Network and Mathematical Morphology Operations},
journal = {Pattern Recognition},
volume = {122},
pages = {108288},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108288},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004684},
author = {Mohsine Elkhayati and Youssfi Elkettani and Mohammed Mourchid},
keywords = {handwritten Arabic graphemes segmentation, directed convolutional neural network, mathematical morphology, over-traces, partial dilation, global erosion},
abstract = {ABSTRACT
Due to the nature of Arabic handwriting, segmenting words into characters/graphemes is the most difficult and critical task of the recognition system. The present paper proposes an approach to segment handwritten Arabic words into graphemes based on a directed Convolutional Neural Network (CNN) and Mathematical Morphology Operations (MMO). Arabic script is cursive, which means that almost all graphemes are connected via horizontal links; therefore, a technique to remove links will facilitate the segmentation of graphemes. In general, an MMO such as erosion seems suitable for getting the job done, but since Arabic handwriting is difficult, MMOs cause information loss and suffer from many issues such as diacritics and over-traces, which lead to over/under/bad segmentations. To overcome limitations, the present paper addresses these issues in the following order: the over-traces issue is addressed for the first time in the literature; a robust algorithm for diacritics extraction is provided; and finally, the main segmentation algorithm adopts a strategy based on a Partial Dilation (PD)-Global Erosion (GE) technique to combat the information loss issue. The PD phase amplifies important regions, while GE eliminates links between graphemes. The complementarity between PD and GE facilitates the extraction of graphemes and creates resistance against information loss. To properly tackle these difficult problems, this article exploits the robustness of CNNs, so a new directed CNN model is suggested. The idea is to draw the model's attention to certain targeted features, which are selected according to the nature of the problem addressed. The proposed directed CNN is used in all phases of the segmentation process. The experimental results are very encouraging and show that the proposed directed CNN model outperformed basic CNN in many experiments. The results also reveal that the followed strategy improved the ability of MMOs to perform segmentation and to compete with other approaches in this research area.}
}
@article{BAYKAL2022108244,
title = {Exploring DeshuffleGANs in Self-Supervised Generative Adversarial Networks},
journal = {Pattern Recognition},
volume = {122},
pages = {108244},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108244},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004167},
author = {Gulcin Baykal and Furkan Ozcelik and Gozde Unal},
keywords = {Self-Supervised generative adversarial networks, Generative adversarial networks, Self-supervised learning, DeshuffleGANs, Deshuffling},
abstract = {Generative Adversarial Networks (GANs) have become the most used networks towards solving the problem of image generation. Self-supervised GANs are later proposed to avoid the catastrophic forgetting of the discriminator and to improve the image generation quality without needing the class labels. However, the generalizability of the self-supervision tasks on different GAN architectures is not studied before. To that end, we extensively analyze the contribution of a previously proposed self-supervision task, deshuffling of the DeshuffleGANs in the generalizability context. We assign the deshuffling task to two different GAN discriminators and study the effects of the task on both architectures. We extend the evaluations compared to the previously proposed DeshuffleGANs on various datasets. We show that the DeshuffleGAN obtains the best FID results for several datasets compared to the other self-supervised GANs. Furthermore, we compare the deshuffling with the rotation prediction that is firstly deployed to the GAN training and demonstrate that its contribution exceeds the rotation prediction. We design the conditional DeshuffleGAN called cDeshuffleGAN to evaluate the quality of the learnt representations. Lastly, we show the contribution of the self-supervision tasks to the GAN training on the loss landscape and present that the effects of these tasks may not be cooperative to the adversarial training in some settings. Our code can be found at https://github.com/gulcinbaykal/DeshuffleGAN.}
}
@article{MOHAMED2022108361,
title = {Face mask recognition from audio: The MASC database and an overview on the mask challenge},
journal = {Pattern Recognition},
volume = {122},
pages = {108361},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108361},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005410},
author = {Mostafa M. Mohamed and Mina A. Nessiem and Anton Batliner and Christian Bergler and Simone Hantke and Maximilian Schmitt and Alice Baird and Adria Mallol-Ragolta and Vincent Karas and Shahin Amiriparian and Björn W. Schuller},
keywords = {COVID-19, Deep learning, Masks, Voice biometrics, Acoustic modelling},
abstract = {The sudden outbreak of COVID-19 has resulted in tough challenges for the field of biometrics due to its spread via physical contact, and the regulations of wearing face masks. Given these constraints, voice biometrics can offer a suitable contact-less biometric solution; they can benefit from models that classify whether a speaker is wearing a mask or not. This article reviews the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020 COMputational PARalinguistics challengE (ComParE), which focused on the following classification task: Given an audio chunk of a speaker, classify whether the speaker is wearing a mask or not. First, we report the collection of the Mask Augsburg Speech Corpus (MASC) and the baseline approaches used to solve the problem, achieving a performance of 71.8% Unweighted Average Recall (UAR). We then summarise the methodologies explored in the submitted and accepted papers that mainly used two common patterns: (i) phonetic-based audio features, or (ii) spectrogram representations of audio combined with Convolutional Neural Networks (CNNs) typically used in image processing. Most approaches enhance their models by adapting ensembles of different models and attempting to increase the size of the training data using various techniques. We review and discuss the results of the participants of this sub-challenge, where the winner scored a UAR of 80.1%. Moreover, we present the results of fusing the approaches, leading to a UAR of 82.6%. Finally, we present a smartphone app that can be used as a proof of concept demonstration to detect in real-time whether users are wearing a face mask; we also benchmark the run-time of the best models.}
}
@article{KOOK2022108263,
title = {Deep and interpretable regression models for ordinal outcomes},
journal = {Pattern Recognition},
volume = {122},
pages = {108263},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108263},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100443X},
author = {Lucas Kook and Lisa Herzog and Torsten Hothorn and Oliver Dürr and Beate Sick},
keywords = {Deep learning, Interpretability, Distributional regression, Ordinal regression, Transformation models},
abstract = {Outcomes with a natural order commonly occur in prediction problems and often the available input data are a mixture of complex data like images and tabular predictors. Deep Learning (DL) models are state-of-the-art for image classification tasks but frequently treat ordinal outcomes as unordered and lack interpretability. In contrast, classical ordinal regression models consider the outcome’s order and yield interpretable predictor effects but are limited to tabular data. We present ordinal neural network transformation models (ontrams), which unite DL with classical ordinal regression approaches. ontrams are a special case of transformation models and trade off flexibility and interpretability by additively decomposing the transformation function into terms for image and tabular data using jointly trained neural networks. The performance of the most flexible ontram is by definition equivalent to a standard multi-class DL model trained with cross-entropy while being faster in training when facing ordinal outcomes. Lastly, we discuss how to interpret model components for both tabular and image data on two publicly available datasets.}
}
@article{ZHAI2022108333,
title = {An effective deep network using target vector update modules for image restoration},
journal = {Pattern Recognition},
volume = {122},
pages = {108333},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108333},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005136},
author = {Sen Zhai and Chao Ren and Zhengyong Wang and Xiaohai He and Linbo Qing},
keywords = {Image restoration, Plug and play method, Convolutional neural network framework, Transformation domain, Target vector update module},
abstract = {Image restoration (IR) has been widely used in many computer vision applications. The model-based IR methods have clear theoretical bases. However, numerous hyper-parameters need to be set empirically, which is often challenging and time-consuming. Because of the powerful nonlinear fitting ability, deep convolutional neural networks (CNNs) have been widely used in IR tasks in recent years. However, it is challenging to design new network architecture to further significantly improve the IR performance. Inspired by the plug and play (P&P) methods, we first decouple the original IR problem into two subproblems with the variable splitting technique. Then, derived from the model-based methods, a novel deep CNN framework in the transformation domain is proposed to mimic the optimization process of the two subproblems. The proposed framework is driven effectively by the target vector update (TVU) module. Extensive experiments demonstrate the effectiveness of our proposed method over other state-of-the-art IR methods.}
}
@article{YANG2022108357,
title = {Continuous conditional random field convolution for point cloud segmentation},
journal = {Pattern Recognition},
volume = {122},
pages = {108357},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108357},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005379},
author = {Fei Yang and Franck Davoine and Huan Wang and Zhong Jin},
keywords = {Point cloud segmentation, Conditional random fields, Message passing, Graph convolution, Mean-field approximation},
abstract = {Point cloud segmentation is the foundation of 3D environmental perception for modern intelligent systems. To solve this problem and image segmentation, conditional random fields (CRFs) are usually formulated as discrete models in label space to encourage label consistency, which is actually a kind of postprocessing. In this paper, we reconsider the CRF in feature space for point cloud segmentation because it can capture the structure of features well to improve the representation ability of features rather than simply smoothing. Therefore, we first model the point cloud features with a continuous quadratic energy model and formulate its solution process as a message-passing graph convolution, by which it can be easily integrated into a deep network. We theoretically demonstrate that the message passing in the graph convolution is equivalent to the mean-field approximation of a continuous CRF model. Furthermore, we build an encoder-decoder network based on the proposed continuous CRF graph convolution (CRFConv), in which the CRFConv embedded in the decoding layers can restore the details of high-level features that were lost in the encoding stage to enhance the location ability of the network, thereby benefiting segmentation. Analogous to the CRFConv, we show that the classical discrete CRF can also work collaboratively with the proposed network via another graph convolution to further improve the segmentation results. Experiments on various point cloud benchmarks demonstrate the effectiveness and robustness of the proposed method. Compared with the state-of-the-art methods, the proposed method can also achieve competitive segmentation performance.}
}
@article{HU2022108304,
title = {Unsupervised descriptor selection based meta-learning networks for few-shot classification},
journal = {Pattern Recognition},
volume = {122},
pages = {108304},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108304},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004842},
author = {Zhengping Hu and Zijun Li and Xueyu Wang and Saiyue Zheng},
keywords = {Meta-learning, Few-shot classification, Unsupervised localization, Descriptor selection},
abstract = {Meta-learning aims to train a classifier on collections of tasks, such that it can recognize new classes given few samples from each. However, current approaches encounter overfitting and poor generalization since the internal representation learning is obstructed by backgrounds and noises in limited samples. To alleviate those issues, we propose the Unsupervised Descriptor Selection (UDS) to tackle few-shot learning tasks. Specifically, a descriptor selection module is proposed to localize and select semantic meaningful regions in feature maps without supervision. The selected features are then mapped into novel vectors by a task-related aggregation module to enhance internal representations. With a simple network structure, UDS makes adaptation between tasks more efficient, and improves the performance in few-shot learning. Extensive experiments with various backbones are conducted on Caltech-UCSD Bird and miniImageNet, indicate that UDS achieves the comparable performance to state-of-the-art methods, and improves the performance of prior meta-learning methods.}
}
@article{RAHMAN2022108345,
title = {Adaptive Decision Forest: An incremental machine learning framework},
journal = {Pattern Recognition},
volume = {122},
pages = {108345},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108345},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005252},
author = {Md Geaur Rahman and Md Zahidul Islam},
keywords = {Incremental learning, Decision forest algorithm, Concept drift, Big data, Online learning},
abstract = {In this study, we present an incremental machine learning framework called Adaptive Decision Forest (ADF), which produces a decision forest to classify new records. Based on our two novel theorems, we introduce a new splitting strategy called iSAT, which allows ADF to classify new records even if they are associated with previously unseen classes. ADF is capable of identifying and handling concept drift; it, however, does not forget previously gained knowledge. Moreover, ADF is capable of handling big data if the data can be divided into batches. We evaluate ADF on nine publicly available natural datasets and one synthetic dataset, and compare the performance of ADF against the performance of eight state-of-the-art techniques. We also examine the effectiveness of ADF in some challenging situations. Our experimental results, including statistical sign test and Nemenyi test analyses, indicate a clear superiority of the proposed framework over the state-of-the-art techniques.}
}
@article{FU2022108264,
title = {Deep momentum uncertainty hashing},
journal = {Pattern Recognition},
volume = {122},
pages = {108264},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108264},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004441},
author = {Chaoyou Fu and Guoli Wang and Xiang Wu and Qian Zhang and Ran He},
keywords = {Combinatorial optimization, Deep hashing, Uncertainty},
abstract = {Combinatorial optimization (CO) has been a hot research topic because of its theoretic and practical importance. As a classic CO problem, deep hashing aims to find an optimal code for each data from finite discrete possibilities, while the discrete nature brings a big challenge to the optimization process. Previous methods usually mitigate this challenge by binary approximation, substituting binary codes for real-values via activation functions or regularizations. However, such approximation leads to uncertainty between real-values and binary ones, degrading retrieval performance. In this paper, we propose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly estimates the uncertainty during training and leverages the uncertainty information to guide the approximation process. Specifically, we model bit-level uncertainty via measuring the discrepancy between the output of a hashing network and that of a momentum-updated network. The discrepancy of each bit indicates the uncertainty of the hashing network to the approximate output of that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can be regarded as image-level uncertainty. It embodies the uncertainty of the hashing network to the corresponding input image. The hashing bit and image with higher uncertainty are paid more attention during optimization. To the best of our knowledge, this is the first work to study the uncertainty in hashing bits. Extensive experiments are conducted on four datasets to verify the superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a million-scale dataset Clothing1M. Our method achieves the best performance on all of the datasets and surpasses existing state-of-the-art methods by a large margin.}
}
@article{YIN2022108209,
title = {Graph-based stock correlation and prediction for high-frequency trading systems},
journal = {Pattern Recognition},
volume = {122},
pages = {108209},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108209},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003903},
author = {Tao Yin and Chenzhengyi Liu and Fangyu Ding and Ziming Feng and Bo Yuan and Ning Zhang},
keywords = {High-frequency trading, Graph attention long short-term memory (GALSTM), Hawkes processes, Portfolio management},
abstract = {In this paper, we have implemented a high-frequency quantitative system that can obtain stable returns for the Chinese A-share market, which has been running for more than 3 months (from March 27, 2020 to June 30, 2020) with the expected results. A number of rules and barriers exist in the Chinese A-share market such as trading restrictions and high fees, as well as scarce and expensive hedging tools. It is difficult to achieve stable absolute returns in such a market. Stock correlation analysis and price prediction play an important role to achieve any profitable trading. The portfolio management and subsequent trading decisions highly depend on the results of stock correlation analysis and price prediction. However, it is nontrivial to analyze and predict any stocks, being time-varying and affected by unlimited factors in a given market. Traditional methods only take some certain factors into consideration but ignore others that may be changed dynamically. In this paper, we propose a novel machine learning model named Graph Attention Long Short-Term Memory (GALSTM) to learn the correlations between stocks and predict their future prices automatically. First, a multi-Hawkes Process is used to initial a correlation graph between stocks. This procedure provides a good training start as the multi-Hawkes Processes will be studied on the most saint feature fluctuations with any correlations being statistically significant. Then an attention-based LSTM is built to learn the weighting matrix underlying the dynamic graph. In addition, we also build matching data process plus portfolio management modules to form a complete system. The proposed GALSTM enables us to expand the scope of stock selection under the premise of controlling risks with limited hedging tools in the A-share market, thereby effectively increasing high-frequency excess returns. We then construct a long and short positions combination, select long positions in the A shares of the entire market, and use stock index futures to short. With GALSTM model, the products managed by our fully automatic quantitative trading system achieved an absolute annual return rate of 44.71% and the standard deviation of daily returns is only 0.42% in three months of operation. Only 1 week loss in 13 weeks of running time.}
}
@article{LIN2022108315,
title = {DDBN: Dual detection branch network for semantic diversity predictions},
journal = {Pattern Recognition},
volume = {122},
pages = {108315},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108315},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004957},
author = {Qifeng Lin and Chengjiang Long and Jianhui Zhao and Gang Fu and Zhiyong Yuan},
keywords = {Adjacent feature compensation, Dual detection branch network, Diversity enhancement strategy, Object detection},
abstract = {It is well known that detail features and context semantics are conducive to improving object detection performance. However, the current single-prediction detectors do not well incorporate these two types of information together. To alleviate the limitation of single-prediction on the use of multiple types of information, we propose a dual detection branch network (DDBN) with adjacent feature compensation and customized training strategy for semantic diversity predictions. Different from the conventional single-prediction models, our DDBN is in the form of a single model with dual different semantic predictions. In particular, two types of adjacent feature compensations are designed to extract detail and context information from different perspectives. Also, a specialized training strategy is customized for our DDBN to well explore the diversity of predictions for improving the performance of object detection. We conduct extensive experiments on three datasets, i.e., DOTA, MS-COCO, and Pascal-VOC, and the experimental results strongly demonstrate the efficacy of our proposed model.}
}
@article{CHEN2022108291,
title = {AE-Net: Fine-grained sketch-based image retrieval via attention-enhanced network},
journal = {Pattern Recognition},
volume = {122},
pages = {108291},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108291},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004714},
author = {Yangdong Chen and Zhaolong Zhang and Yanfei Wang and Yuejie Zhang and Rui Feng and Tao Zhang and Weiguo Fan},
keywords = {Fine-grained sketch-based image retrieval (FG-SBIR), Residual channel attention, Local self-spatial attention, Contrastive learning, Spatial sequence transformer},
abstract = {In this paper, we investigate the task of Fine-grained Sketch-based Image Retrieval (FG-SBIR), which uses hand-drawn sketches as input queries to retrieve the relevant images at the fine-grained instance level. The sketches and images come from different modalities, thus the similarity computation needs to consider both fine-grained and cross-modal characteristics. Existing solutions only focus on fine-grained details or spatial contexts, while ignoring the channel context and spatial sequence information. To mitigate such challenging problems, we propose a novel deep FG-SBIR model, which aims at inferring attention maps along channel dimension and spatial dimension, improving modules of channel attention and spatial attention, and exploring Transformer to enhance the model’s ability for constructing and understanding spatial sequence information. We focus not only on the correlation information between two modalities of sketch and image, but also on the discrimination information inside the single modality. Mutual Loss is especially proposed to enhance the traditional triplet loss, and promote the internal discrimination ability of the model on a single modality. Extensive experiments show that our AE-Net obtains promising results on Sketchy, which is the largest public dataset available for FG-SBIR at present.}
}
@article{PEREZ2022108360,
title = {Skeleton-based relational reasoning for group activity analysis},
journal = {Pattern Recognition},
volume = {122},
pages = {108360},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108360},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005409},
author = {Mauricio Perez and Jun Liu and Alex C. Kot},
keywords = {Group activity recognition, Skeleton information, Relational network, Attention mechanisms},
abstract = {Research on group activity recognition mostly leans on the standard two-stream approach (RGB and Optical Flow) as their input features. Few have explored explicit pose information, with none using it directly to reason about the persons interactions. In this paper, we leverage the skeleton information to learn the interactions between the individuals straight from it. With our proposed method GIRN, multiple relationship types are inferred from independent modules, that describe the relations between the body joints pair-by-pair. Additionally to the joints relations, we also experiment with the previously unexplored relationship between individuals and relevant objects (e.g. volleyball). The individuals distinct relations are then merged through an attention mechanism, that gives more importance to those individuals more relevant for distinguishing the group activity. We evaluate our method in the Volleyball dataset, obtaining competitive results to the state-of-the-art. Our experiments demonstrate the potential of skeleton-based approaches for modeling multi-person interactions.}
}
@article{HAN2022108303,
title = {(AD)2: Adversarial domain adaptation to defense with adversarial perturbation removal},
journal = {Pattern Recognition},
volume = {122},
pages = {108303},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108303},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004830},
author = {Keji Han and Bin Xia and Yun Li},
keywords = {Deep learning, Adversarial example, Domain adaptation},
abstract = {Deep Neural Networks (DNNs) are demonstrated to be vulnerable to adversarial examples, which are crafted by adding adversarial perturbations to the legitimate examples. To address this issue, some defense methods have been proposed. Among them, the adversarial training (AT) is a popular method to improve the robustness of DNNs. However, theory analysis has shown that in the adversarial training framework, the improvement of the robustness will lead to a decline of standard accuracy. In this paper, we propose a modularized defense framework, namely Adversarial Domain Adaptation to Defense ((AD)2). Different from all adversarial training methods, (AD)2 detects adversarial example using a generative algorithm and applies the adversarial domain adaptation method to remove adversarial perturbation. Experimental results show that (AD)2 is effective to remove the adversarial perturbation and mitigate the odds between the robustness and standard accuracy for DNNs.}
}
@article{SHERMIN2022108246,
title = {Integrated generalized zero-shot learning for fine-grained classification},
journal = {Pattern Recognition},
volume = {122},
pages = {108246},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108246},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100426X},
author = {Tasfia Shermin and Shyh Wei Teng and Ferdous Sohel and Manzur Murshed and Guojun Lu},
keywords = {Generalized zero-shot learning, Fine-grained classification, Dense attention mechanism},
abstract = {Embedding learning (EL) and feature synthesizing (FS) are two of the popular categories of fine-grained GZSL methods. EL or FS using global features cannot discriminate fine details in the absence of local features. On the other hand, EL or FS methods exploiting local features either neglect direct attribute guidance or global information. Consequently, neither method performs well. In this paper, we propose to explore global and direct attribute-supervised local visual features for both EL and FS categories in an integrated manner for fine-grained GZSL. The proposed integrated network has an EL sub-network and a FS sub-network. Consequently, the proposed integrated network can be tested in two ways. We propose a novel two-step dense attention mechanism to discover attribute-guided local visual features. We introduce new mutual learning between the sub-networks to exploit mutually beneficial information for optimization. Moreover, we propose to compute source-target class similarity based on mutual information and transfer-learn the target classes to reduce bias towards the source domain during testing. We demonstrate that our proposed method outperforms contemporary methods on benchmark datasets.}
}
@article{ZHANG2022108332,
title = {Gaussian-guided feature alignment for unsupervised cross-subject adaptation},
journal = {Pattern Recognition},
volume = {122},
pages = {108332},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108332},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005124},
author = {Kuangen Zhang and Jiahong Chen and Jing Wang and Yuquan Leng and Clarence W. {de Silva} and Chenglong Fu},
keywords = {Domain adaptation, Feature alignment, Human activity recognition, Human intent recognition, Sensor fusion},
abstract = {Human activities recognition (HAR) and human intent recognition (HIR) are important for medical diagnosis and human-robot interaction. HAR and HIR usually rely on the signals of some wearable sensors, such as inertial measurement unit (IMU), but these signals may be user-dependent, which degrades the performance of the recognition algorithm on new subjects. Traditional supervised learning methods require labeling signals and training specific classifiers for each new subject, which is burdensome. To deal with this problem, this paper proposes a novel non-adversarial cross-subject adaptation method called Gaussian-guided feature alignment (GFA). The proposed GFA metric quantifies the discrepancy between the labeled features of source subjects and the unlabeled features of target subjects so that minimizing the GFA metric leads to the alignment of the source and target features. The GFA metric is estimated by calculating the divergence between the feature distribution and Gaussian distribution, as well as the mean squared error of the mean and variance between source and target features. This paper analytically proves the effect of the GFA metric and validates its performance using three public human activity datasets. Experimental results show that the proposed GFA achieves 1% higher target classification accuracy and 0.5% lower variance than state-of-the-art methods in case of cross-subject validation. These results indicate that the proposed GFA is feasible for improving the generalization of the HAR and HIR.}
}
@article{GALLEGO2022108356,
title = {Efficient k-nearest neighbor search based on clustering and adaptive k values},
journal = {Pattern Recognition},
volume = {122},
pages = {108356},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108356},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005367},
author = {Antonio Javier Gallego and Juan Ramón Rico-Juan and Jose J. Valero-Mas},
keywords = {-Nearest Neighbor, Efficient search, Clustering, Feature learning},
abstract = {The k-Nearest Neighbor (kNN) algorithm is widely used in the supervised learning field and, particularly, in search and classification tasks, owing to its simplicity, competitive performance, and good statistical properties. However, its inherent inefficiency prevents its use in most modern applications due to the vast amount of data that the current technological evolution generates, being thus the optimization of kNN-based search strategies of particular interest. This paper introduces the caKD+ algorithm, which tackles this limitation by combining the use of feature learning techniques, clustering methods, adaptive search parameters per cluster, and the use of pre-calculated K-Dimensional Tree structures, and results in a highly efficient search method. This proposal has been evaluated using 10 datasets and the results show that caKD+ significantly outperforms 16 state-of-the-art efficient search methods while still depicting such an accurate performance as the one by the exhaustive kNN search.}
}
@article{MIAO2022108299,
title = {Graph regularized locally linear embedding for unsupervised feature selection},
journal = {Pattern Recognition},
volume = {122},
pages = {108299},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108299},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004799},
author = {Jianyu Miao and Tiejun Yang and Lijun Sun and Xuan Fei and Lingfeng Niu and Yong Shi},
keywords = {Unsupervised feature selection, Local linear embedding, Graph Laplacian, Manifold regularization},
abstract = {As one of the important dimensionality reduction techniques, unsupervised feature selection (UFS) has enjoyed amounts of popularity over the last few decades, which can not only improve learning performance, but also enhance interpretability and reduce computational costs. The existing UFS methods often model the data in the original feature space, which cannot fully exploit the discriminative information. In this paper, to address this issue, we investigate how to strengthen the relationship between UFS and the feature subspace, so as to select relevant features more straightforwardly and effectively. Methodologically, a novel UFS approach, referred to as Graph Regularized Local Linear Embedding (GLLE), is proposed by integrating local linear embedding (LLE) and manifold regularization constrained in feature subspace into a unified framework. To be more specific, we explicitly define a feature selection matrix composed of 0 and 1, which can realize the process of UFS. For the purpose of modelling the feature selection matrix, we propose to preserve the local linear reconstruction relationship among neighboring data points in the feature subspace, which corresponds to LLE constrained in the feature subspace. To make the feature selection matrix more accurate, we propose to use manifold regularization as an assistant of LLE to find the relevant and representative features such that the selected features can make each sample under the feature subspace be accordance with the manifold assumption. A tailored iterative algorithm based on Alternative Direction Method of Multipliers (ADMM) is designed to solve the proposed optimization problem. Extensive experiments on twelve real-world benchmark datasets are conducted, and the more promising results are achieved compared with the state-of-the-arts approaches.}
}
@article{ZHOU2022108275,
title = {Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition},
journal = {Pattern Recognition},
volume = {122},
pages = {108275},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108275},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004556},
author = {Ling Zhou and Qirong Mao and Xiaohua Huang and Feifei Zhang and Zhihong Zhang},
keywords = {Micro-expression recognition, Deep learning, Attention mechanism, Shared feature, Feature refinement},
abstract = {Micro-expression recognition has become challenging, as it is extremely difficult to extract the subtle facial changes of micro-expressions. Recently, several approaches have proposed various expression-shared features algorithms for micro-expression recognition. However, these approaches do not reveal the specific discriminative characteristics, which leads to sub-optimal performance. This paper proposes a novel Feature Refinement (FeatRef) with expression-specific feature learning and fusion for micro-expression recognition that aims to obtain salient and discriminative features for specific expressions and predicts expressions by fusing expression-specific features. FeatRef consists of an expression proposal module with an attention mechanism and a classification branch. First, an inception module is designed based on optical flow to obtain expression-shared features. Second, to extract salient and discriminative features for specific expressions, expression-shared features are fed into an expression proposal module with attention factors and proposal loss. Last, in the classification branch, category labels are predicted via a fusion of expression-specific features. Experiments on three publicly available databases validate the effectiveness of FeatRef under different protocols. The results on public benchmarks demonstrate that FeatRef provides salient and discriminative information for micro-expression recognition. The results also show that FeatRef achieves better or competitive performance with existing state-of-the-art methods on micro-expression recognition.}
}
@article{S2022108287,
title = {Spatio-Temporal association rule based deep annotation-free clustering (STAR-DAC) for unsupervised person re-identification},
journal = {Pattern Recognition},
volume = {122},
pages = {108287},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108287},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004672},
author = {Sridhar Raj S and Munaga V.N.K. Prasad and Ramadoss Balakrishnan},
keywords = {Unsupervised person re-identification, Clustering, Labeling, Spatio-temporal, Deep learning},
abstract = {Multi-camera video surveillance environment has a variety of emerging research problems among, which person re-identification is the premier one. Unsupervised person re-identification has been explored less in literature than the supervised approach. Images acquired from the video surveillance systems are unlabeled, which denotes that it is naturally an unsupervised learning problem. The state-of-the-art unsupervised methods seek external annotations support such as incorporating transfer learning techniques, partial labeling of train images, etc., which makes them not purely unsupervised and unsuitable for practical real-world surveillance settings. Identity mismatch happens due to the similar costumes and complex environmental factors. To resolve this issue, we introduce a new framework named Spatio-Temporal Association Rule based Deep Annotation-free Clustering (STAR-DAC) which incrementally clusters the unlabeled person re-identification images based on visual features and performs cluster fine-tuning through the mined spatio-temporal association rules. STAR formulations leveraged upto 75% of images for reliable sample selection through cluster fine-tuning. STAR based fine-tune algorithm aims to attain ground-truth labels of an unlabeled dataset and eliminate cluster outliers to stabilize the evaluation. Experiments are performed on image and video-based benchmark person re-identification datasets such as DukeMTMC re-ID, Market1501, MSMT17, CUHK03, GRID and Dukevideo re-ID, iLIDSVid, ViPer respectively. Experimental results clearly show that the proposed STAR-DAC framework outperforms the state-of-the-art methods in case of large scale datasets with multiple cameras.}
}
@article{ZHANG2022108344,
title = {Leveraging local and global descriptors in parallel to search correspondences for visual localization},
journal = {Pattern Recognition},
volume = {122},
pages = {108344},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108344},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005240},
author = {Pengju Zhang and Chaofan Zhang and Bingxi Liu and Yihong Wu},
keywords = {Visual localization, 6DoF pose, Parallel search, Learning based descriptor},
abstract = {Visual localization to compute 6DoF camera pose from a given image has wide applications. Both local and global descriptors are crucial for visual localization. Most of the existing visual localization methods adopt a two-stage strategy: image retrieval first is performed by global descriptors, and then 2D-3D correspondences are made by local descriptors from 2D query image points and its nearest neighbor candidates which are the 3D points visible by these retrieved images. The above two stages are serially performed in these methods. However, due to the fact that 3D points obtained from the retrieval feedback are only rely on global descriptors, these methods cannot fully take the advantages of both local and global descriptors. In this paper, we propose a novel parallel search framework, which fully leverages advantages of both local and global descriptors to get nearest neighbor candidates of a 2D query image point. Specifically, besides using deep learning based global descriptors, we also utilize local descriptors to construct random tree structures for obtaining nearest neighbor candidates of the 2D query image point. We propose a new probability model and a new deep learning based local descriptor when constructing the random trees. In addition, a weighted Hamming regularization term to keep discriminativeness after binarization is given in loss function for the proposed local descriptor. The loss function co-trains both real and binary local descriptors of which the results are integrated into the random trees. Experiments on challenging benchmarks show that the proposed localization method can significantly improve the robustness and accuracy compared with the ones which get nearest neighbor candidates of a query local feature just based on either local or global descriptors.}
}
@article{SHI2022108314,
title = {Image-to-video person re-identification using three-dimensional semantic appearance alignment and cross-modal interactive learning},
journal = {Pattern Recognition},
volume = {122},
pages = {108314},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108314},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004945},
author = {Wei Shi and Hong Liu and Mengyuan Liu},
keywords = {Person re-identification, Cross-modal learning, Appearance alignment},
abstract = {Image-to-video person re-identification (I2V ReID), which aims to retrieve human targets between image-based queries and video-based galleries, has recently become a new research focus. However, the appearance misalignment and modality misalignment in both images and videos caused by pose variations, camera views, misdetections, and different data types, make I2V ReID still challenging. To this end, we propose a deep I2V ReID pipeline based on three-dimensional semantic appearance alignment (3D-SAA) and cross-modal interactive learning (CMIL) to address the aforementioned two challenges. Specifically, in the 3D-SAA module, the aligned local appearance images extracted by dense 3D human appearance estimation are in conjunction with global image and video embedding streams to learn more fine-grained identity features. The aligned local appearance images are further semantically aggregated by the proposed multi-branch aggregation network to weaken the negligible body parts. Moreover, to overcome the influence of modality misalignment, a CMIL module enables the communication between global image and video streams by interactively propagating the temporal information in videos to the channels of image feature maps. Extensive experiments on challenging MARS, DukeMTMC-VideoReID and iLIDS-VID datasets, show the superiority of our approach.}
}
@article{ZHAN2022108262,
title = {Discrete online cross-modal hashing},
journal = {Pattern Recognition},
volume = {122},
pages = {108262},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108262},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004428},
author = {Yu-Wei Zhan and Yongxin Wang and Yu Sun and Xiao-Ming Wu and Xin Luo and Xin-Shun Xu},
keywords = {Cross-modal retrieval, Discrete optimization, Online hashing, Learning to hash},
abstract = {With the prevalence of multimedia content on the Web which usually continuously comes in a stream fashion, online cross-modal hashing methods have attracted extensive interest in recent years. However, most online hashing methods adopt a relaxation strategy or real-valued auxiliary variable strategy to avoid complex optimization of hash codes, leading to large quantization errors. In this paper, based on Discrete Latent Factor model-based cross-modal Hashing (DLFH), we propose a novel cross-modal online hashing method, i.e., Discrete Online Cross-modal Hashing (DOCH). To generate uniform high-quality hash codes of different modal, DOCH not only directly exploits the similarity between newly coming data and old existing data in the Hamming space, but also utilizes the fine-grained semantic information by label embedding. Moreover, DOCH can discretely learn hash codes by an efficient optimization algorithm. Extensive experiments conducted on two real-world datasets demonstrate the superiority of DOCH.}
}
@article{SUH2022108302,
title = {Discriminative feature generation for classification of imbalanced data},
journal = {Pattern Recognition},
volume = {122},
pages = {108302},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108302},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004829},
author = {Sungho Suh and Paul Lukowicz and Yong Oh Lee},
keywords = {Imbalanced classification, Generative adversarial networks, Discriminative feature generation, Transfer learning, Feature map regularization},
abstract = {The data imbalance problem is a frequent bottleneck in the classification performance of neural networks. In this paper, we propose a novel supervised discriminative feature generation (DFG) method for a minority class dataset. DFG is based on the modified structure of a generative adversarial network consisting of four independent networks: generator, discriminator, feature extractor, and classifier. To augment the selected discriminative features of the minority class data by adopting an attention mechanism, the generator for the class-imbalanced target task is trained, and the feature extractor and classifier are regularized using the pre-trained features from a large source data. The experimental results show that the DFG generator enhances the augmentation of the label-preserved and diverse features, and the classification results are significantly improved on the target task. The feature generation model can contribute greatly to the development of data augmentation methods through discriminative feature generation and supervised attention methods.}
}
@article{AVILESRIVERO2022108274,
title = {GraphXCOVID: Explainable deep graph diffusion pseudo-Labelling for identifying COVID-19 on chest X-rays},
journal = {Pattern Recognition},
volume = {122},
pages = {108274},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108274},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004544},
author = {Angelica I. Aviles-Rivero and Philip Sellars and Carola-Bibiane Schönlieb and Nicolas Papadakis},
keywords = {COVID-19, Chest X-ray, Semi-Supervised learning, Deep learning, Explainability},
abstract = {Can one learn to diagnose COVID-19 under extreme minimal supervision? Since the outbreak of the novel COVID-19 there has been a rush for developing automatic techniques for expert-level disease identification on Chest X-ray data. In particular, the use of deep supervised learning has become the go-to paradigm. However, the performance of such models is heavily dependent on the availability of a large and representative labelled dataset. The creation of which is a heavily expensive and time consuming task, and especially imposes a great challenge for a novel disease. Semi-supervised learning has shown the ability to match the incredible performance of supervised models whilst requiring a small fraction of the labelled examples. This makes the semi supervised paradigm an attractive option for identifying COVID-19. In this work, we introduce a graph based deep semi-supervised framework for classifying COVID-19 from chest X-rays. Our framework introduces an optimisation model for graph diffusion that reinforces the natural relation among the tiny labelled set and the vast unlabelled data. We then connect the diffusion prediction output as pseudo-labels that are used in an iterative scheme in a deep net. We demonstrate, through our experiments, that our model is able to outperform the current leading supervised model with a tiny fraction of the labelled examples. Finally, we provide attention maps to accommodate the radiologist’s mental model, better fitting their perceptual and cognitive abilities. These visualisation aims to assist the radiologist in judging whether the diagnostic is correct or not, and in consequence to accelerate the decision.}
}
@article{ZHANG2022108343,
title = {Robust and discrete matrix factorization hashing for cross-modal retrieval},
journal = {Pattern Recognition},
volume = {122},
pages = {108343},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108343},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005239},
author = {Donglin Zhang and Xiao-Jun Wu},
keywords = {Cross-modal retrieval, Hashing, Autoencoder, Discrete optimization,},
abstract = {Hashing based methods have gained great success for cross-modal similarity search, due to its fast query speed and low storage cost. However, there are some challenging problems that need to be further solved: 1) Many approaches are sensitive to noises and outliers, because ℓ2 norm is utilized in the objective function, the error may be amplified. 2) Most existing methods take relaxation or rounding scheme to generate binary codes, causing a large quantization loss. 3) Many supervised cross-media algorithms usually take a large n×n matrix to preserve the similarity relationship, leading to large calculation and making them unscalable. To mitigate these challenges, we develop a novel cross-media search algorithm, i.e., robust and discrete matrix factorization hashing, dubbed RDMH. The method takes a two-step strategy. In the first phase, the ℓ2,1 norm is utilized to improve the robustness, which makes our model not sensitive to noises and outliers. We can learn the hash codes directly by the proposed discrete optimization method instead of relaxation scheme, avoiding the large quantization loss. Moreover, RDMH correlates the hash codes and semantic labels directly instead of manipulating the large similarity matrix. In the second phase, we propose an autoencoder strategy to learn the hash functions, more valuable information can be preserved and making the hash functions more powerful. Comprehensive experiments on several databases demonstrate the superior performance and efficacy of the developed RDMH.}
}
@article{CHEN2022108250,
title = {JSPNet: Learning joint semantic & instance segmentation of point clouds via feature self-similarity and cross-task probability},
journal = {Pattern Recognition},
volume = {122},
pages = {108250},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108250},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004301},
author = {Feng Chen and Fei Wu and Guangwei Gao and Yimu Ji and Jing Xu and Guo-Ping Jiang and Xiao-Yuan Jing},
keywords = {Instance & semantic segmentation, Point could processing, Multi-task learning},
abstract = {In this paper, we propose a novel method named JSPNet, to segment 3D point cloud in semantic and instance simultaneously. First, we analyze the problem in addressing joint semantic and instance segmentation, including the common ground of cooperation of two tasks, conflict of two tasks, quadruplet relation between semantic and instance distributions, and ignorance of existing works. Then we introduce our method to reinforce mutual cooperation and alleviate the essential conflict. Our method has a shared encoder and two decoders to address two tasks. Specifically, to maintain discriminative features and characterize inconspicuous content, a similarity-based feature fusion module is designed to locate the inconspicuous area in the feature of current branch and then select related features from the other branch to compensate for the unclear content. Furthermore, given the salient semantic feature and the salient instance feature, a cross-task probability-based feature fusion module is developed to establish the probabilistic correlation between semantic and instance features. This module could transform features from one branch and further fuse them with the other branch by multiplying probabilistic matrix. Experimental results on a large-scale 3D indoor point cloud dataset S3DIS and a part-segmentation dataset ShapeNet have demonstrated the superiority of our method over existing state-of-the-arts in both semantic and instance segmentation. The proposed method outperforms PointNet with 12% and 26% improvements and outperforms ASIS with 2.7% and 4.3% improvements in terms of mIoU and mPre. Code of this work has been made available at https://github.com/Chenfeng1271/JSPNet.}
}
@article{CHEN2022108355,
title = {GeoConv: Geodesic guided convolution for facial action unit recognition},
journal = {Pattern Recognition},
volume = {122},
pages = {108355},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108355},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005355},
author = {Yuedong Chen and Guoxian Song and Zhiwen Shao and Jianfei Cai and Tat-Jen Cham and Jianmin Zheng},
keywords = {Geodesic guided convolution, 3D morphable face model, Facial action unit recognition, Emotion recognition},
abstract = {Automatic facial action unit (AU) recognition has attracted great attention but still remains a challenging task, as subtle changes of local facial muscles are difficult to thoroughly capture. Most existing AU recognition approaches leverage geometry information in a straightforward 2D or 3D manner, which either ignore 3D manifold information or suffer from high computational costs. In this paper, we propose a novel geodesic guided convolution (GeoConv) for AU recognition by embedding 3D manifold information into 2D convolutions. Specifically, the kernel of GeoConv is weighted by our introduced geodesic weights, which are negatively correlated to geodesic distances on a coarsely reconstructed 3D morphable face model. Moreover, based on GeoConv, we further develop an end-to-end trainable framework named GeoCNN for AU recognition. Extensive experiments on BP4D and DISFA benchmarks show that our approach significantly outperforms the state-of-the-art AU recognition methods.}
}
@article{ZHOU2022108290,
title = {Contextual ensemble network for semantic segmentation},
journal = {Pattern Recognition},
volume = {122},
pages = {108290},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108290},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004702},
author = {Quan Zhou and Xiaofu Wu and Suofei Zhang and Bin Kang and Zongyuan Ge and Longin {Jan Latecki}},
keywords = {Ensemble deconvolution, Semantic segmentation, FCNs, Context aggregation, Encoder-decoder networks},
abstract = {Recently, exploring features from different layers in fully convolutional networks (FCNs) has gained substantial attention to capture context information for semantic segmentation. This paper presents a novel encoder-decoder architecture, called contextual ensemble network (CENet), for semantic segmentation, where the contextual cues are aggregated via densely usampling the convolutional features of deep layer to the shallow deconvolutional layers. The proposed CENet is trained in terms of end-to-end segmentation to match the resolution of input image, and allows us to fully explore contextual features through ensemble of dense deconvolutions. We evaluate our CENet on two widely-used semantic segmentation datasets: PASCAL VOC 2012 and CityScapes. The experimental results demonstrate our CENet achieves superior performance with respect to recent state-of-the-art results. Furthermore, we also evaluate CENet on MS COCO dataset and ISBI 2012 dataset for the task of instance segmentation and biological segmentation, respectively. The experimental results show that CENet obtains promising results on these two datasets.}
}
@article{TAN2022108366,
title = {End-to-End Supermask Pruning: Learning to Prune Image Captioning Models},
journal = {Pattern Recognition},
volume = {122},
pages = {108366},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108366},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100546X},
author = {Jia Huei Tan and Chee Seng Chan and Joon Huang Chuah},
keywords = {Image captioning, Deep network compression, Deep learning},
abstract = {With the advancement of deep models, research work on image captioning has led to a remarkable gain in raw performance over the last decade, along with increasing model complexity and computational cost. However, surprisingly works on compression of deep networks for image captioning task has received little to no attention. For the first time in image captioning research, we provide an extensive comparison of various unstructured weight pruning methods on three different popular image captioning architectures, namely Soft-Attention, Up-Down and Object Relation Transformer. Following this, we propose a novel end-to-end weight pruning method that performs gradual sparsification based on weight sensitivity to the training loss. The pruning schemes are then extended with encoder pruning, where we show that conducting both decoder pruning and training simultaneously prior to the encoder pruning provides good overall performance. Empirically, we show that an 80% to 95% sparse network (up to 75% reduction in model size) can either match or outperform its dense counterpart. The code and pre-trained models for Up-Down and Object Relation Transformer that are capable of achieving CIDEr scores >120 on the MS-COCO dataset but with only 8.7 MB and 14.5 MB in model size (size reduction of 96% and 94% respectively against dense versions) are publicly available at https://github.com/jiahuei/sparse-image-captioning.}
}
@article{TAN2022108298,
title = {A Novel Robust Low-rank Multi-view Diversity Optimization Model with Adaptive-Weighting Based Manifold Learning},
journal = {Pattern Recognition},
volume = {122},
pages = {108298},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108298},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004787},
author = {Junpeng Tan and Zhijing Yang and Jinchang Ren and Bing Wang and Yongqiang Cheng and Wing-Kuen Ling},
keywords = {Low-rank Representation (LRR), Multi-view Subspace Clustering (MVSC), Hilbert Schmidt Independence Criterion (HSIC), Non-negative Matrix Factorization (NMF), Adaptive-Weighting Manifold Learning (AWML)},
abstract = {Multi-view clustering has become a hot yet challenging topic, due mainly to the independence of and information complementarity between different views. Although good results are achieved to a certain extent from typical methods including multi-view based k-means clustering, sparse cooperative representation clustering and subspace clustering, they still suffer from several drawbacks or limitations: (1) When each view is sparse decomposed, it still contains some hidden information for mining, such as the structure of samples, the intra-class similarity measure, and the inter-class diversity discrimination, etc. (2) Most of the existing multi-view methods only consider the local features within each view, but fail to effectively balance the importance of and combine information among different views in a diversified way. To tackle these issues, we propose a novel multi-view diversity learning model based on robust bilinear error decomposition (BED). The BED term with a low rank sparse constraint is an improved non-negative matrix factorization (NMF), which is used to extract the hidden structure information in sparse decomposition and useful diversity discrimination information in error matrix. The preservation of local features and selection of important views are achieved by adaptive weighted manifold learning. Furthermore, the Hilbert Schmidt independence criterion is used as a diversity learning term for mutual learning and fusion among views. Finally, the proposed robust low-rank multi-view diversity learning spectral clustering method is evaluated and benchmarked with eight state-of-the-art methods. Experiments in six real datasets have fully validated the significantly improved accuracy and efficiency of the proposed methodology for effective clustering of multi-view images.}
}
@article{HAMMER2022108339,
title = {Estimating Tukey depth using incremental quantile estimators},
journal = {Pattern Recognition},
volume = {122},
pages = {108339},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108339},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005197},
author = {Hugo L. Hammer and Anis Yazidi and Håvard Rue},
keywords = {Data stream, Incremental quantile estimator, Distributional patterns, Real-time analytics, Tukey depth},
abstract = {Measures of distance or how data points are positioned relative to each other are fundamental in pattern recognition. The concept of depth measures how deep an arbitrary point is positioned in a dataset, and is an interesting concept in this regard. However, while this concept has received a lot of attention in the statistical literature, its application within pattern recognition is still limited. To increase the applicability of the depth concept in pattern recognition, we address the well-known computational challenges associated with the depth concept, by suggesting to estimate depth using incremental quantile estimators. The suggested algorithm can not only estimate depth when the dataset is known in advance, but can also track depth for dynamically varying data streams by using recursive updates. The tracking ability of the algorithm was demonstrated based on a real-life application associated with detecting changes in human activity from real-time accelerometer observations. Given the flexibility of the suggested approach, it can detect virtually any kind of changes in the distributional patterns of the observations, and thus outperforms detection approaches based on the Mahalanobis distance.}
}
@article{BAI2022108331,
title = {Multinomial random forest},
journal = {Pattern Recognition},
volume = {122},
pages = {108331},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108331},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005112},
author = {Jiawang Bai and Yiming Li and Jiawei Li and Xue Yang and Yong Jiang and Shu-Tao Xia},
keywords = {Random forest, Consistency, Differential privacy, Classification},
abstract = {Despite the impressive performance of random forests (RF), its theoretical properties have not been thoroughly understood. In this paper, we propose a novel RF framework, dubbed multinomial random forest (MRF), to analyze its consistency and privacy-preservation. Instead of deterministic greedy split rule or with simple randomness, the MRF adopts two impurity-based multinomial distributions to randomly select a splitting feature and a splitting value, respectively. Theoretically, we prove the consistency of MRF and analyze its privacy-preservation within the framework of differential privacy. We also demonstrate with multiple datasets that its performance is on par with the standard RF. To the best of our knowledge, MRF is the first consistent RF variant that has comparable performance to the standard RF. The code is available at https://github.com/jiawangbai/Multinomial-Random-Forest.}
}
@article{MIAO2022108258,
title = {Balanced single-shot object detection using cross-context attention-guided network},
journal = {Pattern Recognition},
volume = {122},
pages = {108258},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108258},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004386},
author = {Shuyu Miao and Shanshan Du and Rui Feng and Yuejie Zhang and Huayu Li and Tianbi Liu and Lin Zheng and Weiguo Fan},
keywords = {Cross-context attention-guided network, Cross-context attention mechanism, Receptive field attention mechanism, Semantic fusion attention mechanism, Accuracy and speed balance},
abstract = {In real-world application scenarios, object detection usually encounters two technical challenges, i.e., high accuracy and high speed. Although the latest detection frameworks based on anchor-free detection have achieved outstanding performance, they cannot be widely used in real-world scenarios due to their model complexity and slow speed. In this paper, inspired by cross-context attention mechanism of human visual systems, we propose a light but effective single-shot detection framework using Cross-context Attention-guided Network (CCAGNet) to balance the accuracy and speed. CCAGNet uses attention-guided mechanism to highlight the interaction of object-synergy regions, and suppresses non-object-synergy regions by combining Cross-context Attention Mechanism (CCAM), Receptive Field Attention Mechanism (RFAM), and Semantic Fusion Attention Mechanism (SFAM). The main contribution of our work includes establishing a novel attention mechanism that takes the context information of channel, spatial, cross- and adjacent-regions into consideration simultaneously. Extensive experiments demonstrate the feasibility and effectiveness of our method on the public benchmark datasets. To the best of our knowledge, CCAGNet obtains the state-of-the-art performance on both PascalVOC and MSCOCO with the excellent trade-off between accuracy and speed among single-shot detectors. Especially, the Average Precision (AP) metric is significantly improved by 17.0% on small object detection on MSCOCO.}
}
@article{ZHONG2022108286,
title = {Effective and efficient pixel-level detection for diverse video copy-move forgery types},
journal = {Pattern Recognition},
volume = {122},
pages = {108286},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108286},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004660},
author = {Jun-Liu Zhong and Yan-Fen Gan and Chi-Man Vong and Ji-Xiang Yang and Jing-Hong Zhao and Jia-Hua Luo},
keywords = {Video copy-move forgery detection, Thorough feature extraction, Fast keypoint-label matching, Coarse-to-fine filtering, Adaptive block filling},
abstract = {Video copy-move forgery detection (VCMFD) is a significant and greatly challenging task due to a variety of difficulties, including a huge amount of video information, diverse forgery types, rich forgery objects, and homogenous forgery sources. These difficulties raise four unresolved key challenges in VCMFD: i) ineffective detection in some popular forgery cases; ii) inefficient matching in processing numerous video pixels with hundred-dimensional features under dozens of matching iterations; iii) high false positive (FP) in detecting forgery videos; iv) low trade-off of efficiency and effectiveness in filling forgery region, and even failing in indicating forgeries at the pixel level. In this paper, a novel VCMFD method is proposed to address these issues: i) an innovatively improved SIFT structure that can address the thorough feature extraction in all video copy-move forgery cases; ii) a novel fast keypoint-label matching (FKLM) algorithm is proposed that creates some keypoint-label groups so that every high-dimensional feature is assigned into one of these groups. As a result, matching of video pixels can be directly done on a small number of keypoint-label groups only, leading to a nearly 500% raise in matching efficiency; iii) a new coarse-to-fine filtering relying on intrinsic attributes of exact keypoint-matches is designed to more effectively reduce the false keypoint-matches; iv) the adaptive block filling relying on true keypoint-matches contributes to the accurate and efficient suspicious region filling, even at the pixel level. Finally, the suspicious region locations with the forgery vision persistence concept indicate forgery videos. Compared to the state-of-art methods, the experiments show that our proposed method achieves the best detection accuracy, lowest FP, and improved at least 16% and 8% of F1 scores on the GRIP 2.0 dataset and a combination of SULFA 2.0 & REWIND datasets. Furthermore, the proposed method is with low computational time (4.45 s/Mpixels), which is about 1/2-1/3 times of the latest DFMI-BM (8.02 s/Mpixels) and PM-2D (13.1 s/Mpixels) methods.}
}
@article{GAO2022108233,
title = {Discrepant multiple instance learning for weakly supervised object detection},
journal = {Pattern Recognition},
volume = {122},
pages = {108233},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108233},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004143},
author = {Wei Gao and Fang Wan and Jun Yue and Songcen Xu and Qixiang Ye},
keywords = {Weakly supervised detection, Multiple instance learning, Learner discrepancy, Collaborative learning},
abstract = {Multiple Instance Learning (MIL) is a fundamental method for weakly supervised object detection (WSOD), but experiences difficulty in excluding local optimal solutions and may miss objects or falsely localize object parts. In this paper, we introduce discrepantly collaborative modules into MIL and thereby create discrepant multiple instance learning (D-MIL), pursuing optimal solutions in a simple-yet-effective way. D-MIL adopts multiple MIL learners to pursue discrepant yet complementary solutions indicating object parts, which are fused with a collaboration module for precise object localization. D-MIL implements a new “teachers-students” model, where MIL learners act as “teachers” and object detectors as “students”. Multiple teachers provide rich yet complementary information, which are absorbed by students and transferred back to reinforce the performance of teachers. Experiments show that D-MIL significantly improves the baseline while achieves state-of-the-art performance on the challenging MS-COCO object detection benchmark.}
}
@article{YANG2022108228,
title = {Graph matching based on fast normalized cut and multiplicative update mapping},
journal = {Pattern Recognition},
volume = {122},
pages = {108228},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108228},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100409X},
author = {Jing Yang and Xu Yang and Zhang-Bing Zhou and Zhi-Yong Liu},
keywords = {Graph matching, Fast normalized cut, Discrete constraint, Multiplicative update},
abstract = {Point correspondence is a fundamental problem in pattern recognition and computer vision, which can be tackled by graph matching. Since graph matching is basically an NP-complete problem, some approximate methods are proposed to solve it. Continuous relaxation offers an effective approximate method for graph matching problem. However, the discrete constraint is not taken into consideration in the optimization step. In this paper, a fast normalized cut based graph matching method is proposed, where the discrete constraint is introduced into the optimization step. Specifically, first a semidefinite positive affinity matrix based form objective function is constructed by introducing a regularization term which is related to the discrete constraint. Then the fast normalized cut algorithm is utilized to find the continuous solution. Last, the discrete solution of graph matching is obtained by a multiplicative update algorithm. Experiments on both synthetic points and real-world images validate the effectiveness of the proposed method by comparing it with the state-of-the-art methods.}
}