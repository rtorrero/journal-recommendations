@article{WANG2022108811,
title = {Discriminative and regularized echo state network for time series classification},
journal = {Pattern Recognition},
volume = {130},
pages = {108811},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108811},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002928},
author = {Heshan Wang and Yuxi Liu and Dongshu Wang and Yong Luo and Chudong Tong and Zhaomin Lv},
keywords = {Echo state network, Recurrent neural networks, Discriminative feature extraction, Time series classification, Outlier-robust weights},
abstract = {An echo State Network (ESN) is a special structure of a recurrent neural network (RNN) in which the recurrent neurons are randomly connected. ESN models which have achieved a high accuracy on time series prediction tasks can be used as time series prediction models in many domains. Nevertheless, in most ESN models, the input weights are randomly generated and the output weights calculated by the least square method are susceptible to outliers, which cannot guarantee that the ESN models will always be optimal for a given task. In this paper, a novel discriminative and regularized ESN (DR-ESN) combines discriminative feature aggregation (DFA) and outlier-robust weights (ORW) algorithms are proposed for time series classification. DFA is firstly proposed to replace the random input weights of ESN with the constrained weights generated from sample information. In DFA, weight vectors are selected from the vector space spanned by initial input sequence vectors, then the new generated input weights can adequately represent the data features. Secondly, ORW is employed to enhance the robustness of output weights by constraining the weights assigned to samples with large training errors. The weights evaluation and experiments on a massive set of the synthetic time series data, real-world bearing fault data and UCR benchmarks indicate that the proposed DR-ESN can not only considerably improve the original ESN classifier but also effectively suppress the effect of outliers on classification performance.}
}
@article{VALEROMAS2023109190,
title = {Multilabel Prototype Generation for data reduction in K-Nearest Neighbour classification},
journal = {Pattern Recognition},
volume = {135},
pages = {109190},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109190},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006690},
author = {Jose J. Valero-Mas and Antonio Javier Gallego and Pablo Alonso-Jiménez and Xavier Serra},
keywords = {Multilabel classification, Prototype generation, Efficient NN},
abstract = {Prototype Generation (PG) methods are typically considered for improving the efficiency of the k-Nearest Neighbour (kNN) classifier when tackling high-size corpora. Such approaches aim at generating a reduced version of the corpus without decreasing the classification performance when compared to the initial set. Despite their large application in multiclass scenarios, very few works have addressed the proposal of PG methods for the multilabel space. In this regard, this work presents the novel adaptation of four multiclass PG strategies to the multilabel case. These proposals are evaluated with three multilabel kNN-based classifiers, 12 corpora comprising a varied range of domains and corpus sizes, and different noise scenarios artificially induced in the data. The results obtained show that the proposed adaptations are capable of significantly improving—both in terms of efficiency and classification performance—the only reference multilabel PG work in the literature as well as the case in which no PG method is applied, also presenting statistically superior robustness in noisy scenarios. Moreover, these novel PG strategies allow prioritising either the efficiency or efficacy criteria through its configuration depending on the target scenario, hence covering a wide area in the solution space not previously filled by other works.}
}
@article{XU2023109137,
title = {Classification of single-view object point clouds},
journal = {Pattern Recognition},
volume = {135},
pages = {109137},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109137},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006173},
author = {Zelin Xu and Kangjun Liu and Ke Chen and Changxing Ding and Yaowei Wang and Kui Jia},
keywords = {Point cloud classification, Rotation equivariance, Pose estimation},
abstract = {Object point cloud classification has drawn great research attention since the release of benchmarking datasets, such as the ModelNet and the ShapeNet. These benchmarks assume point clouds covering complete surfaces of object instances, for which plenty of high-performing methods have been developed. However, their settings deviate from those often met in practice, where, due to (self-)occlusion, a point cloud covering partial surface of an object is captured from an arbitrary view. We show in this paper that performance of existing point cloud classifiers drops drastically under the considered single-view, partial setting; the phenomenon is consistent with the observation that semantic category of a partial object surface is less ambiguous only when its distribution on the whole surface is clearly specified. To this end, we argue for a single-view, partial setting where supervised learning of object pose estimation should be accompanied with classification. Technically, we propose a baseline method of Pose-Accompanied Point cloud classification Network (PAPNet); built upon SE(3)-equivariant convolutions, the PAPNet learns intermediate pose transformations for equivariant features defined on vector fields, which makes the subsequent classification easier (ideally) in the category-level, canonical pose. By adapting existing ModelNet40 and ScanNet datasets to the single-view, partial setting, experiment results can verify the necessity of object pose estimation and superiority of our PAPNet to existing classifiers.}
}
@article{SHI2022108837,
title = {Multimodal channel-wise attention transformer inspired by multisensory integration mechanisms of the brain},
journal = {Pattern Recognition},
volume = {130},
pages = {108837},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108837},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003181},
author = {Qianqian Shi and Junsong Fan and Zuoren Wang and Zhaoxiang Zhang},
keywords = {Multisensory integration, Top-down attention, Multimodal transformer, Fine-grained bird recognition, Emotion recognition},
abstract = {Multisensory integration has attracted intense studies for decades. How to combine visual and auditory information to optimize perception and decision-making is a key question in neuroscience as well as machine learning. Inspired by the mechanisms of multisensory integration in the brain, we propose a multimodal channel-wise attention transformer (MCAT) that performs reliability-weighted integration and revises the weights allocation according to a top-down attention-like mechanism. We apply MCAT on EF-LSTM neural networks for a fine-grained video bird recognition task, and on MulT neural networks for an emotion recognition task. The performance of both models is improved remarkably. Ablation study shows that the attention mechanism is indispensable for effective multisensory integration. Moreover, we found that cross-modal integration models are in accordance with the law of inverse effectiveness of multisensory integration in the brain, which reveals that our model may have mechanisms similar to those in the brain. Taken together, the results demonstrate that the brain-inspired MCAT block is effective for improving multisensory integration, providing useful clues for designing new algorithms and understanding multisensory integration in the brain.}
}
@article{SHAN2022108748,
title = {Self-Attention based fine-grained cross-media hybrid network},
journal = {Pattern Recognition},
volume = {130},
pages = {108748},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108748},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002291},
author = {Wei Shan and Dan Huang and Jiangtao Wang and Feng Zou and Suwen Li},
keywords = {Fine-Grained, Cross-Media, Retrieval, Attention},
abstract = {Due to the heterogeneity gap, the data representations of different types of media are inconsistent. It is challenging to measure the fine-grained gap between different media. To this end, we propose a self-attention-based hybrid network to learn the common representations of different media data. Specifically, we first utilize a local self-attention layer to learn the common attention space between different media data. Then we propose a similarity concatenation method to understand the content relationship between features. To further improve the robustness of the model, we also learn a local position encoding to capture the spatial relationships between features. Therefore, our proposed approach can effectively reduce the gap between different feature distributions on cross-media retrieval tasks. Extensive experiments and ablation studies demonstrate that our proposed method achieves state-of-the-art performance. The source code and models are publicly available at: https://github.com/NUST-Machine-Intelligence-Laboratory/SAFGCMHN.}
}
@article{DU2023109154,
title = {Prototype-Guided Feature Learning for Unsupervised Domain Adaptation},
journal = {Pattern Recognition},
volume = {135},
pages = {109154},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109154},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006331},
author = {Yongjie Du and Deyun Zhou and Yu Xie and Yu Lei and Jiao Shi},
keywords = {Unsupervised domain adaptation, Class prototype, Pseudo labeling, Label filtering},
abstract = {Unsupervised Domain Adaptation transfers knowledge from the source domain to the target domain. It makes remarkable progress in alleviating the label-shortage problem in machine learning. Existing methods focus on aligning the two domain distributions directly. However, due to domain discrepancy, there may be some samples in the source domain being unnecessary or even harmful to the target tasks. Avoiding transferring knowledge from these samples is crucial. Existing researches are limited in this area. To this end, we propose a new unsupervised domain adaptation approach named the prototype-guided feature learning. The proposed method contains three main innovations. Firstly, we propose to utilize the more representative source-domain samples, class prototypes, to learn a domain-invariant subspace with the target samples. Secondly, the modified nearest class prototype method is proposed to predict the target samples by exploiting the structural information of the target domain efficiently. Thirdly, a multi-stage label filtering method is proposed to alleviate the mislabeling problem during training. Extensive experiments manifest that our method is competitive compared to the current mainstream unsupervised domain adaptive methods.}
}
@article{QIN2023109125,
title = {Weakly supervised adversarial learning via latent space for hyperspectral target detection},
journal = {Pattern Recognition},
volume = {135},
pages = {109125},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109125},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006057},
author = {Haonan Qin and Weiying Xie and Yunsong Li and Kai Jiang and Jie Lei and Qian Du},
keywords = {Hyperspectral image, Target detection, Weakly supervised learning, Adversarial learning, Latent space},
abstract = {As an advanced technique in remote sensing, hyperspectral target detection (HTD) is widely concerned in civilian and military applications. However, the limitation of prior and mixed pixels phenomenon makes HTD models sensitive to data corruption under various interference from environment. In this work, a novel two-stage detection framework based on adversarial learning is proposed, which extracts spectral features in latent space through background reconstruction under weak supervision. To address the issues of insufficient utilization of both background information and limited prior knowledge, the generative adversarial network (GAN) is applied to estimate background in a weakly supervised manner with target-based constraints and channel-wise attention, which produces the detection proposal in the first stage. Then, a refined result is produced in the second stage, in which the input data consists of the refined data and refined feature map based on previous detection proposal. To provide samples for weakly supervised learning (WSL), the pseudo datasets are produced by a coarse sample selection procedure, which makes full use of limited prior information. Finally, an exponential constrained nonlinear function is adopted to acquire pixel-level prediction via suppressing the background and combining features from different stages. Experiments on real hyperspectral images (HSIs) captured by different sensors at various scenes verify the effectiveness of the proposed framework.}
}
@article{WANG2023109146,
title = {A Learnable Gradient operator for face presentation attack detection},
journal = {Pattern Recognition},
volume = {135},
pages = {109146},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109146},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006252},
author = {Caixun Wang and Bingyao Yu and Jie Zhou},
keywords = {Face presentation attack detection, Learnable gradient operator, Depth-supervised network},
abstract = {Face presentation attack detection (PAD) aims to protect the security of face recognition systems. The existing depth-supervised method using stacked vanilla convolutions cannot explicitly extract efficient fine-grained information (e.g., spatial gradient magnitude) for the distinction between bona fide and attack presentations. To address this issue, the Sobel operator has been demonstrated effective to acquire gradient magnitude due to the fast calculation capacity for high-frequency information. However, the Sobel operator is hand-crafted so cannot deal with complex textures. Differently, we develop a learnable gradient operator (LGO) to adaptively learn gradient information in a data-driven way, which is a generalization of existing gradient operators and effectively captures detailed discriminative clues from raw pixels. In parallel, we propose an adaptive gradient loss for better optimization. Extensive experimental comparisons with the state-of-the-art methods on the widely used Replay-Attack, CASIA-FASD, OULU-NPU, and SiW datasets demonstrate the superior performance of the proposed approach.}
}
@article{HUANG2023109170,
title = {SAPENet: Self-Attention based Prototype Enhancement Network for Few-shot Learning},
journal = {Pattern Recognition},
volume = {135},
pages = {109170},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109170},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006495},
author = {Xilang Huang and Seon Han Choi},
keywords = {Few-shot learning, Multi-head self-attention mechanism, Image classification, -Nearest neighbor},
abstract = {Few-shot learning considers the problem of learning unseen categories given only a few labeled samples. As one of the most popular few-shot learning approaches, Prototypical Networks have received considerable attention owing to their simplicity and efficiency. However, a class prototype is typically obtained by averaging a few labeled samples belonging to the same class, which treats the samples as equally important and is thus prone to learning redundant features. Herein, we propose a self-attention based prototype enhancement network (SAPENet) to obtain a more representative prototype for each class. SAPENet utilizes multi-head self-attention mechanisms to selectively augment discriminative features in each sample feature map, and generates channel attention maps between intra-class sample features to attentively retain informative channel features for that class. The augmented feature maps and attention maps are finally fused to obtain representative class prototypes. Thereafter, a local descriptor-based metric module is employed to fully exploit the channel information of the prototypes by searching k similar local descriptors of the prototype for each local descriptor in the unlabeled samples for classification. We performed experiments on multiple benchmark datasets: miniImageNet, tieredImageNet, and CUB-200-2011. The experimental results on these datasets show that SAPENet achieves a considerable improvement compared to Prototypical Networks and also outperforms related state-of-the-art methods.}
}
@article{GUTIERREZLOPEZ2023109158,
title = {Optimum Bayesian thresholds for rebalanced classification problems using class-switching ensembles},
journal = {Pattern Recognition},
volume = {135},
pages = {109158},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109158},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006379},
author = {Aitor Gutiérrez-López and Francisco-Javier González-Serrano and Aníbal R. Figueiras-Vidal},
keywords = {Bayesian framework, Ensembles, Rebalancing techniques, Imbalanced classification, Label switching},
abstract = {Asymmetric label switching is an effective and principled method for creating a diverse ensemble of learners for imbalanced classification problems. This technique can be combined with other rebalancing mechanisms, such as those based on cost policies or class proportion modifications. In this study, and under the Bayesian theory framework, we specify the optimal decision thresholds for the combination of these mechanisms. In addition, we propose using a gating network to aggregate the learners contributions as an additional mechanism to improve the overall performance of the system.}
}
@article{ZHANG2022108833,
title = {Visual-to-EEG cross-modal knowledge distillation for continuous emotion recognition},
journal = {Pattern Recognition},
volume = {130},
pages = {108833},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108833},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003144},
author = {Su Zhang and Chuangao Tang and Cuntai Guan},
keywords = {Continuous emotion recognition, Knowledge distillation, Cross-modality},
abstract = {Visual modality is one of the most dominant modalities for current continuous emotion recognition methods. Compared to which the EEG modality is relatively less sound due to its intrinsic limitation such as subject bias and low spatial resolution. This work attempts to improve the continuous prediction of the EEG modality by using the dark knowledge from the visual modality. The teacher model is built by a cascade convolutional neural network - temporal convolutional network (CNN-TCN) architecture, and the student model is built by TCNs. They are fed by video frames and EEG average band power features, respectively. Two data partitioning schemes are employed, i.e., the trial-level random shuffling (TRS) and the leave-one-subject-out (LOSO). The standalone teacher and student can produce continuous prediction superior to the baseline method, and the employment of the visual-to-EEG cross-modal KD further improves the prediction with statistical significance, i.e., p-value <0.01 for TRS and p-value <0.05 for LOSO partitioning. The saliency maps of the trained student model show that the brain areas associated with the active valence state are not located in precise brain areas. Instead, it results from synchronized activity among various brain areas. And the fast beta and gamma waves, with the frequency of 18−30Hz and 30−45Hz, contribute the most to the human emotion process compared to other bands. The code is available at https://github.com/sucv/Visual_to_EEG_Cross_Modal_KD_for_CER.}
}
@article{VAQUERO2023109141,
title = {Real-time siamese multiple object tracker with enhanced proposals},
journal = {Pattern Recognition},
volume = {135},
pages = {109141},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109141},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006215},
author = {Lorenzo Vaquero and Víctor M. Brea and Manuel Mucientes},
keywords = {Multiple visual object tracking, Siamese CNN, Motion estimation},
abstract = {Maintaining the identity of multiple objects in real-time video is a challenging task, as it is not always feasible to run a detector on every frame. Thus, motion estimation systems are often employed, which either do not scale well with the number of targets or produce features with limited semantic information. To solve the aforementioned problems and allow the tracking of dozens of arbitrary objects in real-time, we propose SiamMOTION. SiamMOTION includes a novel proposal engine that produces quality features through an attention mechanism and a region-of-interest extractor fed by an inertia module and powered by a feature pyramid network. Finally, the extracted tensors enter a comparison head that efficiently matches pairs of exemplars and search areas, generating quality predictions via a pairwise depthwise region proposal network and a multi-object penalization module. SiamMOTION has been validated on five public benchmarks, achieving leading performance against current state-of-the-art trackers. Code available at: https://www.github.com/lorenzovaquero/SiamMOTION}
}
@article{SUN2022108788,
title = {Novel hyperbolic clustering-based band hierarchy (HCBH) for effective unsupervised band selection of hyperspectral images},
journal = {Pattern Recognition},
volume = {130},
pages = {108788},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108788},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002692},
author = {He Sun and Lei Zhang and Jinchang Ren and Hua Huang},
keywords = {Hyperspectral image, Unsupervised band selection, Hyperbolic space clustering, Hierarchical clustering},
abstract = {For dimensionality reduction of HSI, many clustering-based unsupervised band selection (UBS) methods have been proposed due to their superiority of reducing the high redundancy between selected bands. However, most of these methods fail to reflect the data structure of HSI, leading to inconsistent results of band selection. To tackle this particular issue, we have proposed a novel hyperbolic clustering-based band hierarchy (HCBH) to fully represent the underlying spectral structure and obtain a more consistent band selection. With the proposed adaptive hyperbolic clustering, the performance can be effectively improved with the aid of geometrical information. By introducing a cluster-centre based ranking metric, the desired band subset can be naturally obtained during the clustering process. Experimental results on three popularly used datasets have validated the superior performance of the proposed approach, which outperforms a few state-of-the-art (SOTA) UBS approaches.}
}
@article{HEZAM2023109186,
title = {COVID-19 and Rumors: A Dynamic Nested Optimal Control Model},
journal = {Pattern Recognition},
volume = {135},
pages = {109186},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109186},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006653},
author = {Ibrahim M. Hezam and Abdulkarem Almshnanah and Ahmed A. Mubarak and Amrit Das and Abdelaziz Foul and Adel Fahad Alrasheedi},
keywords = {COVID-19, genetic algorithm, KKT, nested optimal control, rumors},
abstract = {Unfortunately, the COVID-19 outbreak has been accompanied by the spread of rumors and depressing news. Herein, we develop a dynamic nested optimal control model of COVID-19 and its rumor outbreaks. The model aims to curb the epidemics by reducing the number of individuals infected with COVID-19 and reducing the number of rumor-spreaders while minimizing the cost associated with the control interventions. We use the modified approximation Karush–Kuhn–Tucker conditions with the Hamiltonian function to simplify the model before solving it using a genetic algorithm. The present model highlights three prevention measures that affect COVID-19 and its rumor outbreaks. One represents the interventions to curb the COVID-19 pandemic. The other two represent interventions to increase awareness, disseminate the correct information, and impose penalties on the spreaders of false rumors. The results emphasize the importance of interventions in curbing the spread of the COVID-19 pandemic and its associated rumor problems alike.}
}
@article{QIAN2022108796,
title = {3D Object Detection for Autonomous Driving: A Survey},
journal = {Pattern Recognition},
volume = {130},
pages = {108796},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108796},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002771},
author = {Rui Qian and Xin Lai and Xirong Li},
keywords = {3D object detection, Autonomous driving, Point clouds},
abstract = {Autonomous driving is regarded as one of the most promising remedies to shield human beings from severe crashes. To this end, 3D object detection serves as the core basis of perception stack especially for the sake of path planning, motion prediction, and collision avoidance etc.. Taking a quick glance at the progress we have made, we attribute challenges to visual appearance recovery in the absence of depth information from images, representation learning from partially occluded unstructured point clouds, and semantic alignments over heterogeneous features from cross modalities. Despite existing efforts, 3D object detection for autonomous driving is still in its infancy. Recently, a large body of literature have been investigated to address this 3D vision task. Nevertheless, few investigations have looked into collecting and structuring this growing knowledge. We therefore aim to fill this gap in a comprehensive survey, encompassing all the main concerns including sensors, datasets, performance metrics and the recent state-of-the-art detection methods, together with their pros and cons. Furthermore, we provide quantitative comparisons with the state of the art. A case study on fifteen selected representative methods is presented, involved with runtime analysis, error analysis, and robustness analysis. Finally, we provide concluding remarks after an in-depth analysis of the surveyed works and identify promising directions for future work.}
}
@article{TAN2023109112,
title = {A label distribution manifold learning algorithm},
journal = {Pattern Recognition},
volume = {135},
pages = {109112},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109112},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005921},
author = {Chao Tan and Sheng Chen and Xin Geng and Genlin Ji},
keywords = {Multi-label learning, Label distribution learning, Manifold learning, Dimension reduction, Linear regression},
abstract = {In this paper, we propose a novel label distribution manifold learning (LDML) method for solving the multilabel distribution learning problem. First, using manifold learning, we extract the accurate and reduced-dimension features of the training data. Second, we estimate the unknown label distributions associated with the extracted reduced-dimension features based on multi-output kernel regression. Third, we use the extracted reduced-dimension features and their associated estimated label distributions to form an enhanced maximum entropy model, which enables us to accurately and efficiently estimate the unknown true label distributions for the training data. We refer to this algorithm as the LDML. We also propose to apply the tangent space alignment regression in the second stage, and the resulting algorithm is called the LDML-R. The LDML-R has better label distribution learning performance than the LDML but imposes higher complexity than the latter. We evaluate the proposed LDML and LDML-R algorithms on 15 real-world data sets with ground-truth label distributions, and the experimental results obtained show that our method has advantages in terms of learning accuracy compared to the latest multi-label distribution learning approaches. We also use another 10 real-world multi-class data sets, which do not have the ground-truth label distributions, to demonstrate the superior multilabel classification performance of our LDML-R algorithm over the existing state-of-the-art multi-label classification algorithms.}
}
@article{WANG2023109166,
title = {M-CBN: Manifold constrained joint image dehazing and super-resolution based on chord boosting network},
journal = {Pattern Recognition},
volume = {135},
pages = {109166},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109166},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006458},
author = {Pengyu Wang and Hongqing Zhu and Han Zhang and Nan Wang},
keywords = {Image dehazing, Chord boosting network, Super-resolution, Frequency feature cross-collaboration, Manifold constraint},
abstract = {This paper proposes a Manifold Constrained Chord Boosting Network (M-CBN), which incorporates the super-resolution principle to achieve image dehazing. M-CBN is a task-specific image restoration network that explicitly learns the mapping from Low-resolution (LR) hazy images to High-resolution (HR) haze-free images. Hence, we design a preliminary image degradation to imitate super-resolution training on hazy images. In M-CBN, a plug-and-play Cross-linked Dual Projection Module (CDPM) for skip connections is developed. In CDPM, back-projections for HR encoder features and LR decoder features are cross-linked for better recovery of spatial information, and a cross-resolution spatial attention is designed to enhance fusion features. Then, to boost the generation of image details and textures, we propose a Chord Residual Module (CRM), which can separately process High-frequency (HF) and Low-frequency (LF) features by progressive inner-frequency updating and dense inter-frequency cross-collaboration to enhance decoding features. Finally, a manifold constraint dual discriminator is established. The static discriminator explicitly constrains dehazed images in the expected manifold to unify the joint learning of image dehazing and super-resolution. And the dynamic discriminator implicitly optimizes the network by adversarial training. Extensive experiments on general, dense and non-homogeneous haze datasets and cross-domain dehazing tasks show the proposed M-CBN presents high-quality dehazed results with natural colors and clear details.}
}
@article{CHEN2022108760,
title = {Discrete curve model for non-elastic shape analysis on shape manifold},
journal = {Pattern Recognition},
volume = {130},
pages = {108760},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108760},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002412},
author = {Peng Chen and Xutao Li and Changxing Ding and Jianxing Liu and Ligang Wu},
keywords = {Discrete curve model, Non-elastic shape analysis, Shape manifold, Shape synthesis, Shape retrieval, Shape arithmetics},
abstract = {In this paper, we construct a novel finite dimensional shape manifold for shape analyses. Elements of the shape manifold are a set of discrete, planar, and closed curves, which stand for object boundaries and are represented by direction function. On this manifold, we use a set of N-dimensional Fourier basis to construct the tangent space of the shape manifold as a finite dimensional space. Furthermore, we construct the shape manifold as a Riemannian manifold, in which the Riemannian metric is interpreted as an l2 metric. Our method improves the performance of bending-only models in the issues of shape analysis including the shape synthesis, comparison, and statistic analysis. We evaluate the performance of the manifold via the following applications: 1) shape interpolation and extrapolation between curves, 2) shape retrieval on the Flavia leaf database, 3) shape synthesis using an estimated probability distribution on the manifold, and 4) a novel application named shape arithmetic. All the above experiments clearly demonstrate our approach achieves superior performance to state-of-the-art methods.}
}
@article{ZHANG2022108784,
title = {Self-supervised rigid transformation equivariance for accurate 3D point cloud registration},
journal = {Pattern Recognition},
volume = {130},
pages = {108784},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108784},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002655},
author = {Zhiyuan Zhang and Jiadai Sun and Yuchao Dai and Dingfu Zhou and Xibin Song and Mingyi He},
keywords = {Point cloud, Rigid transformation equivariance, Learned cost volume},
abstract = {Transformation equivariance has been widely investigated in 3D point cloud representation learning for more informative descriptors, which formulates the change of the representation with respect to the transformation of the input point clouds explicitly. In this paper, we extend this property to the task of 3D point cloud registration and propose a rigid transformation equivariance (RTE) for accurate 3D point cloud registration. Specifically, RTE formulates the change of the relative pose explicitly with respect to the rigid transformation of the input point clouds. To exploit RTE, we adopt a Siamese structure network with two shared registration branches. One focuses on the input pair of point clouds, and the other one focuses on the new pair achieved by applying two random rigid transformations to the input point clouds respectively. Since the change of the two output relative poses has been predicted according to RTE, a new additional self-supervised loss is obtained to supervise the training. This general network structure can be integrated with most learning-based point cloud registration frameworks easily to improve the performance. Our method adopts the state-of-the-art virtual point-based pipelines as our shared branches, in which we propose a data-driven matching based on learned cost volume (LCV) rather than traditional hand-crafted matching strategies. Experimental evaluations on both synthetic datasets and real datasets validate the effectiveness of our proposed framework. The source code will be made public.}
}
@article{ZHANG2022108821,
title = {LSRML: A latent space regularization based meta-learning framework for MR image segmentation},
journal = {Pattern Recognition},
volume = {130},
pages = {108821},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108821},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003028},
author = {Bo Zhang and Yunpeng Tan and Hui Wang and Zheng Zhang and Xiuzhuang Zhou and Jingyun Wu and Yue Mi and Haiwen Huang and Wendong Wang},
keywords = {Latent space regularization, Meta learning, Domain generalization, Domain discriminator, Multi-source domain adaptation},
abstract = {Data sources for medical image segmentation can be quite extensive, and models trained with data from a source domain may perform poorly on data from the target domain owing to domain shift issues. To overcome the impact of domain shift, we propose a novel meta-learning-based multi-source domain adaptation framework for medical image segmentation. Specifically, we designed a domain discriminator module to produce category prediction over the latent features, and an image reconstruction module to reconstruct the foreground and background of the target domain image separately. Furthermore, we constructed a large-scale multi-modal prostate dataset, which contained 495,902 magnetic resonance images of 419 cases, with prostate and lesion masks, as well as diagnostic descriptions for each patient. We evaluated our proposed method through extensive experiments using the proposed and the benchmark datasets. Experimental results show that our model achieves better segmentation and generalization performance compared to state-of-the-art approaches.}
}
@article{TANG2022108792,
title = {Learning attention-guided pyramidal features for few-shot fine-grained recognition},
journal = {Pattern Recognition},
volume = {130},
pages = {108792},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108792},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002734},
author = {Hao Tang and Chengcheng Yuan and Zechao Li and Jinhui Tang},
keywords = {Few-shot learning, Fine-grained recognition, Weakly-supervised learning},
abstract = {Few-shot fine-grained recognition (FS-FGR) aims to distinguish several highly similar objects from different sub-categories with limited supervision. However, traditional few-shot learning solutions typically exploit image-level features and are committed to capturing global silhouettes while accidentally ignore to exploring local details, resulting in an inevitable problem of inconspicuous but distinguishable information loss. Thus, how to effectively address the fine-grained recognition issue given limited samples still remains a major challenging. In this article, we tend to propose an effective bidirectional pyramid architecture to enhance internal representations of features to cater to fine-grained image recognition task in the few-shot learning scenario. Specifically, we deploy a multi-scale feature pyramid and a multi-level attention pyramid on the backbone network, and progressively aggregated features from different granular spaces via both of them. We then further present an attention-guided refinement strategy in collaboration with a multi-level attention pyramid to reduce the uncertainty brought by backgrounds conditioned by limited samples. In addition, the proposed method is trained with the meta-learning framework in an end-to-end fashion without any extra supervision. Extensive experimental results on four challenging and widely-used fine-grained benchmarks show that the proposed method performs favorably against state-of-the-arts, especially in the one-shot scenarios.}
}
@article{LIU2022108829,
title = {CVM-Cervix: A hybrid cervical Pap-smear image classification framework using CNN, visual transformer and multilayer perceptron},
journal = {Pattern Recognition},
volume = {130},
pages = {108829},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108829},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003107},
author = {Wanli Liu and Chen Li and Ning Xu and Tao Jiang and Md Mamunur Rahaman and Hongzan Sun and Xiangchen Wu and Weiming Hu and Haoyuan Chen and Changhao Sun and Yudong Yao and Marcin Grzegorzek},
keywords = {Convolutional neural network, Visual transformer, Multilayer perceptron, Cervical cell classification, Pap smear, Image classification},
abstract = {Cervical cancer is the seventh most common cancer among all the cancers worldwide and the fourth most common cancer among women. Cervical cytopathology image classification is an important method to diagnose cervical cancer. However, manual inspection is very troublesome, and experts are prone to make mistakes. The emergence of the automatic computer-aided diagnosis system solves this problem. This paper proposes a framework called CVM-Cervix based on deep learning to perform cervical cell classification tasks. It can analyze pap slides quickly and accurately. CVM-Cervix first proposes a Convolutional Neural Network module and a Visual Transformer module for local and global feature extraction respectively, then a Multilayer Perceptron module is designed to fuse the local and global features for the final classification. Experimental results show the effectiveness and potential of the proposed CVM-Cervix in the field of cervical Pap smear image classification. In addition, according to the practical needs of clinical work, we perform a lightweight post-processing to compress the model.}
}
@article{LIU2022108808,
title = {FastOPM—A practical method for partial match of time series},
journal = {Pattern Recognition},
volume = {130},
pages = {108808},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108808},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002898},
author = {Jixue Liu and Jiuyong Li and Lin Liu},
keywords = {Time series, Query processing, Global optimization, Partial match},
abstract = {In applications like stock markets, engineering, medicine, etc., a large amount of time series data has been collected. Interrogating the data for patterns is important for analysis like event prediction and event investigation. A fundamental operation to support such analysis is query processing. In this paper, we aim to efficiently find the optimal match of a query in a timeseries when the match is calculated based on the trend and allows points to be skipped from the middle and ends of the sequences. This problem requires global optimization. The solutions in the literature have prohibitively high time complexities and are not practical for long timeseries. Our method consists of three parts. The first part is an efficiency improvement algorithm called FastOPM which applies the Dijkstra algorithm to get the optimal solution in an efficient manner. The second part derives bounds for optimal solutions. The third part is an algorithm for efficiently searching the target timeseries for the best optimal match of a query. Our experiments show that our method is faster than the baseline method, the bounds are effective, and the search algorithm can identify the best optimal match efficiently. Overall, our algorithm effectively outperforms the state-of-the-art algorithms DTW and MASS in retrieving target segments.}
}
@article{TABAK2022108795,
title = {Distributional barycenter problem through data-driven flows},
journal = {Pattern Recognition},
volume = {130},
pages = {108795},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108795},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200276X},
author = {Esteban G. Tabak and Giulio Trigila and Wenjun Zhao},
keywords = {Optimal transport, Barycenter problem, Pattern visualization, Simulation, Generative models},
abstract = {A new method is proposed for the solution of the data-driven optimal transport barycenter problem and of the more general distributional barycenter problem that the article introduces. The distributional barycenter problem provides a conceptual and computational toolbox for central problems in pattern recognition, such as the simulation of conditional distributions, the construction of a representative for a family of distributions indexed by a covariate and a new class of data-based generative models. The method proposed improves on previous approaches based on adversarial games, by slaving the discriminator to the generator and minimizing the need for parameterizations. It applies not only to a discrete family of distributions, but to more general distributions conditioned to factors z of any cardinality and type. The methodology is applied to numerical examples, including an analysis of the MNIST data set with a new cost function that penalizes non-isometric maps.}
}
@article{YU2022108772,
title = {Auto-weighted sample-level fusion with anchors for incomplete multi-view clustering},
journal = {Pattern Recognition},
volume = {130},
pages = {108772},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108772},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002539},
author = {Xiao Yu and Hui Liu and Yuxiu Lin and Yan Wu and Caiming Zhang},
keywords = {Incomplete data, Multi-view clustering, Anchor, Auto-weighted, Large-scale},
abstract = {Aiming at solving the problem of clustering in the multi-view datasets which include samples with information missing in one or more views, incomplete multi-view clustering has received considerable attention. However, most studies can not get satisfying accuracy and efficiency when dealing with datasets in which a considerable number of instances are missing in partial views. To address this problem, a method named Auto-weighted Sample-level Fusion with Anchors for Incomplete Multi-view Clustering (ASA-IC) is proposed in this paper. It designs an auto-weighted sample-level fusion strategy, which realizes the optimized conversion from the individual instance-to-anchor similarity learning to the concensus instance-to-anchor similarity matrix construction. ASA-IC can not only handle incomplete samples and effectively explore the relationship between each instance and anchors, but also deal with various incomplete clustering situations and be applied in large-scale datasets as well. Besides, experiments on 5 complete datasets and 27 incomplete ones illustrate its effectiveness quantitatively and qualitatively.}
}
@article{SHEN2022108828,
title = {Classification for high-dimension low-sample size data},
journal = {Pattern Recognition},
volume = {130},
pages = {108828},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108828},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003090},
author = {Liran Shen and Meng Joo Er and Qingbo Yin},
keywords = {Binary linear classifier, Quadratic programming, Data piling, Covariance matrix},
abstract = {High-dimension and low-sample-size (HDLSS) data sets have posed great challenges to many machine learning methods. To deal with practical HDLSS problems, development of new classification techniques is highly desired. After the cause of the over-fitting phenomenon is identified, a new classification criterion for HDLSS data sets, termed tolerance similarity, is proposed to emphasize maximization of within-class variance on the premise of class separability. Leveraging on this criterion, a novel linear binary classifier, termed No-separated Data Maximum Dispersion classifier (NPDMD), is designed. The main idea of the NPDMD is to spread samples of two classes in a large interval in the respective positive or negative space along the projecting direction when the distance between the projection means for two classes is large enough. The salient features of the proposed NPDMD are: (1) The NPDMD operates well on HDLSS data sets; (2) The NPDMD solves the objective function in the entire feature space to avoid the data-piling phenomenon. (3) The NPDMD leverages on the low-rank property of the covariance matrix for HDLSS data sets to accelerate the computation speed. (4) The NPDMD is suitable for different real-word applications. (5) The NPDMD can be implemented readily using Quadratic Programming. Not only theoretical properties of the NPDMD have been derived, but also a series of evaluations have been conducted on one simulated and six real-world benchmark data sets, including face classification and mRNA classification. Experimental results and comprehensive studies demonstrate the superiority of the NPDMD in terms of correct classification rate, mean within-group correct classification rate and the area under the ROC curve.}
}
@article{HUANG2023109145,
title = {Exploring modality-shared appearance features and modality-invariant relation features for cross-modality person Re-IDentification},
journal = {Pattern Recognition},
volume = {135},
pages = {109145},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109145},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006240},
author = {Nianchang Huang and Jianan Liu and Yongjiang Luo and Qiang Zhang and Jungong Han},
keywords = {Cross-modality person Re-IDentification, Visible images, Thermal infrared images, Modality-shared appearance features, Modality-invariant relation features},
abstract = {Most existing cross-modality person Re-IDentification works rely on discriminative modality-shared features for reducing cross-modality variations and intra-modality variations. Despite their preliminary success, such modality-shared appearance features cannot capture enough modality-invariant discriminative information due to a massive discrepancy between RGB and IR images. To address this issue, on top of appearance features, we further capture the modality-invariant relations among different person parts (referred to as modality-invariant relation features), which help to identify persons with similar appearances but different body shapes. To this end, a Multi-level Two-streamed Modality-shared Feature Extraction (MTMFE) sub-network is designed, where the modality-shared appearance features and modality-invariant relation features are first extracted in a shared 2D feature space and a shared 3D feature space, respectively. The two features are then fused into the final modality-shared features such that both cross-modality variations and intra-modality variations can be reduced. Besides, a novel cross-modality center alignment loss is proposed to further reduce the cross-modality variations. Experimental results on several benchmark datasets demonstrate that our proposed method exceeds state-of-the-art algorithms by a wide margin.}
}
@article{ZHU2023109143,
title = {Shape robustness in style enhanced cross domain semantic segmentation},
journal = {Pattern Recognition},
volume = {135},
pages = {109143},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109143},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006161},
author = {Siyu Zhu and Yingjie Tian},
keywords = {Domain adaptation, Semantic segmentation, Transfer learning},
abstract = {This paper focuses on domain adaptation method based on style transfer. Previous methods based on style transfer pay attention to the transformation of texture features between domains and maintain semantic consistency to the greatest extent. However, these methods have different effects on domain gaps in different types of categories. The categories with large texture difference and small structure difference can be improved better. For the categories with small texture difference and large structure difference, it causes negative transfer. In this paper, a shape robustness enhanced domain adaptive segmentation algorithm is proposed. Firstly, we adopt adjustable style transfer methods to enhance the style diversity of source domain images. Next, we differentiated different types of image features to weaken the negative transfer in the process of adversarial training. The results of this paper on general data sets GTA5 and SYNTHIA are better than other style transfer methods. Further experiments show that we improve the shape robustness of style enhancement method in domain adaptive segmentation task.}
}
@article{XU2023109149,
title = {Event-driven daily activity recognition with enhanced emergent modeling},
journal = {Pattern Recognition},
volume = {135},
pages = {109149},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109149},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006288},
author = {Zimin Xu and Guoli Wang and Xuemei Guo},
keywords = {Emergent paradigm, Marker-based stigmergy, Directed-weighted network, Activity modeling, Daily activity recognition},
abstract = {With the population aging, elderly health monitoring is triggering more studies on daily activity recognition as the fundamental of ambient assisted living. It is remarkable that activity recognition remains difficulties including how to adequately extract feature structure and settle the issue of activity confusion. To address these challenges, we propose a novel activity modeling method under the emergent paradigm with marker-based stigmergy and the directed-weighted network with additional context-aware information. In the modeling process, stigmergy is first introduced to aggregate the context information at the low level for generating activity pheromone trails, and then the constructed stigmergic trails are represented in form of directed-weighted network with distinguishability of individual pheromone source corresponding to location. The potential advantage is that the robust trails with distinguishable individual initial positions are feasible to supplement user’s daily habits and thus both inter-class and intra-class distances can be kept at acceptable levels. Experiments on Aruba demonstrates that the proposed emergent modeling method can effectively deal with the problems of feature extraction and activity ambiguity and achieve good classification performance.}
}
@article{JIANG2023109169,
title = {Robust low tubal rank tensor completion via factor tensor norm minimization},
journal = {Pattern Recognition},
volume = {135},
pages = {109169},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109169},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006483},
author = {Wei Jiang and Jun Zhang and Changsheng Zhang and Lijun Wang and Heng Qi},
keywords = {Low tubal rank tensor completion, Schatten- norm, Tensor double nuclear norm, Tensor frobenius/nuclear norm},
abstract = {Recent research has demonstrated that low tubal rank recovery based on tensor has received extensive attention. In this correspondence, we define tensor double nuclear norm and tensor Frobenius/nuclear hybrid norm to induce a surrogate for tensor tubal rank, and prove that they are equivalent to tensor Schatten-p norm for p=1/2 and p=2/3. Based on the definition, we propose two novel tractable tensor completion models called Double Nuclear norm regularized Tensor Completion (DNTC) and Frobenius/Nuclear hybrid norm regularized Tensor Completion (FNTC) by integrating these two norm minimization and factorization methods into a joint learning framework. Furthermore, we adopt invertible linear transforms to obtain low tubal rank tensors, which makes the model more flexible and effective. Two efficient algorithms are designed to solve the proposed tensor completion models by incorporating the convexity of the factor norms. Comprehensive experiments are conducted on synthetic and real datasets to achieve better results in comparison with some state-of-the-art approaches.}
}
@article{TANG2022108787,
title = {Contrastive author-aware text clustering},
journal = {Pattern Recognition},
volume = {130},
pages = {108787},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108787},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002680},
author = {Xudong Tang and Chao Dong and Wei Zhang},
keywords = {Text clustering, Contrastive learning, Representation learning},
abstract = {In the era of User Generated Content (UGC), authors (IDs) of texts widely exist and play a key role in determining the topic categories of texts. Existing text clustering efforts are mainly attributed to utilizing textual information, but the effect of authors on text clustering remains largely underexplored. To mitigate this issue, we propose a novel Contrastive Author-aware Text clustering approach, dubbed as CAT. CAT injects author information not only in characterizing texts through representations but also in pushing or pulling text representations of different authors through contrastive learning, which is rarely adopted by text clustering. Specifically, the developed contrastive learning method conducts both cluster-instance contrast by the text representation augmentation and instance-instance contrast by the multi-view representations. We perform comprehensive experiments on three public datasets, demonstrating that CAT largely outperforms strong competitive text clustering baselines and validating the effectiveness of the CAT’s main components.}
}
@article{HU2022108824,
title = {Model scheduling and sample selection for ensemble adversarial example attacks},
journal = {Pattern Recognition},
volume = {130},
pages = {108824},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108824},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003053},
author = {Zichao Hu and Heng Li and Liheng Yuan and Zhang Cheng and Wei Yuan and Ming Zhu},
keywords = {Adversarial example, Black-box attack, Model scheduling, Sample selection},
abstract = {Adversarial examples refer to the malicious inputs that can mislead deep neural networks (DNNs) to falsely classify them. In practice, some adversarial examples are transferable and hence can deceive different target models. In multi-stage ensemble adversarial example attacks, adversaries can generate strongly transferable adversarial examples through iteratively perturbing legitimate examples to attack well-trained source models in a white-box manner. Limited by computational and memory resources (e.g., GPU memory), however, adversaries cannot handle all models and all legitimate examples at a time. This brings an important but never studied research issue: how to optimally schedule source models and appropriately select samples to improve adversarial example transferability and reduce unnecessary computational overheads? To shed light on this problem, we develop a novel multi-stage ensemble adversarial example attack method based on our proposed strategies of model scheduling and sample selection. The first strategy schedules source models to be attacked in every stage, based on the criteria of decision boundary similarity and model diversity. The second selects input samples to be handled by ensemble attacks, according to their sensitivity level for adversarial perturbations. To our knowledge, we are the first to study model scheduling and sample selection for multi-stage ensemble attacks. We conduct extensive experiments on three datasets with a variety of source and target models. Experiments show that our model scheduling based ensemble attack outperforms the all-model ensemble attack and the state-of-the-art ensemble attacks SCES, SMBEA and EnsembleFool in transferability. Moreover, our sample selection strategy improves attack success rate by about 138%.}
}
@article{DUAN2023109140,
title = {Iterative embedding distillation for open world vehicle recognition},
journal = {Pattern Recognition},
volume = {135},
pages = {109140},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109140},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006203},
author = {Junxian Duan and Xiang Wu and Yibo Hu and Chaoyou Fu and Zi Wang and Ran He},
keywords = {Vehicle reidentification, Iterative embedding distillation},
abstract = {Vehicle recognition poses a practical but challenging problem in many real-world surveillance applications. Since vehicle recognition is an open-set problem, it is a critical issue to learn a discriminative visual embedding space rather than a well-performing classifier. In this paper, we propose an iterative embedding distillation (IED) framework for open-set vehicle recognition. The soft target in knowledge distillation is utilized to establish the interclass relations from an instance level rather than a category level. Towards the open-set problem, we extend knowledge distillation to embedding distillation in an iterative learning way, in which three types of loss functions are studied to iteratively transfer the distributions of embeddings from the teacher network to the student network. To demonstrate the universal nature of IED, we implement the IED framework on two basic convolutional neural networks and verify it using the cross-dataset testing protocols without retraining or fine-tuning. Extensive experimental results show that IED obtains quite encouraging results and outperforms state-of-the-art methods on various large-scale vehicle recognition datasets including VeRi-776, Vehicle-ID, Vehicle-1M, VD1 and VD2.}
}
@article{PU2022108832,
title = {Learning a deep dual-level network for robust DeepFake detection},
journal = {Pattern Recognition},
volume = {130},
pages = {108832},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108832},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003132},
author = {Wenbo Pu and Jing Hu and Xin Wang and Yuezun Li and Shu Hu and Bin Zhu and Rui Song and Qi Song and Xi Wu and Siwei Lyu},
keywords = {DeepFake detection, Multitask learning, Imbalanced learning, AUC optimization},
abstract = {Face manipulation techniques, especially DeepFake techniques, are causing severe social concerns and security problems. When faced with skewed data distributions such as those found in the real world, existing DeepFake detection methods exhibit significantly degraded performance, especially the AUC score. In this paper, we focus on DeepFake detection in real-world situations. We propose a dual-level collaborative framework to detect frame-level and video-level forgeries simultaneously with a joint loss function to optimize both the AUC score and error rate at the same time. Our experiments indicate that the AUC loss boosts imbalanced learning performance and outperforms focal loss, a state-of-the-art loss function to address imbalanced data. In addition, our multitask structure enables mutual reinforcement of frame-level and video-level detection and achieves outstanding performance in imbalanced learning. Our proposed method is also more robust to video quality variations and shows better generalization ability in cross-dataset evaluations than existing DeepFake detection methods. Our implementation is available online at https://github.com/PWB97/Deepfake-detection.}
}
@article{CHEN2023109185,
title = {LPCL: Localized prominence contrastive learning for self-supervised dense visual pre-training},
journal = {Pattern Recognition},
volume = {135},
pages = {109185},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109185},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006641},
author = {Zihan Chen and Hongyuan Zhu and Hao Cheng and Siya Mi and Yu Zhang and Xin Geng},
keywords = {Self-supervised learning, Contrastive learning, Dense representation},
abstract = {Self-supervised pre-training has attracted increasing attention given its promising performance in training backbone networks without using labels. By far, most methods focus on image classification with datasets containing iconic objects and simple background, e.g. ImageNet. However, these methods show sub-optimal performance for dense prediction tasks (e.g. object detection and scene parsing) when directly pre-training on datasets (e.g. PASCAL VOC and COCO) with multiple objects and cluttered backgrounds. Researchers explored self-supervised dense pre-training methods by adapting recent image pre-training methods. Nevertheless, they require a large number of negative samples and a long training time to reach reasonable performance. In this paper, we propose LPCL, a novel self-supervised representation learning method for dense predictions to settle these issues. To guide the instance information in multi-instance datasets, we define an online object patch selection module to select the local patches with the high possibility of containing instance area in the augmented views efficiently during learning. After obtaining the patches, we present a novel multi-level contrastive learning method considering the instance representation of global-level, local-level and position-level without using negative samples. We conduct extensive experiments with LPCL directly pre-trained on PASCAL VOC and COCO. For PASCAL VOC image classification task, our model achieves state-of-the-art 86.2% accuracy pre-trained on COCO(+9.7% top-1 accuracy compared with baseline BYOL). On object detection, instance segmentation and semantic segmentation task, our proposed model also achieved competitive results compared with other state-of-the-art methods.}
}
@article{SUN2023109157,
title = {A discriminatively deep fusion approach with improved conditional GAN (im-cGAN) for facial expression recognition},
journal = {Pattern Recognition},
volume = {135},
pages = {109157},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109157},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006367},
author = {Zhe Sun and Hehao Zhang and Jiatong Bai and Mingyang Liu and Zhengping Hu},
keywords = {Facial expression recognition, Discriminatively deep fusion approach, Improved conditional generative adversarial network, Discriminative loss function},
abstract = {Considering most deep learning-based methods heavily depend on huge labels, it is still a challenging issue for facial expression recognition to extract discriminative features of training samples with limited labels. Given above, we propose a discriminatively deep fusion (DDF) approach based on an improved conditional generative adversarial network (im-cGAN) to learn abstract representation of facial expressions. First, we employ facial images with action units (AUs) to train the im-cGAN to generate more labeled expression samples. Subsequently, we utilize global features learned by the global-based module and the local features learned by the region-based module to obtain the fused feature representation. Finally, we design the discriminative loss function (D-loss) that expands the inter-class variations while minimizing the intra-class distances to enhance the discrimination of fused features. Experimental results on JAFFE, CK+, Oulu-CASIA, and KDEF datasets demonstrate the proposed approach is superior to some state-of-the-art methods.}
}
@article{NAI2022108775,
title = {Dynamic feature fusion with spatial-temporal context for robust object tracking},
journal = {Pattern Recognition},
volume = {130},
pages = {108775},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108775},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002564},
author = {Ke Nai and Zhiyong Li and Haidong Wang},
keywords = {Object tracking, Dynamic feature fusion, Spatial-temporal context, Correlation filters framework},
abstract = {Feature fusion has been widely used for improving the tracking performance. However, how to effectively analyze the characteristics of different visual features to realize dynamical feature fusion is still a challenging task. In this paper, we propose a spatial-temporal context-based dynamic feature fusion method (STCDFF) with the correlation filters framework for object tracking. The proposed STCDFF method exploits spatial-temporal context to deeply analyze the characteristics of multiple visual features (e.g., HOG, Color-Names and CNN features) to perform feature fusion. On the one hand, spatial context is employed to evaluate the discriminative ability of different features to distinguish the target object from the background. On the other hand, temporal context is utilized to consider the representative ability of different features to capture significant appearance changes of the target object. The weight of a feature is decided by both its discriminative ability and representative ability. By exploring spatial-temporal context for feature fusion, the STCDFF method can fully utilize the strengths of different features to handle complex appearance changes and background clutters to achieve better performance. Extensive experiments on multiple object tracking datasets prove that our STCDFF method performs competitively against several popular tracking methods.}
}
@article{ZHANG2023109181,
title = {Large motion anime head animation using a cascade pose transform network},
journal = {Pattern Recognition},
volume = {135},
pages = {109181},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109181},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006604},
author = {Jiale Zhang and Chengxin Liu and Ke Xian and Zhiguo Cao},
keywords = {Talking head animation, Generative adversarial networks, Pose transformation, Anime image generation, Anime dataset},
abstract = {We study the problem of talking head animation from a single image where a target anime talking head is generated to mimic the change of facial expression and head movement of source anime characters. Most existing methods focus on generating talking heads from real humans. However, few efforts have been made to create anime talking head. Compared with human head generation, the key challenges of anime head generation are: how to align the pose and facial expression of the target head with that of the source head without explicit facial landmarks. To address this, we propose CPTNetV2, a cascaded pose transform network that unifies face pose transformation and head pose transformation. At the core of CPTNetV2 is the implicit encoding of facial changes and head movement by a pose vector. Given the pose vector, we introduce a mask generator to animate facial expression (e.g., close eyes and open mouth) and a grid generator to simulate head movement, followed by a fusion module to generate talking heads. To tackle large displacement and improve the quality of generation, we further design a details inpainting module with pose vector decomposition to reduce the receptive field of network required for pose transformation. In particular, we collect an anime talking head dataset AniHead-2K that includes around 2000 anime characters with different face/head poses. Extensive experiments on AniHead-2K demonstrate that CPTNetV2 can achieve arbitrary pose transformation conditioned on the target pose vector and outperforms other state-of-the-art methods. We also verify the effectiveness of each module through ablative studies. Additional results show that CPTNetV2 has good generalization and is applicable to generate anime talking head even based on human videos. The dataset will be made available at: https://github.com/zhangjiale487/AniHead-2K.}
}
@article{MANDEL2023109107,
title = {Detection confidence driven multi-object tracking to recover reliable tracks from unreliable detections},
journal = {Pattern Recognition},
volume = {135},
pages = {109107},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109107},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005878},
author = {Travis Mandel and Mark Jimenez and Emily Risley and Taishi Nammoto and Rebekka Williams and Max Panoff and Meynard Ballesteros and Bobbie Suarez},
keywords = {Multi-object tracking, Model uncertainty, Performance evaluation, Scarce data, Dataset, Marine science applications},
abstract = {Multi-object tracking (MOT) systems often rely on accurate object detectors; however, accurate detectors are not available in every application domain. We present Robust Confidence Tracking (RCT), an offline MOT algorithm designed for settings where detection quality is poor. Whereas prior methods simply threshold and discard detection confidence information, RCT relies on the exact detection confidence values to increase track quality throughout the entire tracking pipeline. This innovation (along with some simple and well-studied heuristics) allows RCT to achieve robust performance with minimal identity switches, even when provided with completely unfiltered detections. To compare trackers in the presence of unreliable detections, we present a challenging real-world underwater fish tracking dataset, FISHTRAC. In an large-scale evaluation across FISHTRAC, UA-DETRAC, and MOTChallenge data, RCT outperforms a wide variety of trackers, including deep trackers and more classic approaches. We have publically released our FISHTRAC codebase and training dataset at https://github.com/tmandel/fish-detrac, which will facilitate comparing trackers on understudied problems.}
}
@article{TAN2023109189,
title = {A Novel Label Enhancement Algorithm Based on Manifold Learning},
journal = {Pattern Recognition},
volume = {135},
pages = {109189},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109189},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006689},
author = {Chao Tan and Sheng Chen and Xin Geng and Genlin Ji},
keywords = {Multi-label learning, Label enhancement, Incremental subspace learning, Label propagation, Manifold learning, Conditional random field},
abstract = {We propose a label enhancement model to solve the multi-label learning (MLL) problem by using the incremental subspace learning to enrich the label space and to improve the ability of label recognition. In particular, we use the incremental estimation of the feature function representing the manifold structure to guide the construction of the label space and to transform the local topology from the feature space to the label space. First, we build a recursive form for incremental estimation of the feature function representing the feature space information. Second, the label propagation is used to obtain the hidden supervisory information of labels in the data. Finally, an enhanced maximum entropy model based on conditional random field is established as the objective, to obtain the predicted label distribution. The enriched label information in the manifold space obtained in first step and the estimated label distributions provided in second step are employed to train this enhanced maximum entropy model by a gradient-descent iterative optimization to obtain the label distribution predictor’s parameters with enhanced accuracy. We evaluate our method on 24 real-world datasets. Experimental results demonstrate that our label enhancement manifold learning model has advantages in predictive performance over the latest MLL methods.}
}
@article{CHIANG2022108807,
title = {A multi-embedding neural model for incident video retrieval},
journal = {Pattern Recognition},
volume = {130},
pages = {108807},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108807},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002886},
author = {Ting-Hui Chiang and Yi-Chun Tseng and Yu-Chee Tseng},
keywords = {Artificial intelligence, Computer vision, Deep metric learning, Incident video retrieval},
abstract = {Many internet search engines have been developed, however, the retrieval of video clips remains a challenge. This paper considers the retrieval of incident videos, which may contain more spatial and temporal semantics. We propose an encoder-decoder ConvLSTM model that explores multiple embeddings of a video to facilitate comparison of similarity between a pair of videos. The model is able to encode a video into an embedding that integrates both its spatial information and temporal semantics. Multiple video embeddings are then generated from coarse- and fine-grained features of a video to capture high- and low-level meanings. Subsequently, a learning-based comparative model is proposed to compare the similarity of two videos based on their embeddings. Extensive evaluations are presented and show that our model outperforms state-of-the-art methods for several video retrieval tasks on the FIVR-200K, CC_WEB_VIDEO, and EVVE datasets.}
}
@article{YOU2023109173,
title = {Unsupervised Feature Selection via Neural Networks and Self-Expression with Adaptive Graph Constraint},
journal = {Pattern Recognition},
volume = {135},
pages = {109173},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109173},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006525},
author = {Mengbo You and Aihong Yuan and Dongjian He and Xuelong Li},
keywords = {Unsupervised feature selection, Manifold structure, Adaptive graph constraint, Neural networks},
abstract = {Unsupervised feature selection (UFS), which selects the most important feature subset and eliminates the unnecessary information for the upcoming data analysis, is a significant problem in machine learning and has been explored for years. Most UFS methods map features into a pseudo label space by multiplying a projection matrix constrained with sparsity to learn the mapping from the features to the labels. However, the mapping relationship is usually not linear, and linear regression may result in a suboptimal selection. To address this issue, we propose a novel UFS method, called neural networks embedded self-expression (NNSE). NNSE replaces the linear regression of traditional spectral analysis methods with neural networks to learn the pseudo label space. Besides, we embed neural networks into the self-expression model to improve the representative ability by preserving the local structure with an adaptive graph regularization module. Then we propose an efficient alternative iterative algorithm to solve the proposed model. Experimental results on 8 public datasets show NNSE outperforms the other state-of-the-art methods. Moreover, experimental results are also presented to show the convergence of the proposed method. The source code is available at: https://github.com/misteru/NNSE.}
}
@article{DINESH2022108783,
title = {Fully convolutional Deep Stacked Denoising Sparse Auto encoder network for partial face reconstruction},
journal = {Pattern Recognition},
volume = {130},
pages = {108783},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108783},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002643},
author = {P.S. Dinesh and M. Manikandan},
keywords = {Partial face recognition, Deep learning algorithm, Fully convolutional network, Autoencoder},
abstract = {Face recognition is one of the most successful applications of image analysis. Since 1960s, automatic face recognition research has been carried out, but the problem is still unresolved. Therefore, in this manuscript, a novel Partial face reconstruction (PFR) algorithm called Self- motivated feature mapping (SMFM) combining a Fully Convolutional Network (FCN) and Deep Stacked Denoising Sparse Autoencoders (DS-DSA) algorithm is proposed to overcome the challenges. The proposed approach focuses on the generation of feature maps from the Fully Convolutional Network and it is used Deep Stacked Denoising Sparse Autoencoders to perform the partial face reconstruction. The spatial maps are generated by extracting the features from Fully Convolutional Network and it is supplied as the input for partial reconstruction and re-identification to the Deep Stacked Denoising Sparse Autoencoders network. The main aim of the proposed work is “to enhance the accuracy during facial reconstruction”. The proposed approach is implemented in MATLAB platform. The performance of the proposed approach attains 23.45% and 20.41% accuracy,25.93`% and 19.43% sensitivity, 22.21% and 24.41% precision and20.21% and 23.41% Specificity greater than the existing approaches, like Partial Face Reconstruction using generative adversarial networks (GANs), Partial Face Reconstruction using Deep Recurrent neural network (DRNN).}
}
@article{CHEN2022108739,
title = {Few-shot Website Fingerprinting attack with Meta-Bias Learning},
journal = {Pattern Recognition},
volume = {130},
pages = {108739},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108739},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002205},
author = {Mantun Chen and Yongjun Wang and Xiatian Zhu},
keywords = {User privacy, Internet anonymity, Data traffic, Website fingerprinting, Deep learning, Neural network, Few-shot learning, Meta-learning, Parameter factorization},
abstract = {Website fingerprinting (WF) attack aims to identify which website a user is visiting from the traffic data patterns. Whilst existing methods assume many training samples, we investigate a more realistic and scalable few-shot WF attack with only a few labeled training samples per website. To solve this problem, we introduce a novel Meta-Bias Learning (MBL) method for few-shot WF learning. Taking the meta-learning strategy, MBL simulates and optimizes the target tasks. Moreover, a new model parameter factorization idea is introduced for facilitating meta-training with superior task adaptation. Expensive experiments show that our MBL outperforms significantly existing hand-crafted feature and deep learning based alternatives in both closed-world and open-world attack scenarios, at the absence and presence of defense.}
}
@article{LIU2023109071,
title = {Center and Scale Prediction: Anchor-free Approach for Pedestrian and Face Detection},
journal = {Pattern Recognition},
volume = {135},
pages = {109071},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109071},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005519},
author = {Wei Liu and Irtiza Hasan and Shengcai Liao},
keywords = {Object Detection, Convolutional Neural Networks, Feature Detection, anchor-free, Anchor-free},
abstract = {Object detection traditionally requires sliding-window classifier in modern deep learning based approaches. However, both of these approaches requires tedious configurations in bounding boxes. Generally speaking, single-class object detection is to tell where the object is, and how big it is. Traditional methods combine the ”where” and ”how” subproblems into a single one through the overall judgement of various scales of bounding boxes. In view of this, we are interesting in whether the ”where” and ”how” subproblems can be separated into two independent subtasks to ease the problem definition and the difficulty of training. Accordingly, we provide a new perspective where detecting objects is approached as a high-level semantic feature detection task. Like edges, corners, blobs and other feature detectors, the proposed detector scans for feature points all over the image, for which the convolution is naturally suited. However, unlike these traditional low-level features, the proposed detector goes for a higher-level abstraction, that is, we are looking for central points where there are objects, and modern deep models are already capable of such a high-level semantic abstraction. Like blob detection, we also predict the scales of the central points, which is also a straightforward convolution. Therefore, in this paper, pedestrian and face detection is simplified as a straightforward center and scale prediction task through convolutions. This way, the proposed method enjoys an anchor-free setting, considerably reducing the difficulty in training configuration and hyper-parameter optimization. Though structurally simple, it presents competitive accuracy on several challenging benchmarks, including pedestrian detection and face detection. Furthermore, a cross-dataset evaluation is performed, demonstrating a superior generalization ability of the proposed method.}
}
@article{QIN2022108791,
title = {Enforced block diagonal subspace clustering with closed form solution},
journal = {Pattern Recognition},
volume = {130},
pages = {108791},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108791},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002722},
author = {Yalan Qin and Hanzhou Wu and Jian Zhao and Guorui Feng},
keywords = {Subspace clustering, General form, Analytical, Nonnegative, Symmetrical solution},
abstract = {Subspace clustering aims to fit each category of data points by learning an underlying subspace and then conduct clustering according to the learned subspace. Ideally, the learned subspace is expected to be block diagonal such that the similarities between clusters are zeros. In this paper, we provide the explicit theoretical connection between spectral clustering and the subspace clustering based on block diagonal representation. We propose Enforced Block Diagonal Subspace Clustering (EBDSC) and show that the spectral clustering with the Radial Basis Function kernel can be regarded as EBDSC. Compared with the exiting subspace clustering methods, an analytical, nonnegative and symmetrical solution can be obtained by EBDSC. An important difference with respect to the existing ones is that our model is a more general case. EBDSC directly uses the obtained solution as the similarity matrix, which can avoid the complex computation of the optimization program. Then the solution obtained by the proposed method can be used for the final clustering. Finally, we provide the experimental analysis to show the efficiency and effectiveness of our method on the synthetic data and several benchmark data sets in terms of different metrics.}
}
@article{LIU2023109184,
title = {Combining Deep Denoiser and Low-rank Priors for Infrared Small Target Detection},
journal = {Pattern Recognition},
volume = {135},
pages = {109184},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109184},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200663X},
author = {Ting Liu and Qian Yin and Jungang Yang and Yingqian Wang and Wei An},
keywords = {Low-rank prior, Deep denoiser prior, Infrared small target detection, Plug-and-play},
abstract = {Many existing low-rank methods have achieved good detection performance in uniform scenes, but they suffer from a high false alarm rate in complex noisy scenes. Therefore, it is important to improve the detection performance of low-rank models in noisy scenes. In this paper, we first formulate an implicit regularizer by plugging a denoising neural network (termed as deep denoiser), which can learn deep image priors from a large number of natural images. Then, we use the weighted sum of weighted tensor nuclear norm for more accurate background estimation. Finally, alternating direction multiplier method is used to solve the model under the plug-and-play framework. By integrating low-rank prior with deep denoiser prior, our model achieves higher accuracy. Experiments on different scenes demonstrate that our method achieves an improved performance in terms of visual effects and quantitative metrics. Specially, the overall accuracy of AUC value (AUCOA) achieved by the proposed method on Sequences 1-6 are 1.24%, 1.16%, 0.63%, 1.9%, 0.82%, 2.06% higher than those achieved by the second top performing methods, respectively.}
}
@article{SEGU2023109115,
title = {Batch normalization embeddings for deep domain generalization},
journal = {Pattern Recognition},
volume = {135},
pages = {109115},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109115},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005957},
author = {Mattia Segu and Alessio Tonioni and Federico Tombari},
keywords = {Domain generalization, Domain representation learning, Learning from multiple sources},
abstract = {Domain generalization aims at training machine learning models to perform robustly across different and unseen domains. Several methods train models from multiple datasets to extract domain-invariant features, hoping to generalize to unseen domains. Instead, first we explicitly train domain-dependent representations leveraging ad-hoc batch normalization layers to collect independent domain’s statistics. Then, we propose to use these statistics to map domains in a shared latent space, where membership to a domain is measured by means of a distance function. At test time, we project samples from an unknown domain into the same space and infer properties of their domain as a linear combination of the known ones. We apply the same mapping strategy at training and test time, learning both a latent representation and a powerful but lightweight ensemble model. We show a significant increase in classification accuracy over current state-of-the-art techniques on popular domain generalization benchmarks: PACS, Office-31 and Office-Caltech.}
}
@article{GAO2022108789,
title = {Time-varying Group Lasso Granger Causality Graph for High Dimensional Dynamic system},
journal = {Pattern Recognition},
volume = {130},
pages = {108789},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108789},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002709},
author = {Wei Gao and Haizhong Yang},
keywords = {Time-varying Granger causality, Feature selection, Group Lasso, Financial market network},
abstract = {Feature selection is a crucial preprocessing step in data analysis and machine learning. Since causal relationships imply the underlying mechanism of a system, causality-based feature selection methods have gradually attracted great attentions. For a high dimensional system undergoing dynamic transformation, because of the non-stationarity and sample scarcity, modeling the causal structure among these features is difficult. In this paper, we propose a time-varying Granger causal networks to capture the causal relations underlying high dimensional time-varying vector autoregressive models with high order lagged dependence. A kernel reweighted group lasso method is proposed, which overcomes the limitations of sample scarcity and transforms the problem of Granger causal structural learning into a group variable selection problem. The asymptotic consistency of the proposed algorithm is proved. We apply the time-varying Granger causal networks to simulation experiments and real data in the financial market. The study demonstrates that the method provides an efficient tool to detect changes and analysis characters of causal dependency structure in network evolution.}
}
@article{CHEN2023109168,
title = {Rethinking Local and Global Feature Representation for Dense Prediction},
journal = {Pattern Recognition},
volume = {135},
pages = {109168},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109168},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006471},
author = {Mohan Chen and Li Zhang and Rui Feng and Xiangyang Xue and Jianfeng Feng},
keywords = {Dense prediction, Vision transformer, Semantic segmentation, Depth estimation, Object detection},
abstract = {Although fully convolution networks (FCNs) have dominated dense prediction tasks (e.g., semantic segmentation, depth estimation and object detection) for decades, they are inherently limited in capturing long-range structured relationship with the layers of local kernels. While recent Transformer-based models have proven extremely successful in computer vision tasks by capturing global representation, they would deteriorate dense prediction results by over-smoothing the regions containing fine details (e.g., boundaries and small objects). To this end, we aim to provide an alternative perspective by rethinking local and global feature representation for the dense prediction task. Specifically, we deploy a Dual-Stream Convolution-Transformer architecture, called DSCT, by taking advantage of both the convolution and Transformer to learn a rich feature representation, combining with a task decoder to provide a powerful dense prediction model. DSCT extracts high resolution local feature representation from convolution layers and global feature representation from Transformer layers. With the local and global context modeled explicitly in every layer, the two streams can be combined with a decoder to perform task of semantic segmentation, monocular depth estimation or object detection. Extensive experiments show that DSCT can achieve superior performance on the three tasks above. For semantic segmentation, DSCT builds a new state of the art on Cityscapes validation set (83.31% mIoU) with only 80,000 training iterations and appealing performance (49.27% mIoU) on ADE20K validation set, outperforming most of the alternatives. For monocular depth estimation, our model achieves 2.423 RMSE on KITTI Eigen split, superior to most of the convolution or Transformer counterparts. For object detection, without using FPN, we can achieve 44.5% APb on COCO dataset when using Faster R-CNN, which is higher than Conformer.}
}
@article{PEI2023109148,
title = {Person-Specific Face Spoofing Detection Based on a Siamese Network},
journal = {Pattern Recognition},
volume = {135},
pages = {109148},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109148},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006276},
author = {Mingtao Pei and Bin Yan and Huiling Hao and Meng Zhao},
keywords = {Face spoofing detection, Identity information, Siamese network},
abstract = {Face spoofing detection is an essential prerequisite for face recognition applications. Previous face spoofing detection methods usually trained a binary classifier to classify the input face as a spoof face or a real face before face recognition, and client identity information was not utilized. In this paper, we propose a person-specific face spoofing detection method to employ client identity information for face spoofing detection. In our method, face spoofing is detected after face recognition rather than before face recognition; that is, the input face is recognized first, and the client identity is used to assist face spoofing detection. We train a deep Siamese network with image pairs. Each image pair consists of two real face images or one real and one spoof face image. The face images in each pair come from the same client. The deep Siamese network is trained by joint Bayesian loss together with contrastive loss and softmax loss. In testing, an input face image is recognized first, then the real face image of the identified client is retrieved, and an image pair is formed by the test face image and the retrieved real face image. The image pair is classified by the trained Siamese network to determine whether the input test image is a real face or not. The experimental results demonstrate the effectiveness of our method.}
}
@article{YANG2022108823,
title = {Asymmetric cross–modal hashing with high–level semantic similarity},
journal = {Pattern Recognition},
volume = {130},
pages = {108823},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108823},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003041},
author = {Fan Yang and Yufeng Liu and Xiaojian Ding and Fumin Ma and Jie Cao},
keywords = {Cross-modal retrieval, Hashing, Similarity search, Supervised, Optimization},
abstract = {Cross-modal hashing aims at using modality content to retrieve semantically relevant objects of different modalities, so cross-modal retrieval has attracted much attention. To effectively exploit the discriminative label information and retain more semantic information in the process of hash learning, we propose a novel cross-modal hashing method, named high-level semantic similarity analysis hashing (HSSAH) for cross-modal retrieval. To reduce time complexity and enhance discriminant ability in hash codes, HSSAH constructs an asymmetric high-level semantic similarity learning framework to replace the binary semantic similarity matrix. Moreover, the developed HSSAH is a two-stage approach, and a semantic-enhanced scheme is proposed in the second stage, which fully leverages the label information to gain more powerful hash functions. We conducted comprehensive experiments on three benchmark datasets to evaluate the performance of HSSAH. Experimental results show that HSSAH can achieve significantly better retrieval precision and outperforms several state-of-the-art approaches.}
}
@article{YU2023109131,
title = {Mix-ViT: Mixing attentive vision transformer for ultra-fine-grained visual categorization},
journal = {Pattern Recognition},
volume = {135},
pages = {109131},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109131},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006112},
author = {Xiaohan Yu and Jun Wang and Yang Zhao and Yongsheng Gao},
keywords = {Ultra-fine-grained visual categorization, Vision transformer, Self-supervised learning, Attentive mixing},
abstract = {Ultra-fine-grained visual categorization (ultra-FGVC) moves down the taxonomy level to classify sub-granularity categories of fine-grained objects. This inevitably poses a challenge, i.e., classifying highly similar objects with limited samples, which impedes the performance of recent advanced vision transformer methods. To that end, this paper introduces Mix-ViT, a novel mixing attentive vision transformer to address the above challenge towards improved ultra-FGVC. The core design is a self-supervised module that mixes the high-level sample tokens and learns to predict whether a token has been substituted after attentively substituting tokens. This drives the model to understand the contextual discriminative details among inter-class samples. Via incorporating such a self-supervised module, the network gains more knowledge from the intrinsic structure of input data and thus improves generalization capability with limited training sample. The proposed Mix-ViT achieves competitive performance on seven publicly available datasets, demonstrating the potential of vision transformer compared to CNN for the first time in addressing the challenging ultra-FGVC tasks. The code is available at https://github.com/Markin-Wang/MixViT}
}
@article{2023109221,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {135},
pages = {109221},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(22)00700-2},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322007002}
}
@article{FANG2023109139,
title = {M2RNet: Multi-modal and multi-scale refined network for RGB-D salient object detection},
journal = {Pattern Recognition},
volume = {135},
pages = {109139},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109139},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006197},
author = {Xian Fang and Mingfeng Jiang and Jinchao Zhu and Xiuli Shao and Hongpeng Wang},
keywords = {Saliency detection, Deep learning, Multi-modal feature, Multi-scale feature, Loss function},
abstract = {Salient object detection is a fundamental topic in computer vision, which has promising application prospects. The previous methods based on RGB-D may potentially suffer from the incompatibility of multi-modal feature fusion and the insufficiency of multi-scale feature aggregation. To tackle these two dilemmas, we propose a novel multi-modal and multi-scale refined network (M2RNet). Specifically, three essential components are presented in this network. The nested dual attention module (NDAM) explicitly exploits the combined features of RGB and depth flows. The adjacent interactive aggregation module (AIAM) gradually integrates the neighbor features of high, middle and low levels. The joint hybrid optimization loss (JHOL) makes the predictions have a prominent outline. Extensive experiments quantitatively and qualitatively demonstrate that our method outperforms other state-of-the-art approaches.}
}
@article{CHEN2022108827,
title = {GasHis-Transformer: A multi-scale visual transformer approach for gastric histopathological image detection},
journal = {Pattern Recognition},
volume = {130},
pages = {108827},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108827},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003089},
author = {Haoyuan Chen and Chen Li and Ge Wang and Xiaoyan Li and Md {Mamunur Rahaman} and Hongzan Sun and Weiming Hu and Yixin Li and Wanli Liu and Changhao Sun and Shiliang Ai and Marcin Grzegorzek},
keywords = {Gastric histropathological image, Multi-scale visual transformer, Image detection},
abstract = {In this paper, a multi-scale visual transformer model, referred as GasHis-Transformer, is proposed for Gastric Histopathological Image Detection (GHID), which enables the automatic global detection of gastric cancer images. GasHis-Transformer model consists of two key modules designed to extract global and local information using a position-encoded transformer model and a convolutional neural network with local convolution, respectively. A publicly available hematoxylin and eosin (H&E) stained gastric histopathological image dataset is used in the experiment. Furthermore, a Dropconnect based lightweight network is proposed to reduce the model size and training time of GasHis-Transformer for clinical applications with improved confidence. Moreover, a series of contrast and extended experiments verify the robustness, extensibility and stability of GasHis-Transformer. In conclusion, GasHis-Transformer demonstrates high global detection performance and shows its significant potential in GHID task.}
}
@article{HUANG2022108818,
title = {Hippocampus-heuristic character recognition network for zero-shot learning in Chinese character recognition},
journal = {Pattern Recognition},
volume = {130},
pages = {108818},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108818},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002990},
author = {Guanjie Huang and Xiangyu Luo and Shaowei Wang and Tianlong Gu and Kaile Su},
keywords = {Chinese character recognition, Hippocampus thinking, Radical analysis, Zero-shot learning, Label embedding},
abstract = {The recognition of Chinese characters has always been a challenging task due to their huge variety and complex structures. The current radical-based methods fail to recognize Chinese characters without learning all of their radicals in the training stage. To this end, we propose a novel Hippocampus-heuristic Character Recognition Network (HCRN), which can recognize unseen Chinese characters only by training part of radicals. More specifically, the network architecture of HCRN is a new pseudo-siamese network designed by us, which can learn features from pairs of input samples and use them to predict unseen characters. The experimental results on the recognition of printed and handwritten characters show that HCRN is robust and effective on zero/few-shot learning tasks. For the printed characters, the mean accuracy of HCRN outperforms the state-of-the-art approach by 23.93% on recognizing unseen characters. For the handwritten characters, HCRN improves the mean accuracy by 11.25% on recognizing unseen characters.}
}
@article{HE2023109188,
title = {Generalized minimum error entropy for robust learning},
journal = {Pattern Recognition},
volume = {135},
pages = {109188},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109188},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006677},
author = {Jiacheng He and Gang Wang and Kui Cao and He Diao and Guotai Wang and Bei Peng},
keywords = {Generalized Gaussian density, Generalized error entropy, Quantized generalized error entropy, Adaptive filtering, Kernel recursive least squares, Multilayer perceptron},
abstract = {The applications of error entropy (EE) are sometimes limited because its shape cannot be flexibly adjusted by the default Gaussian kernel function to adapt to noise variation and thus lowers the performance of algorithms based on minimum error entropy (MEE) criterion. In this paper, a generalized EE (GEE) is proposed by introducing the generalized Gaussian density (GGD) as its kernel function to improve the robustness of EE. In addition, GEE can be further improved to reduce its computational load by the quantized GEE (QGEE). Furthermore, two learning criteria, called generalized minimum error entropy (GMEE) and quantized generalized minimum error entropy (QGMEE), are developed based on GEE and QGEE, and new adaptive filtering (AF), kernel recursive least squares (KRLS), and multilayer perceptron (MLP) based on the proposed criteria are presented. Several numerical simulations indicate that the performance of proposed algorithms performs better than that of algorithms based on MEE.}
}
@article{SHI2023109180,
title = {JRA-Net: Joint representation attention network for correspondence learning},
journal = {Pattern Recognition},
volume = {135},
pages = {109180},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109180},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006598},
author = {Ziwei Shi and Guobao Xiao and Linxin Zheng and Jiayi Ma and Riqing Chen},
keywords = {Correspondences, Joint representation, Attention mechanism, Outlier rejection, Pose estimation},
abstract = {In this paper, we propose a Joint Representation Attention Network (JRA-Net), an end-to-end network, to establish reliable correspondences for image pairs. The initial correspondences generated by the local feature descriptor usually suffer from heavy outliers, which makes the network unable to learn a powerful enough representation for distinguishing inliers and outliers. To this end, we design a novel attention mechanism. The proposed attention mechanism not only takes into account the correlations between global context and geometric information, but also introduces the joint representation of different scales to suppress trivial correspondences and highlight crucial correspondences. In addition, to improve the generalization ability of attention mechanism, we present an innovative weight function, to effectively adjust the importance of the attention mechanism in a learning manner. Finally, by combining the above components, the proposed JRA-Net is able to effectively infer the probabilities of correspondences being inliers. Empirical experiments on challenging datasets demonstrate the effectiveness and generalization of JRA-Net. We achieve remarkable improvements compared with the current state-of-the-art approaches on outlier rejection and relative pose estimation.}
}
@article{PATIL2022108822,
title = {Dual-frame spatio-temporal feature modulation for video enhancement},
journal = {Pattern Recognition},
volume = {130},
pages = {108822},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108822},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200303X},
author = {Prashant W. Patil and Sunil Gupta and Santu Rana and Svetha Venkatesh},
keywords = {Multi-frame features, Spatio-temporal feature modulation, Recurrent feature sharing, Multi-weather video enhancement},
abstract = {Current video enhancement approaches have achieved good performance in specific rainy, hazy, foggy, and snowy weather conditions. However, they currently suffer from two important limitations. First, they can only handle degradation caused by single weather. Second, they use large, complex models with 10–50 millions of parameters needing high computing resources. As video enhancement is a pre-processing step for applications like video surveillance, traffic monitoring, autonomous driving, etc., it is necessary to have a lightweight enhancement module. Therefore, we propose a dual-frame spatio-temporal feature modulation architecture to handle the degradation caused by diverse weather conditions. The proposed architecture combines the concept of spatio-temporal multi-resolution feature modulation with a multi-receptive parallel encoders and domain-based feature filtering modules to learn domain-specific features. Further, the architecture provides temporal consistency with recurrent feature merging, achieved by providing feedback of the previous frame output. The indoor (REVIDE, NYUDepth), synthetically generated outdoor weather degraded video de-hazing, and de-raining with veiling effect databases are used for experimentation. Also, the performance of the proposed method is analyzed for night-time de-hazing and de-raining with veiling effect weather conditions. Experimental results show the superior performance of our framework compared to existing state-of-the-art methods used for video de-hazing (indoor/outdoor) and de-raining with veiling effect weather conditions. The code is available at https://github.com/pwp1208/PR2022}
}
@article{XU2023109152,
title = {Fast subspace clustering by learning projective block diagonal representation},
journal = {Pattern Recognition},
volume = {135},
pages = {109152},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109152},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006318},
author = {Yesong Xu and Shuo Chen and Jun Li and Chunyan Xu and Jian Yang},
keywords = {Subspace clustering, Block diagonal representation, Large-scale data},
abstract = {Block Diagonal Representation (BDR) has attracted massive attention in subspace clustering, yet the high computational cost limits its widespread application. To address this issue, we propose a novel approach called Projective Block Diagonal Representation (PBDR), which rapidly pursues a representation matrix with the block diagonal structure. Firstly, an effective sampling strategy is utilized to select a small subset of the original large-scale data. Then, we learn a projection mapping to match the block diagonal representation matrix on the selected subset. After training, we employ the learned projection mapping to quickly generate the representation matrix with an ideal block diagonal structure for the original large-scale data. Additionally, we further extend the proposed PBDR model (i.e., PBDRℓ1 and PBDR*) by capturing the global or local structure of the data to enhance block diagonal coding capability. This paper also proves the effectiveness of the proposed model theoretically. Especially, this is the first work to directly learn a representation matrix with a block diagonal structure to handle the large-scale subspace clustering problem. Finally, experimental results on publicly available datasets show that our approaches achieve faster and more accurate clustering results compared to the state-of-the-art block diagonal-based subspace clustering approaches, which demonstrates its practical usefulness.}
}
@article{LAN2022108819,
title = {Towards lifelong object recognition: A dataset and benchmark},
journal = {Pattern Recognition},
volume = {130},
pages = {108819},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108819},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003004},
author = {Chuanlin Lan and Fan Feng and Qi Liu and Qi She and Qihan Yang and Xinyue Hao and Ivan Mashkin and Ka Shun Kei and Dong Qiang and Vincenzo Lomonaco and Xuesong Shi and Zhengwei Wang and Yao Guo and Yimin Zhang and Fei Qiao and Rosa H.M. Chan},
keywords = {Robotic vision, Continual learning, Lifelong learning, Object recognition},
abstract = {Lifelong learning algorithms aim to enable robots to handle open-set and detrimental conditions, and yet there is a lack of adequate datasets with diverse factors for benchmarking. In this work, we constructed and released a lifelong learning robotic vision dataset, OpenLORIS-Object. This dataset was collected by RGB-D camera capturing dynamic environment in daily life scenarios with diverse factors, including illumination, occlusion, object pixel size and clutter, of quantified difficulty levels. To the best of our knowledge, this is an unique real-world dataset for robotic vision with independent and quantifiable environmental factors, which are currently unaccounted for in other lifelong learning datasets such as CORe50 and NICO. We tested 9 state-of-the-art algorithms with 4 evaluation metrics over the dataset in Domain Incremental Learning, Task Incremental Learning, and Class Incremental Learning scenarios. The results demonstrate that these existing algorithms are insufficient to handle lifelong learning task in dynamic environments. Our dataset and benchmarks are now publicly available at this website.22https://lifelong-robotic-vision.github.io/dataset/object}
}
@article{NOVAKOVIC2022108790,
title = {The CP‐ABM approach for modelling COVID‐19 infection dynamics and quantifying the effects of non‐pharmaceutical interventions},
journal = {Pattern Recognition},
volume = {130},
pages = {108790},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108790},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002710},
author = {Aleksandar Novakovic and Adele H. Marshall},
keywords = {COVID-19, Non pharmaceutical interventions, Change point detection, Agent based model, Genetic algorithm},
abstract = {The motivation for this research is to develop an approach that reliably captures the disease dynamics of COVID-19 for an entire population in order to identify the key events driving change in the epidemic through accurate estimation of daily COVID-19 cases. This has been achieved through the new CP-ABM approach which uniquely incorporates Change Point detection into an Agent Based Model taking advantage of genetic algorithms for calibration and an efficient infection centric procedure for computational efficiency. The CP-ABM is applied to the Northern Ireland population where it successfully captures patterns in COVID-19 infection dynamics over both waves of the pandemic and quantifies the significant effects of non-pharmaceutical interventions (NPI) on a national level for lockdowns and mask wearing. To our knowledge, there is no other approach to date that has captured NPI effectiveness and infection spreading dynamics for both waves of the COVID-19 pandemic for an entire country population.}
}
@article{BAGIROV2023109144,
title = {Finding compact and well-separated clusters: Clustering using silhouette coefficients},
journal = {Pattern Recognition},
volume = {135},
pages = {109144},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109144},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006239},
author = {Adil M. Bagirov and Ramiz M. Aliguliyev and Nargiz Sultanova},
keywords = {Cluster analysis, Cluster validity index, Silhouette coefficients, Nonsmooth optimization, Incremental algorithm},
abstract = {Finding compact and well-separated clusters in data sets is a challenging task. Most clustering algorithms try to minimize certain clustering objective functions. These functions usually reflect the intra-cluster similarity and inter-cluster dissimilarity. However, the use of such functions alone may not lead to the finding of well-separated and, in some cases, compact clusters. Therefore additional measures, called cluster validity indices, are used to estimate the true number of well-separated and compact clusters. Some of these indices are well-suited to be included into the optimization model of the clustering problem. Silhouette coefficients are among such indices. In this paper, a new optimization model of the clustering problem is developed where the clustering function is used as an objective and silhouette coefficients are used to formulate constraints. Then an algorithm, called CLUSCO (CLustering Using Silhouette COefficients), is designed to construct clusters incrementally. Three schemes are discussed to reduce the computational complexity of the algorithm. Its performance is evaluated using fourteen real-world data sets and compared with that of three state-of-the-art clustering algorithms. Results show that the CLUSCO is able to compute compact clusters which are significantly better separable in comparison with those obtained by other algorithms.}
}
@article{XING2022108806,
title = {Joint prediction of monocular depth and structure using planar and parallax geometry},
journal = {Pattern Recognition},
volume = {130},
pages = {108806},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108806},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002874},
author = {Hao Xing and Yifan Cao and Maximilian Biber and Mingchuan Zhou and Darius Burschka},
keywords = {Monocular depth estimation, Plane and parallax geometry, Structure information, Joint prediction model},
abstract = {Supervised learning depth estimation methods can achieve good performance when trained on high-quality ground-truth, like LiDAR data. However, LiDAR can only generate sparse 3D maps which causes losing information. Obtaining high-quality ground-truth depth data per pixel is difficult to acquire. In order to overcome this limitation, we propose a novel approach combining structure information from a promising Plane and Parallax geometry pipeline with depth information into a U-Net supervised learning network, which results in quantitative and qualitative improvement compared to existing popular learning-based methods. In particular, the model is evaluated on two large-scale and challenging datasets: KITTI Vision Benchmark and Cityscapes dataset and achieve the best performance in terms of relative error. Compared with pure depth supervision models, our model has impressive performance on depth prediction of thin objects and edges, and compared to structure prediction baseline, our model performs more robustly.}
}
@article{QINGYUN2022108786,
title = {Cross-modality attentive feature fusion for object detection in multispectral remote sensing imagery},
journal = {Pattern Recognition},
volume = {130},
pages = {108786},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108786},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002679},
author = {Fang Qingyun and Wang Zhaokui},
keywords = {Cross-modality, Attention, Feature fusion, Object detection, Multispectral remote sensing imagery},
abstract = {Cross-modality fusing complementary information of multispectral remote sensing image pairs can improve the perception ability of detection algorithms, making them more robust and reliable for a wider range of applications, such as nighttime detection. Compared with prior methods, we think different features should be processed specifically, the modality-specific features should be retained and enhanced, while the modality-shared features should be cherry-picked from the RGB and thermal IR modalities. Following this idea, a novel and lightweight multispectral feature fusion approach with joint common-modality and differential-modality attentions are proposed, named Cross-Modality Attentive Feature Fusion (CMAFF). Given the intermediate feature maps of RGB and thermal images, our module parallel infers attention maps from two separate modalities, common- and differential-modality, then the attention maps are multiplied to the input feature map respectively for adaptive feature enhancement or selection. Extensive experiments demonstrate that our proposed approach can achieve the state-of-the-art performance at a low computation cost.}
}
@article{WANG2022108794,
title = {The iterative convolution–thresholding method (ICTM) for image segmentation},
journal = {Pattern Recognition},
volume = {130},
pages = {108794},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108794},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002758},
author = {Dong Wang and Xiao-Ping Wang},
keywords = {Convolution, Thresholding, Image segmentation, Heat kernel},
abstract = {Variational methods, which have been tremendously successful in image segmentation, work by minimizing a given objective functional. The objective functional usually consists of a fidelity term and a regularization term. Because objective functionals may vary from different types of images, developing an efficient, simple, and general numerical method to minimize them has become increasingly vital. However, many existing methods are model-based, converge relatively slowly, or involve complicated techniques. In this paper, we develop a novel iterative convolution–thresholding method (ICTM) that is simple, efficient, and applicable to a wide range of variational models for image segmentation. In ICTM, the interface between two different segment domains is implicitly represented by the characteristic functions of domains. The fidelity term is usually written into a linear functional of the characteristic functions, and the regularization term is approximated by a functional of characteristic functions in terms of heat kernel convolution. This allows us to design an iterative convolution–thresholding method to minimize the approximate energy. The method has the energy-decaying property, and thus the unconditional stability is theoretically guaranteed. Numerical experiments show that the method is simple, easy to implement, robust, and applicable to various image segmentation models.}
}
@article{CHEN2022108781,
title = {Residual objectness for imbalance reduction},
journal = {Pattern Recognition},
volume = {130},
pages = {108781},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108781},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200262X},
author = {Joya Chen and Dong Liu and Bin Luo and Xuezheng Peng and Tong Xu and Enhong Chen},
keywords = {Object detection, Class imbalance, Residual objectness},
abstract = {As most object detectors rely on dense candidate samples to cover objects, they have always suffered from the extreme imbalance between very few foreground samples and numerous background samples during training, i.e., the foreground-background imbalance. Although several resampling and reweighting schemes (e.g., OHEM, Focal Loss, GHM) have been proposed to alleviate the imbalance, they are usually heuristic with multiple hyper-parameters, which is difficult to generalize on different object detectors and datasets. In this paper, we propose a novel Residual Objectness (ResObj) mechanism that adaptively learns how to address the foreground-background imbalance problem in object detection. Specifically, we first formulate the imbalance problems on all object classes as an imbalance problem on an “objectness” class. Then, we design multiple cascaded objectness estimators with residual connections for that objectness class to progressively distinguish the foreground samples from background samples. With our residual objectness mechanism, object detectors can learn how to address the foreground-background problem in an end-to-end way, rather than rely on hand-crafted resampling or reweighting schemes. Extensive experiments on the COCO benchmark demonstrate the effectiveness and compatibility of our method for various object detectors: the RetinaNet-ResObj, YOLOv3-ResObj and FasterRCNN-ResObj achieve relative 3%∼4% Average Precision (AP) improvements compared with their vanilla models, respectively.}
}
@article{SUH2022108810,
title = {Two-stage generative adversarial networks for binarization of color document images},
journal = {Pattern Recognition},
volume = {130},
pages = {108810},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108810},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002916},
author = {Sungho Suh and Jihun Kim and Paul Lukowicz and Yong Oh Lee},
keywords = {Document image binarization, Generative adversarial networks, Optical character recognition, Color document image enhancement},
abstract = {Document image enhancement and binarization methods are often used to improve the accuracy and efficiency of document image analysis tasks such as text recognition. Traditional non-machine-learning methods are constructed on low-level features in an unsupervised manner but have difficulty with binarization on documents with severely degraded backgrounds. Convolutional neural network (CNN)based methods focus only on grayscale images and on local textual features. In this paper, we propose a two-stage color document image enhancement and binarization method using generative adversarial neural networks. In the first stage, four color-independent adversarial networks are trained to extract color foreground information from an input image for document image enhancement. In the second stage, two independent adversarial networks with global and local features are trained for image binarization of documents of variable size. For the adversarial neural networks, we formulate loss functions between a discriminator and generators having an encoder–decoder structure. Experimental results show that the proposed method achieves better performance than many classical and state-of-the-art algorithms over the Document Image Binarization Contest (DIBCO) datasets, the LRDE Document Binarization Dataset (LRDE DBD), and our shipping label image dataset. We plan to release the shipping label dataset as well as our implementation code at github.com/opensuh/DocumentBinarization/.}
}
@article{SUN2022108692,
title = {General nonconvex total variation and low-rank regularizations: Model, algorithm and applications},
journal = {Pattern Recognition},
volume = {130},
pages = {108692},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108692},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200173X},
author = {Tao Sun and Dongsheng Li},
keywords = {Low-Rank, Total Variation, Nonconvex and nonsmooth minimization, Regularization, image restoration},
abstract = {Total Variation and Low-Rank regularizations have shown significant successes in machine learning, data mining, and image processing in past decades. This paper develops the general nonconvex composite regularized model, which contains previous regularizers and motivates novel ones. Although the classical Alternating Direction Methods of Multiplier (ADMM) algorithm is applicable for this model, the nonconvexity of the problem and the complicacy of choosing the parameters increase the difficulty in the use of ADMM. Thus, by the penalty method, we propose the Alternating Minimization (AM) algorithm, whose convergence results are proved under mild assumptions. The proposed model and algorithm are applied to the image restoration problem. Numerical results demonstrate the efficiency of our model and algorithm.}
}
@article{PANG2023109138,
title = {Cross-modal co-feedback cellular automata for RGB-T saliency detection},
journal = {Pattern Recognition},
volume = {135},
pages = {109138},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109138},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006185},
author = {Yu Pang and Hao Wu and Chengdong Wu},
keywords = {RGB-T saliency detection, Cellular automata, Cross-modal co-feedback framework, Pixel-wise refinement},
abstract = {Saliency cellular automata (CA), a temporally evolving model to efficiently locate salient object, has achieved great progress in saliency detection task. However, most the previous CA models originate from RGB data and are thus limited in some extreme scenes. Inspired by the observation that thermal infrared data (T) can overcome the limitation of RGB data themselves in some cases and RGB-T saliency detection has gained more and more attention. In this paper, we contribute a novel RGB-T saliency detection approach via Cross-modal Co-feedback Cellular Automata (C3A). Before this, we firstly present a novel weighted background-based map (WBM) to give each superpixel(image patch) an initial saliency value. Then C3A is proposed to improve the quality of the WBM. Specifically, it firstly establishes two complementary cellular automata (CA) mechanisms dependent on RGB and thermal infrared data, which respectively refine the WBM based on two different perspectives. To bridge RGB-T modalities, an iterative cross-modal co-feedback framework is contributed to optimize constantly their results. In other words, we regard the result of one modality(RGB or T)-induced CA as important feedback to update and optimize another modality(T or RGB)-induced CA during the iteration. Two modalities constantly pull out the useful and confident data to the opposite side, and so two CAs’ results are constantly refined until a stable state is generated, we then integrate the results of two modalities-induced CAs into the saliency map. Finally, a novel boundary-guided pixel-wise refinement (BPR) technology is proposed to further overcome the influence of inaccurate superpixel segmentation to the C3A and refine the saliency map generated by our C3A. For fairness, the proposed method is compared with other state-of-the-art methods on three RGB-T datasets, experimental results show the superiority of our model.}
}
@article{WAN2023109150,
title = {G2DA: Geometry-guided dual-alignment learning for RGB-infrared person re-identification},
journal = {Pattern Recognition},
volume = {135},
pages = {109150},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109150},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200629X},
author = {Lin Wan and Zongyuan Sun and Qianyan Jing and Yehansen Chen and Lijing Lu and Zhihang Li},
keywords = {Person re-identification, Cross-modality matching, optimal transport, Feature alignment, Channel exchange},
abstract = {RGB-Infrared (IR) person re-identification aims to retrieve person-of-interest from heterogeneous cameras, easily suffering from large image modality discrepancy caused by different sensing wavelength ranges. Existing works usually minimize such discrepancy by aligning modality distribution of global features, while neglecting deep semantics and high-order structural relations within each class. This might render the misalignment between heterogeneous samples. In this paper, we propose Geometry-Guided Dual-Alignment (G2DA) learning, which yields better sample-level modality alignment for RGB-IR ReID by solving a graph-enabled distribution matching task that maximizes agreement between multi-modality node representations considering edge topology. Specifically, we covert RGB/IR images into semantic-aligned graphs, in which whole-part features and their similarities are represented by nodes and associated edges, respectively. To simultaneously implement node- and edge-wise alignment (Dual Alignment), we introduce Optimal Transport (OT) as a metric to calculate cross-modality human body matching scores. By minimizing the displacement cost across RGB-IR graphs, G2DA could learn not just modality-invariant but structurally consistent cross-modality representations. Furthermore, we advance a Message Fusion Attention (MFA) mechanism to adaptively smooth the node representations within each RGB/IR graph, effectively alleviating occlusions caused by other individuals and/or objects. Extensive experiments on two standard benchmark datasets validate the superiority of G2DA, yielding competitive performance against previous state-of-the-arts.}
}
@article{NGUYEN2023109155,
title = {A novel multi-branch wavelet neural network for sparse representation based object classification},
journal = {Pattern Recognition},
volume = {135},
pages = {109155},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109155},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006343},
author = {Tan-Sy Nguyen and Marie Luong and Mounir Kaaniche and Long H. Ngo and Azeddine Beghdadi},
keywords = {Object classification, Sparse coding, Wavelet transform, Neural networks, Multi-branch architecture},
abstract = {Recent advances in acquisition and display technologies have led to an enormous amount of visual data, which requires appropriate storage and management tools. One of the fundamental needs is the design of efficient image classification and recognition solutions. In this paper, we propose a wavelet neural network approach for sparse representation-based object classification. The proposed approach aims to exploit the advantages of sparse coding, multi-scale wavelet representation as well as neural networks. More precisely, a wavelet transform is firstly applied to the image datasets. The generated approximation and detail wavelet subbands are then fed into a multi-branch neural network architecture. This architecture produces multiple sparse codes that are efficiently combined during the classification stage. Extensive experiments, carried out on various types of standard object datasets, have shown the efficiency of the proposed method compared to the existing sparse coding and deep learning-based methods.}
}
@article{HUANG2023109147,
title = {Face anti-spoofing using feature distilling and global attention learning},
journal = {Pattern Recognition},
volume = {135},
pages = {109147},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109147},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006264},
author = {Rui Huang and Xin Wang},
keywords = {Face anti-spoofing, Anti-interference, Multi-level distillation, Global spatial attention learning, Pixel-wise supervision},
abstract = {Face anti-spoofing (FAS) is essential to assure the security of face recognition systems. Recently, some deep learning based FAS methods have achieved promising results under intra-dataset testing. However, they often fail in generalizing to unseen attacks due to the failure of extracting intrinsic features from face images. In this paper, we propose an end-to-end FAS method which consists of an anti-interference feature distillation module, a global spatial attention learning module and a pyramid binary mask supervision module. The deep features from the pretrained ResNet34 network are first distilled at multiple levels to capture intrinsic information via removing interference of features. Then, the multi-level distilled features are further refined by using a global spatial learning mechanism. Finally, the pyramid pixel-wise supervision is assembled to boost performance. Extensive experimental results on five benchmark datasets show the superior performance of our proposed method on intra-dataset testing and on cross-dataset testing.}
}
@article{SATHYASEELAN2023109134,
title = {Sequence patterns and HMM profiles to predict proteome wide zinc finger motifs},
journal = {Pattern Recognition},
volume = {135},
pages = {109134},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109134},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006148},
author = {Chakkarai Sathyaseelan and L Ponoop Prasad Patro and Thenmalarchelvi Rathinavelan},
keywords = {Zinc finger classification, Zinc finger motif, Zinc finger proteome, Pfam HMM profile, Zinc finger prediction},
abstract = {Zinc finger (ZnF) is an important class of nucleic acid and protein recognition domain, wherein, zinc ion is the inorganic co-factor that forms a tetrahedral geometry with the cysteine and/or histidine residues. ZnF domains take up diverse architectures with different ZnF motifs and have a wide range of biological functions. Nonetheless, predicting the ZnF motif(s) from the sequence is quite challenging. To this end, 74 unique ZnF sequence patterns are collected from the literature and classified into 32 different classes. Since the shorter length of ZnF sequence patterns leads to inaccurate predictions, ZnF domain Pfam HMM profiles defined under 6 ZnF Pfam clans (215 HMM profiles) and a few undefined Pfam clans (74 HMM profiles) are used for the prediction. A web server, namely, ZnF-Prot (https://project.iith.ac.in/znprot/) is developed which can predict the presence of 31 ZnF domains in a protein/proteome sequence of any organism. The use of ZnF sequence patterns and Pfam HMM profiles resulted in an accurate prediction of 610 test cases (taken randomly from 249 organisms) considered here. Additionally, the application of ZnF-Prot is demonstrated by considering Arabidopsis thaliana, Homo sapiens, Saccharomyces cerevisiae, Caenorhabditis elegans and Ciona intestinalis proteomes as test cases, wherein, 87–96% of the predicted ZnF motifs are cross-validated.}
}
@article{LIN2023109021,
title = {Self-Supervised Leaf Segmentation under Complex Lighting Conditions},
journal = {Pattern Recognition},
volume = {135},
pages = {109021},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109021},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005015},
author = {Xufeng Lin and Chang-Tsun Li and Scott Adams and Abbas Z. Kouzani and Richard Jiang and Ligang He and Yongjian Hu and Michael Vernon and Egan Doeven and Lawrence Webb and Todd Mcclellan and Adam Guskich},
keywords = {Self-supervised learning, Convolutional neural networks, Image-based plant phenotyping, Leaf segmentation, Color correction, Cannabis},
abstract = {As an essential prerequisite task in image-based plant phenotyping, leaf segmentation has garnered increasing attention in recent years. While self-supervised learning is emerging as an effective alternative to various computer vision tasks, its adaptation for image-based plant phenotyping remains rather unexplored. In this work, we present a self-supervised leaf segmentation framework consisting of a self-supervised semantic segmentation model, a color-based leaf segmentation algorithm, and a self-supervised color correction model. The self-supervised semantic segmentation model groups the semantically similar pixels by iteratively referring to the self-contained information, allowing the pixels of the same semantic object to be jointly considered by the color-based leaf segmentation algorithm for identifying the leaf regions. Additionally, we propose to use a self-supervised color correction model for images taken under complex illumination conditions. Experimental results on datasets of different plant species demonstrate the potential of the proposed self-supervised framework in achieving effective and generalizable leaf segmentation.}
}
@article{CHEN2023109179,
title = {Watching the BiG artifacts: Exposing DeepFake videos via Bi-granularity artifacts},
journal = {Pattern Recognition},
volume = {135},
pages = {109179},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109179},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006586},
author = {Han Chen and Yuezun Li and Dongdong Lin and Bin Li and Junqiang Wu},
keywords = {Multimedia forensics, Deepfake detection, Granularity artifacts, Multi-task learning},
abstract = {Recent years have witnessed significant advances in AI-based face manipulation techniques, known as DeepFakes, which has brought severe threats to society. Hence, an emerging and increasingly important research topic is how to detect DeepFake videos. In this paper, we propose a new DeepFake detection method based on Bi-granularity artifacts (BiG-Arts). We observe that the most of DeepFake video generation can commonly introduce bi-granularity artifacts: the intrinsic-granularity artifacts and extrinsic-granularity artifacts. Specifically, the intrinsic-granularity artifacts are caused by a common series of operations in model generation such as up-convolution or up-sampling, while the extrinsic-granularity artifacts are introduced by a common step in post-processing that blends the synthesized face to original video. To this end, we formulate DeepFake detection as multi-task learning problem, to simultaneously predict the intrinsic and extrinsic artifacts. Benefiting from the guidance of detecting Bi-granularity artifacts, our method is notably boosted in both within-datasets and cross-datasets scenarios. Extensive experiments are conducted on several DeepFake datasets, which corroborates the superiority of our method. Our method has been contributed as a part of the solution to achieve the Top-1 rank in DFGC competition (https://competitions.codalab.org/competitions/29583).}
}
@article{DONG2022108797,
title = {Identifying the key frames: An attention-aware sampling method for action recognition},
journal = {Pattern Recognition},
volume = {130},
pages = {108797},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108797},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002783},
author = {Wenkai Dong and Zhaoxiang Zhang and Chunfeng Song and Tieniu Tan},
keywords = {Action recognition, Deep learning, Reinforcement learning, Pseudo labels},
abstract = {Deep learning based methods have achieved remarkable progress in action recognition. Existing works mainly focus on designing novel deep architectures to learn video representations for action recognition. Most existing methods treat sampled frames equally and average all the frame-level predictions to generate video-level predictions at the testing stage. However, within a video, discriminative actions may occur sparsely in a few frames whereas most other frames are irrelevant to the ground truth which may even lead to wrong results. As a result, we think that the strategy of selecting relevant frames would be a further important key to enhance the existing deep learning based action recognition. In this paper, we propose an attention-aware sampling method for action recognition, which aims to discard the irrelevant and misleading frames and preserve the most discriminative frames. We formulate the process of mining key frames from videos as a Markov decision process and train the attention agent through deep reinforcement learning without extra labels. The agent takes features and predictions from the baseline model as inputs and generates importance scores for all frames. Moreover, our approach is extensible, which can be applied to different existing deep learning based action recognition models. We achieve very competitive action recognition performance on two widely used action recognition datasets.}
}
@article{CUI2022108773,
title = {Pseudo loss active learning for deep visual tracking},
journal = {Pattern Recognition},
volume = {130},
pages = {108773},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108773},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002540},
author = {Zhiyan Cui and Na Lu and Weifeng Wang},
keywords = {Active learning, Visual tracking, Pseudo loss, Pseudo label},
abstract = {In visual tracking tasks, the training data are commonly composed of a large number of video sequences and each frame in the sequences needs to be labeled manually, which is labor-intensive and time-consuming. In addition, considering the similarity among the consecutive frames in the same sequence, there is significant redundancy in the training data. To address these problems, a novel pseudo loss active learning (PLAL) method is developed in this paper. PLAL aims to select the most informative and least redundant data for training to reduce the cost of labeling and maintain competitive tracking results simultaneously. Firstly, the Gaussian distribution based pseudo label is generated for the unlabeled candidates based on the tracking model which is initially trained on a small amount of training data. Then, the pseudo loss based on cross entropy is designed to compute the difference between the pseudo label and the target response map. The pseudo loss measures the uncertainty of the target spatial context which is used as the informativeness criterion of the image frame for selection. Meanwhile, a sampling interval threshold and a temporal penalty are employed for frame selection to avoid drastic variation in target appearance and reduce the redundancy within the consecutive candidate frames. Only the selected frames are labeled by the oracle (human expert) and then added to the training data. Extensive experiments on public benchmarks (OTB2013, OTB2015, VOT2018, UAV123, GOT-10K, TrackingNet, LaSOT, OxUvA and TLP) demonstrate that PLAL method outperforms the baseline and other recent active learning approaches. With only 3% of labeled data from the training dataset, PLAL reaches competitive performance (98-100%) compared to the model trained on the entire training dataset.}
}
@article{XIANG2023109151,
title = {Similarity learning with deep CRF for person re-identification},
journal = {Pattern Recognition},
volume = {135},
pages = {109151},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109151},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006306},
author = {Jun Xiang and Ziyuan Huang and Xiaoping Jiang and Jianhua Hou},
keywords = {Person re-identification, Deep learning, Conditional random field (CRF), Group-wise similarities},
abstract = {The core of person re-identification (Re-ID) lies in robustly estimating similarities for each probe-gallery image pair. A common practice in existing works is to calculate the similarity of each image pair independently, ignoring relations between different probe-gallery pairs. In this paper, we present a deep learning conditional random field (Deep-CRF) graph to model group-wise similarities within a batch of images, and regard the Re-ID task as a CRF node labeling problem. Unlike the existing deep CRF based approach where the CRF inference is only involved in the training stage, our method intends to fully exploit the potential of CRF model, exhibiting inference consistency in both training and testing. Specifically, we design unary potentials for computing each probe-gallery similarity separately. To efficiently encode relationships between different probe-gallery pairs, pairwise potentials are built on an arbitrary node pair whose learning is achieved by a joint matching strategy using bidirectional LSTM. We pose the CRF inference as a RNN learning process, where unary and pairwise potentials are jointly optimized in an end-to-end manner. Extensive experiments on three large-scale person Re-ID datasets demonstrate the effectiveness of the proposed method. Our Deep-CRF achieves the best results compared with the previous graph-based deep learning approaches and substantially exceeds the existing deep CRF framework by 8% in Rank1 accuracy on CUHK03 dataset. It also behaves competitive among the current state-of-the-art methods.}
}
@article{HUANG2023109142,
title = {PLFace: Progressive Learning for Face Recognition with Mask Bias},
journal = {Pattern Recognition},
volume = {135},
pages = {109142},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109142},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006227},
author = {Baojin Huang and Zhongyuan Wang and Guangcheng Wang and Kui Jiang and Zhen Han and Tao Lu and Chao Liang},
keywords = {Face recognition, Progressive learning, Mask bias},
abstract = {The outbreak of the COVID-19 coronavirus epidemic has promoted the development of masked face recognition (MFR). Nevertheless, the performance of regular face recognition is severely compromised when the MFR accuracy is blindly pursued. More facts indicate that MFR should be regarded as a mask bias of face recognition rather than an independent task. To mitigate mask bias, we propose a novel Progressive Learning Loss (PLFace) that achieves a progressive training strategy for deep face recognition to learn balanced performance for masked/mask-free faces recognition based on margin losses. Particularly, our PLFace adaptively adjusts the relative importance of masked and mask-free samples during different training stages. In the early stage of training, PLFace mainly learns the feature representations of mask-free samples. At this time, the regular sample embeddings shrink to the corresponding prototype, which represents the center of each class while being stored in the last linear layer. In the later stage of training, PLFace converges on mask-free samples and further focuses on masked samples until the masked sample embeddings are also gathered in the center of the class. The entire training process emphasizes the paradigm that normal samples shrink first and masked samples gather afterward. Extensive experimental results on popular regular and masked face benchmarks demonstrate that our proposed PLFace can effectively eliminate mask bias in face recognition. Compared to state-of-the-art competitors, PLFace significantly improves the accuracy of MFR while maintaining the performance of normal face recognition.}
}
@article{FENG2022108777,
title = {DMT: Dynamic mutual training for semi-supervised learning},
journal = {Pattern Recognition},
volume = {130},
pages = {108777},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108777},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002588},
author = {Zhengyang Feng and Qianyu Zhou and Qiqi Gu and Xin Tan and Guangliang Cheng and Xuequan Lu and Jianping Shi and Lizhuang Ma},
keywords = {Dynamic mutual training, Inter-model disagreement, Noisy pseudo label, Semi-supervised learning},
abstract = {Recent semi-supervised learning methods use pseudo supervision as core idea, especially self-training methods that generate pseudo labels. However, pseudo labels are unreliable. Self-training methods usually rely on single model prediction confidence to filter low-confidence pseudo labels, thus remaining high-confidence errors and wasting many low-confidence correct labels. In this paper, we point out it is difficult for a model to counter its own errors. Instead, leveraging inter-model disagreement between different models is a key to locate pseudo label errors. With this new viewpoint, we propose mutual training between two different models by a dynamically re-weighted loss function, called Dynamic Mutual Training (DMT). We quantify inter-model disagreement by comparing predictions from two different models to dynamically re-weight loss in training, where a larger disagreement indicates a possible error and corresponds to a lower loss value. Extensive experiments show that DMT achieves state-of-the-art performance in both image classification and semantic segmentation. Our codes are released at https://github.com/voldemortX/DST-CBC.}
}
@article{2022108853,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {130},
pages = {108853},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(22)00334-X},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200334X}
}
@article{KADIOGLU2022108688,
title = {Sample complexity of rank regression using pairwise comparisons},
journal = {Pattern Recognition},
volume = {130},
pages = {108688},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108688},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001698},
author = {Berkan Kadıoğlu and Peng Tian and Jennifer Dy and Deniz Erdoğmuş and Stratis Ioannidis},
keywords = {Sample complexity, Rank regression, Pairwise comparisons, Features},
abstract = {We consider a rank regression setting, in which a dataset of N samples with features in Rd is ranked by an oracle via M pairwise comparisons. Specifically, there exists a latent total ordering of the samples; when presented with a pair of samples, a noisy oracle identifies the one ranked higher with respect to the underlying total ordering. A learner observes a dataset of such comparisons and wishes to regress sample ranks from their features. We show that to learn the model parameters with ϵ>0 accuracy, it suffices to conduct M∈Ω(dNlog3N/ϵ2) comparisons uniformly at random when N is Ω(d/ϵ2).}
}
@article{CHAN2022108793,
title = {Online multiple object tracking using joint detection and embedding network},
journal = {Pattern Recognition},
volume = {130},
pages = {108793},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108793},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002746},
author = {Sixian Chan and Yangwei Jia and Xiaolong Zhou and Cong Bai and Shengyong Chen and Xiaoqin Zhang},
keywords = {One-shot MOT, Joint detection and tracking, YOLO tracker},
abstract = {Multiple object tracking (MOT) generally employs the paradigm of tracking-by-detection, where object detection and object tracking are executed conventionally using separate systems. Current progress in MOT has focused on detecting and tracking objects by harnessing the representational power of deep learning. Since existing methods always combine two submodules in the same network, it is particularly important that they must be trained effectively together. Therefore, the development of a suitable network architecture for the end-to-end joint training of detection and tracking submodules remains a challenging issue. The present work addresses this issue by proposing a novel architecture denoted as YOLOTracker that performs online MOT by exploiting a joint detection and embedding network. First, an efficient and powerful joint detection and tracking model is constructed to accomplish instance-level embedded training, which can ensure that the proposed tracker achieves highly accurate MOT results with high efficiency. Then, the Path Aggregation Network is employed to combine low-resolution and high-resolution features for integrating textural features and semantic information and mitigating the misalignment of the re-identification features. Experiments are conducted on three challenging and publicly available benchmark datasets and results demonstrate the proposed tracker outperforms other state-of-the-art MOT trackers in terms of accuracy and efficiency.}
}
@article{WANG2022108809,
title = {Directly solving normalized cut for multi-view data},
journal = {Pattern Recognition},
volume = {130},
pages = {108809},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108809},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002904},
author = {Chen Wang and Xiaojun Chen and Feiping Nie and Joshua Zhexue Huang},
keywords = {Clustering, Graph cut, Multi-view},
abstract = {Graph-based multi-view clustering, which aims to uncover clusters from multi-view data with graph clustering technique, is one of the most important multi-view clustering methods. Such methods usually perform eigen-decomposition first to solve the relaxed problem and then obtain the final cluster indicator matrix from eigenvectors by k-means or spectral rotation. However, such a two-step process may result in undesired clustering result since the two steps aim to solve different problems. In this paper, we propose a k-way normalized cut method for multi-view data, named as the Multi-view Discrete Normalized Cut (MDNC). The new method learns a set of implicit weights for each view to identify its quality, and a novel iterative algorithm is proposed to directly solve the new model without relaxation and post-processing. Moreover, we propose a new method to adjust the distribution of the implicit view weights to obtain better clustering result. Extensive experimental results show that the performance of our approach is superior to the state-of-the-art methods.}
}