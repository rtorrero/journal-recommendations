@article{BEHROUZI2022108202,
title = {Graph variational auto-encoder for deriving EEG-based graph embedding},
journal = {Pattern Recognition},
volume = {121},
pages = {108202},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108202},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003848},
author = {Tina Behrouzi and Dimitrios Hatzinakos},
keywords = {Biometrics, Functional connectivity, Electroencephalogram (EEG), Graph variational auto encoder (GVAE), Graph deep learning},
abstract = {Graph embedding is an effective method for deriving low-dimensional representations of graph data. The power of graph deep learning methods to characterize electroencephalogram (EEG) graph embedding is still in question. We designed a novel graph variational auto-encoder (GVAE) method to extract nodal features of brain functional connections. A new decoder model for the GVAEs network is proposed, which considers the node neighborhood of the reconstructed adjacency matrix. The GVAE is applied and tested on 3 biometric databases which contain 64 to 9 channels’ EEG recordings. For all datasets, promising results with more than 95% accuracy and considerably low computational cost are achieved compared to state-of-the-art user identification methods. The proposed GVAE is robust to a limited number of nodes and stable to users’ task performance. Moreover, we developed a traditional variational auto-encoder to demonstrate that more accurate features can be obtained when observing EEG-based brain connectivity from a graph perspective.}
}
@article{DOMBROWSKI2022108194,
title = {Towards robust explanations for deep neural networks},
journal = {Pattern Recognition},
volume = {121},
pages = {108194},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108194},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003769},
author = {Ann-Kathrin Dombrowski and Christopher J. Anders and Klaus-Robert Müller and Pan Kessel},
keywords = {Explanation method, Saliency map, Adversarial attacks, Manipulation, Neural networks,},
abstract = {Explanation methods shed light on the decision process of black-box classifiers such as deep neural networks. But their usefulness can be compromised because they are susceptible to manipulations. With this work, we aim to enhance the resilience of explanations. We develop a unified theoretical framework for deriving bounds on the maximal manipulability of a model. Based on these theoretical insights, we present three different techniques to boost robustness against manipulation: training with weight decay, smoothing activation functions, and minimizing the Hessian of the network. Our experimental results confirm the effectiveness of these approaches.}
}
@article{NAKANO2022108211,
title = {Deep tree-ensembles for multi-output prediction},
journal = {Pattern Recognition},
volume = {121},
pages = {108211},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108211},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003927},
author = {Felipe Kenji Nakano and Konstantinos Pliakos and Celine Vens},
keywords = {Ensemble learning, Deep-forest, Multi-output prediction, Multi-target regression, Multi-label classification},
abstract = {Recently, deep neural networks have expanded the state-of-art in various scientific fields and provided solutions to long standing problems across multiple application domains. Nevertheless, they also suffer from weaknesses since their optimal performance depends on massive amounts of training data and the tuning of an extended number of parameters. As a countermeasure, some deep-forest methods have been recently proposed, as efficient and low-scale solutions. Despite that, these approaches simply employ label classification probabilities as induced features and primarily focus on traditional classification and regression tasks, leaving multi-output prediction under-explored. Moreover, recent work has demonstrated that tree-embeddings are highly representative, especially in structured output prediction. In this direction, we propose a novel deep tree-ensemble (DTE) model, where every layer enriches the original feature set with a representation learning component based on tree-embeddings. In this paper, we specifically focus on two structured output prediction tasks, namely multi-label classification and multi-target regression. We conducted experiments using multiple benchmark datasets and the obtained results confirm that our method provides superior results to state-of-the-art methods in both tasks.}
}
@article{PENG2022108108,
title = {Multi-dimensional clustering through fusion of high-order similarities},
journal = {Pattern Recognition},
volume = {121},
pages = {108108},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108108},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002958},
author = {Hong Peng and Haiyan Wang and Yu Hu and Weiwei Zhou and Hongmin Cai},
keywords = {High-order similarity, Low-rank, Multi-dimensional clustering, Spectral clustering},
abstract = {Clustering objects with heterogeneous attributes captured from different dimensions remains challenging in integrating the multiple dimensional information. Most of the current multi-dimensional clustering models pin on direct sample-wised similarity and fail to exploit hidden mutual affinity among different sampling spaces. Thus, it is hard to capture a legible cluster structure. To tackle this issue, we propose a High-order multi-dimensional Spectral Clustering method (HSC). The proposed HSC aims to learn a high-order similarity to characterize the intrinsic relationship among different dimensional spaces instead of the ordinary similarity. It then performs a clustering task within a latent space by jointly learning the high-order similarity and ordinary similarity. Extensive experiments over synthetic and real-world data sets show that the proposed HSC outperforms benchmark multi-dimensional methods in most scenarios and is capable of revealing a reliable structure concealed across multi-dimensional spaces.}
}
@article{XIA2022108177,
title = {WC-KNNG-PC: Watershed clustering based on k-nearest-neighbor graph and Pauta Criterion},
journal = {Pattern Recognition},
volume = {121},
pages = {108177},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108177},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003642},
author = {Jianhua Xia and Jinbing Zhang and Yang Wang and Lixin Han and Hong Yan},
keywords = {Watershed clustering, -nearest neighbor graph (KNNG), Pauta criterion, Shared nearest neighbor (SNN)},
abstract = {Watershed clustering utilizes the concept of watershed algorithm to process clustering or cluster analyzes. The most attractive characteristic of this method is the capability to determine automatically the number of clusters from the data sets. However, in terms of the literature, the purposes of the original watershed clustering algorithm and the improved version are the detection of the clusters within two-dimensional linear data sets. In order to enable watershed clustering to deal with the dataset with multiple dimensions and nonlinear structures, we introduce k-nearest neighbor graph (KNNG), the shared nearest neighbor method and Pauta Criterion into watershed clustering to present a new watershed graph clustering with noise detection, WC-KNNG-PC. This approach first calculates a KNNG for the data sets, and then compute catchment basins (subclusters), basin immersions (connectivity between basins) and outliers. To prevent the merger of illegal subclusters, a maximum normalization stability factor, based on t-nearest neighbors and angle, MNSF, is proposed to detect the invalid basin immersions. Finally, a basin level similarity using median criterion is presented to merge the catchment basins to obtain the final clustering. Experiments on complex synthetic datasets and multidimensional real-world datasets have successfully demonstrated that the performance of the WC-KNNG-PC in clustering some various dimensional and complex datasets with heterogeneous density and diverse shapes.}
}
@article{WANG2022108220,
title = {Pedestrian attribute recognition: A survey},
journal = {Pattern Recognition},
volume = {121},
pages = {108220},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108220},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004015},
author = {Xiao Wang and Shaofei Zheng and Rui Yang and Aihua Zheng and Zhe Chen and Jin Tang and Bin Luo},
keywords = {Pedestrian attribute recognition, Multi-label learning, Multi-task learning, Deep learning, CNN-RNN},
abstract = {Pedestrian Attribute Recognition (PAR) is an important task in computer vision community and plays an important role in practical video surveillance. The goal of this paper is to review existing works using traditional methods or based on deep learning networks. Firstly, we introduce the background of pedestrian attribute recognition, including the fundamental concepts and formulation of pedestrian attributes and corresponding challenges. Secondly, we analyze popular solutions for this task from eight perspectives. Thirdly, we discuss the specific attribute recognition, then, give a comparison between deep learning and traditional algorithm based PAR methods. After that, we show the connections between PAR and other computer vision tasks. Fourthly, we introduce the benchmark datasets, evaluation metrics in this community, and give a brief performance comparison. Finally, we summarize this paper and give several possible research directions for PAR. The project page of this paper can be found at: https://sites.google.com/view/ahu-pedestrianattributes/.}
}
@article{CHENG2022108218,
title = {Financial time series forecasting with multi-modality graph neural network},
journal = {Pattern Recognition},
volume = {121},
pages = {108218},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108218},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100399X},
author = {Dawei Cheng and Fangzhou Yang and Sheng Xiang and Jin Liu},
keywords = {Graph neural network, Graph attention, Deep learning, Quantitative investment},
abstract = {Financial time series analysis plays a central role in hedging market risks and optimizing investment decisions. This is a challenging task as the problems are always accompanied by multi-modality streams and lead-lag effects. For example, the price movements of stock are reflections of complicated market states in different diffusion speeds, including historical price series, media news, associated events, etc. Furthermore, the financial industry requires forecasting models to be interpretable and compliant. Therefore, in this paper, we propose a multi-modality graph neural network (MAGNN) to learn from these multimodal inputs for financial time series prediction. The heterogeneous graph network is constructed by the sources as nodes and relations in our financial knowledge graph as edges. To ensure the model interpretability, we leverage a two-phase attention mechanism for joint optimization, allowing end-users to investigate the importance of inner-modality and inter-modality sources. Extensive experiments on real-world datasets demonstrate the superior performance of MAGNN in financial market prediction. Our method provides investors with a profitable as well as interpretable option and enables them to make informed investment decisions.}
}
@article{SAEZ2022108198,
title = {ANCES: A novel method to repair attribute noise in classification problems},
journal = {Pattern Recognition},
volume = {121},
pages = {108198},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108198},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003800},
author = {José A. Sáez and Emilio Corchado},
keywords = {Attribute noise, Noise correction, Noise filtering, Noisy data, Classification},
abstract = {Noise negatively affects the complexity and performance of models built in classification problems. The most common approach to mitigate its consequences is the usage of preprocessing techniques, known as noise filters, which are designed to remove noisy samples from the training data. Nevertheless, they are specifically oriented to deal with errors affecting class labels. Their employment may not always result in an improvement when noise affects attribute values. In these cases, correcting the errors is an interesting alternative to traditional noise filtering that has not been enough studied so far in the specialized literature. This research proposes an attribute noise correction method with the final aim of increasing the performance of the classification algorithms used later. The identification of noisy data is based on an error score assigned to each one of the attribute values in the dataset, which are then passed through an optimization process to correct their potential noise. The validity of the proposed method is studied in an exhaustive experimental study, in which it is compared to several well-known preprocessing methods to deal with noisy datasets. The results obtained show the suitability of attribute noise correction with respect to the other alternatives when data suffer from attribute noise.}
}
@article{WANG2022108215,
title = {Graph convolutional autoencoders with co-learning of graph structure and node attributes},
journal = {Pattern Recognition},
volume = {121},
pages = {108215},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108215},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003964},
author = {Jie Wang and Jiye Liang and Kaixuan Yao and Jianqing Liang and Dianhui Wang},
keywords = {Graph representation learning, Graph convolutional autoencoders, Graph filter},
abstract = {Recently, graph representation learning based on autoencoders has received much attention. However, these methods suffer from two limitations. First, most graph autoencoders ignore the reconstruction of either the graph structure or the node attributes, which often leads to a poor latent representation of the graph-structured data. Second, for existing graph autoencoders models, the encoder and decoder are mainly composed of an initial graph convolutional network (GCN) or its variants. These traditional GCN-based graph autoencoders more or less encounter the problem of incomplete filtering, which causes these models to be unstable in practical applications. To address the above issues, this paper proposes the Graph convolutional Autoencoders with co-learning of graph Structure and Node attributes (GASN) based on variational autoencoders. Specifically, the proposed GASN encodes and decodes the node attributes and graph structure comprehensively in the graph-structured data. Furthermore, we design a completely low-pass graph encoder and a high-pass graph decoder. The experimental results on real-world datasets demonstrate that the proposed GASN achieves state-of-the-art performance on node clustering, link prediction, and visualization tasks.}
}
@article{XIAO2022108157,
title = {Transtrack: Online meta-transfer learning and Otsu segmentation enabled wireless gesture tracking},
journal = {Pattern Recognition},
volume = {121},
pages = {108157},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108157},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003447},
author = {Jiang Xiao and Huichuwu Li and Hai Jin},
keywords = {Individual diversity, Meta-transfer learning, Gesture tracking, Channel state information, Data alignment, Online learning},
abstract = {Individual diversity poses a cross-user performance variance challenge that stumbles the practicality, especially for the wireless gesture tracking systems. Since the difficulty of annotating low-semantic wireless data limits constructing a big dataset, the recognizer should quickly adjust to different individuals via small datasets. To this end, we present TransTrack, an accurate wireless indoor gesture tracking system that can adjust to different users quickly. The key insight is that each unlabeled gesture contains learnable individual features that can help the gesture tracking model learning how to adapt to different users. Specifically, TransTrack uses recursive Otsu segmentation to separate gesture-induced signals with the background noise inspired by image segmentation. It then augments training data to learn the transferable features by leveraging the redundant information. A datum-based alignment method is proposed to unlock the limitation of classifier selection without distortion. Finally, TransTrack proposes an online meta-transfer learning method that collects unlabeled data transparently to train the tracking model for different tasks. Extensive experiments show that TransTrack can quickly adapt to different users and conditions.}
}
@article{YANG2022108231,
title = {BiconNet: An edge-preserved connectivity-based approach for salient object detection},
journal = {Pattern Recognition},
volume = {121},
pages = {108231},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108231},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100412X},
author = {Ziyun Yang and Somayyeh Soltanian-Zadeh and Sina Farsiu},
keywords = {Salient object detection, Visual saliency, Connectivity modeling, Deep learning, Edge modeling},
abstract = {Salient object detection (SOD) is viewed as a pixel-wise saliency modeling task by traditional deep learning-based methods. A limitation of current SOD models is insufficient utilization of inter-pixel information, which usually results in imperfect segmentation near edge regions and low spatial coherence. As we demonstrate, using a saliency mask as the only label is suboptimal. To address this limitation, we propose a connectivity-based approach called bilateral connectivity network (BiconNet), which uses connectivity masks together with saliency masks as labels for effective modeling of inter-pixel relationships and object saliency. Moreover, we propose a bilateral voting module to enhance the output connectivity map, and a novel edge feature enhancement method that efficiently utilizes edge-specific features. Through comprehensive experiments on five benchmark datasets, we demonstrate that our proposed method can be plugged into any existing state-of-the-art saliency-based SOD framework to improve its performance with negligible parameter increase.}
}
@article{2022108133,
title = {Expression of concern: “Deep Adaptive Feature Embedding with Local Sample Distributions for Person Re-identification” [Pattern Recognition, Volume 73, January 2018, Pages 275-288]},
journal = {Pattern Recognition},
volume = {121},
pages = {108133},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108133},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003204}
}
@article{USMANYASEEN2022108207,
title = {Cloud based scalable object recognition from video streams using orientation fusion and convolutional neural networks},
journal = {Pattern Recognition},
volume = {121},
pages = {108207},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108207},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003885},
author = {Muhammad {Usman Yaseen} and Ashiq Anjum and Giancarlo Fortino and Antonio Liotta and Amir Hussain},
keywords = {Scalable video anaytics, Feature fusion, Object orientation, Object recognition, Convolutional neural networks, Cloud-based video analytics},
abstract = {Object recognition from live video streams comes with numerous challenges such as the variation in illumination conditions and poses. Convolutional neural networks (CNNs) have been widely used to perform intelligent visual object recognition. Yet, CNNs still suffer from severe accuracy degradation, particularly on illumination-variant datasets. To address this problem, we propose a new CNN method based on orientation fusion for visual object recognition. The proposed cloud-based video analytics system pioneers the use of bi-dimensional empirical mode decomposition to split a video frame into intrinsic mode functions (IMFs). We further propose these IMFs to endure Reisz transform to produce monogenic object components, which are in turn used for the training of CNNs. Past works have demonstrated how the object orientation component may be used to pursue accuracy levels as high as 93%. Herein we demonstrate how a feature-fusion strategy of the orientation components leads to further improving visual recognition accuracy to 97%. We also assess the scalability of our method, looking at both the number and the size of the video streams under scrutiny. We carry out extensive experimentation on the publicly available Yale dataset, including also a self generated video datasets, finding significant improvements (both in accuracy and scale), in comparison to AlexNet, LeNet and SE-ResNeXt, which are three most commonly used deep learning models for visual object recognition and classification.}
}
@article{SELLAMI2022108224,
title = {Deep neural networks-based relevant latent representation learning for hyperspectral image classification},
journal = {Pattern Recognition},
volume = {121},
pages = {108224},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108224},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004052},
author = {Akrem Sellami and Salvatore Tabbone},
keywords = {Deep learning, Representation learning, Hyperspectral image classification, Feature extraction},
abstract = {The classification of hyperspectral image is a challenging task due to the high dimensional space, with large number of spectral bands, and low number of labeled training samples. To overcome these challenges, we propose a novel methodology for hyperspectral image classification based on multi-view deep neural networks which fuses both spectral and spatial features by using only a small number of labeled samples. Firstly, we process the initial hyperspectral image in order to extract a set of spectral and spatial features. Each spectral vector is the spectral signature of each pixel of the image. The spatial features are extracted using a simple deep autoencoder, which seeks to reduce the high dimensionality of data taking into account the neighborhood region for each pixel. Secondly, we propose a multi-view deep autoencoder model which allows fusing the spectral and spatial features extracted from the hyperspectral image into a joint latent representation space. Finally, a semi-supervised graph convolutional network is trained based on thee fused latent representation space to perform the hyperspectral image classification. The main advantage of the proposed approach is to allow the automatic extraction of relevant information while preserving the spatial and spectral features of data, and improve the classification of hyperspectral images even when the number of labeled samples is low. Experiments are conducted on three real hyperspectral images respectively Indian Pines, Salinas, and Pavia University datasets. Results show that the proposed approach is competitive in classification performances compared to state-of-the-art.}
}
@article{SANTRA2022108257,
title = {Part-based annotation-free fine-grained classification of images of retail products},
journal = {Pattern Recognition},
volume = {121},
pages = {108257},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108257},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004374},
author = {Bikash Santra and Avishek Kumar Shaw and Dipti Prasad Mukherjee},
keywords = {Fine-grained classification, Reconstruction-classification network, Supervised convolutional autoencoder, Retail product detection},
abstract = {We propose a novel solution that classifies very similar images (fine-grained classification) of variants of retail products displayed on the racks of supermarkets. The proposed scheme simultaneously captures object-level and part-level cues of the product images. The object-level cues of the product images are captured with our novel reconstruction-classification network (RC-Net). For annotation-free modeling of part-level cues, the discriminatory parts of the product images are identified around the keypoints. The ordered sequences of these discriminatory parts, encoded using convolutional LSTM, describe the products uniquely. Finally, the part-level and object-level models jointly determine the products explicitly explaining coarse to finer descriptions of the products. This bi-level architecture is embedded in R-CNN for recognizing variants of retail products on the rack. We perform extensive experiments on one In-house and three benchmark datasets. The proposed scheme outperforms competing methods in almost all the evaluations.}
}
@article{HAO2022108232,
title = {Spatiotemporal consistency-enhanced network for video anomaly detection},
journal = {Pattern Recognition},
volume = {121},
pages = {108232},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108232},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004131},
author = {Yi Hao and Jie Li and Nannan Wang and Xiaoyu Wang and Xinbo Gao},
keywords = {Anomaly detection, Unsupervised learning, Spatiotemporal consistency},
abstract = {Video anomaly detection aims to detect abnormal segments in a video sequence, which is a key problem in video surveillance. Based on deep prediction methods, we propose a spatiotemporal consistency-enhanced network to generate spatiotemporal consistency predictions. A 3D CNN-based encoder and 2D CNN-based decoder constitute the main part of our model. A resampling strategy is applied to the latent space vector when the model is trained by the normal data, yet this can cause the model to perform poorly if the data include abnormal data. Moreover, we combine an input clip with a generated frame into a reformed video clip, which is then fed into a discriminator that is constructed by the 3D CNN to evaluate the consistency of the input clip. Owing to the adversarial training between the generator and discriminator, the spatiotemporal consistency of the generated results is enhanced. During the testing stage, the abnormal data generates a different appearance and motion changes, which affect the ability of our model to predict spatiotemporal consistency in future images. Then, the prediction quality gap between normal and anomalous contents is used to infer whether anomalies occur. Extensive experiments confirm that the proposed method achieves state-of-the-art performance on three benchmark datasets, including ShanghaiTech, CUHK Avenue, and UCSD Ped2.}
}
@article{ZHANG2022108201,
title = {Novel fuzzy clustering algorithm with variable multi-pixel fitting spatial information for image segmentation},
journal = {Pattern Recognition},
volume = {121},
pages = {108201},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108201},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003836},
author = {Hang Zhang and Haili Li and Ning Chen and Shengfeng Chen and Jian Liu},
keywords = {Fuzzy clustering, Image segmentation, Spatial information, Variable filter window, Variable generalized neighbourhood window},
abstract = {Spatial information is often used to enhance the robustness of traditional fuzzy c-means (FCM) clustering algorithms. Although some recently emerged improvements are remarkable, the computational complexity of these algorithms is high, which may lead to lack of practicability. To address this problem, an efficient variant named the fuzzy clustering algorithm with variable multi-pixel fitting spatial information (FCM-VMF) is presented. First, a fuzzy clustering algorithm with multi-pixel fitting spatial information (FCM-MF) is developed. Specifically, by dividing the input image into several filter windows, the spatial information of all pixels in each filter window can be obtained simultaneously by fitting the pixels in its corresponding neighbourhood window, which enormously reduces the computational complexity. However, the FCM-MF may result in the loss of edge information. Therefore, the FCM-VMF integrates a variable window strategy with FCM-MF. In this strategy, to preserve more edge information, the sizes of the filter window and generalized neighbourhood window are adaptively reduced. The experimental results show that FCM-VMF is as effective as some recent algorithms. Notably, the FCM-VMF has extremely high efficiency, which means it has a better prospect of application.}
}
@article{WU2022108239,
title = {Learning hybrid ranking representation for person re-identification},
journal = {Pattern Recognition},
volume = {121},
pages = {108239},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108239},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004209},
author = {Guile Wu and Xiatian Zhu and Shaogang Gong},
keywords = {Person re-identification, Ranking representation, Ranking ensemble},
abstract = {Contemporary person re-identification (re-id) methods mostly compute independentlya feature representation of each person image in the query set and the gallery set. This strategy fails to consider any ranking context information of each probe image in the query set represented implicitly by the whole gallery set. Some recent re-ranking re-id methods therefore propose to take a post-processing strategy to exploit such contextual information for improving re-id matching performance. However, post-processing is independent of model training without jointly optimising the re-id feature and the ranking context information for better compatibility. In this work, for the first time, we show that the appearance feature and the ranking context information can be jointly optimised for learning more discriminative representations and achieving superior matching accuracy. Specifically, we propose to learn a hybrid ranking representation for person re-id with a two-stream architecture: (1) In the external stream, we use the ranking list of each probe image to learn plausible visual variations among the top ranks from the gallery as the external ranking information; (2) In the internal stream, we employ the part-based fine-grained feature as the internal ranking information, which mitigates the harm of incorrect matches in the ranking list. Assembling these two streams generates a hybrid ranking representation for person matching. Extensive experiments demonstrate the superiority of our method over the state-of-the-art methods on four large-scale re-id benchmarks (Market-1501, DukeMTMC-ReID, CUHK03 and MSMT17), under both supervised and unsupervised settings.}
}
@article{JIANG2022108167,
title = {GLMNet: Graph learning-matching convolutional networks for feature matching},
journal = {Pattern Recognition},
volume = {121},
pages = {108167},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108167},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100354X},
author = {Bo Jiang and Pengfei Sun and Bin Luo},
keywords = {Graph matching, Graph learning, Graph convolutional network, Laplacian sharpening},
abstract = {Recently, graph convolutional networks (GCNs) have been employed for graph matching problem. It can integrate graph node feature embedding, node-wise affinity learning and matching optimization together in a unified end-to-end model. However, first, the matching graphs feeding to existing graph matching networks are generally fixed and independent of graph matching task, which thus are not guaranteed to be optimal for the graph matching task. Second, existing methods generally employ smoothing-based graph convolution to generate graph node embeddings, in which extensive smoothing convolution operation may dilute the desired discriminatory information of graph nodes. To overcome these issues, we propose a novel Graph Learning-Matching Network (GLMNet) for graph matching problem. GLMNet has three main aspects. (1) It integrates graph learning into graph matching which thus adaptively learns a pair of optimal graphs for graph matching task. (2) It further employs a Laplacian sharpening graph convolution to generate more discriminative node embeddings for graph matching. (3) A new constraint regularized loss is designed for GLMNet training which can encode the desired one-to-one matching constraints in matching optimization. Experiments demonstrate the effectiveness of GLMNet.}
}
@article{XU2022108210,
title = {Head pose estimation using deep neural networks and 3D point clouds},
journal = {Pattern Recognition},
volume = {121},
pages = {108210},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108210},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003915},
author = {Yuanquan Xu and Cheolkon Jung and Yakun Chang},
keywords = {Head pose estimation, Deep learning, Graph convolution, Multi-layer perceptron, Point cloud},
abstract = {In this paper, we propose head pose estimation using deep neural networks and 3D point cloud. Unlike existing methods that either take 2D RGB image or 2D depth image as input, we adopt 3D point cloud data generated from depth to estimate 3D head poses. To further improve robustness and accuracy of head pose estimation, we classify 3D angles of head poses into 36 classes with 5 ∘ interval and predict the probability of each angle in a class based on multi-layer perceptron (MLP). While traditional iterative methods for head model construction require high computation and memory costs, the proposed method is lightweight and computationally efficient by utilizing a sampled 3D point cloud as input combined with a graph convolutional neural network (GCNN). Experimental results on Biwi Kinect Head Pose dataset show that the proposed method achieves outstanding performance in head pose estimation and outperforms state-of-the-art ones in terms of accuracy.}
}
@article{WANG2022108206,
title = {Semi-supervised student-teacher learning for single image super-resolution},
journal = {Pattern Recognition},
volume = {121},
pages = {108206},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108206},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003873},
author = {Lin Wang and Kuk-Jin Yoon},
keywords = {Semi-supervised learning, Image super-resolution, Student-teacher model, Adversarial learning},
abstract = {Most existing approaches for single image super-resolution (SISR) resort to quality low-high resolution (LR-HR) pairs and available degradation kernels to train networks for a specific task in hand in a fully supervised manner. Labeled data used for training are, however, usually limited in terms of the quantity and the diversity degradation kernels. The learned SR networks with one degradation kernel (e.g., bicubic) do not generalize well and their performance sharply deteriorates on other kernels (e.g., blurred or noise). In this paper, we address the critical challenge for SISR: limited labeled LR images and degradation kernels. We propose a novel Semi-supervised Student-Teacher Super-Resolution approach called S2TSR that super-resolves both labelled and unlabeled LR images via adversarial learning. To better exploit the information from labeled LR images, we propose a student-teacher framework (S-T) via knowledge transfer from supervised learning (T) to unsupervised learning (S). Specifically, the S-T knowledge transfer is based on a shared SR network, partial weight sharing of dual discriminators, and a pair matching network which also plays as a ‘latent discriminator’. Lastly, to learn better features from the limited labeled LR images, we propose a new SR network via non-local and attention mechanisms. Experiments demonstrate that our approach substantially improves unsupervised methods and performs favorably over fully supervised methods.}
}
@article{DONG2022108176,
title = {Lifelong robotic visual-tactile perception learning},
journal = {Pattern Recognition},
volume = {121},
pages = {108176},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108176},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003630},
author = {Jiahua Dong and Yang Cong and Gan Sun and Tao Zhang},
keywords = {Lifelong machine learning, Robotics, Visual-tactile perception, Cross-modality learning, Multi-task learning},
abstract = {Lifelong machine learning can learn a sequence of consecutive robotic perception tasks via transferring previous experiences. However, 1) most existing lifelong learning based perception methods only take advantage of visual information for robotic tasks, while neglecting another important tactile sensing modality to capture discriminative material properties; 2) Meanwhile, they cannot explore the intrinsic relationships across different modalities and the common characterization among different tasks of each modality, due to the distinct divergence between heterogeneous feature distributions. To address above challenges, we propose a new Lifelong Visual-Tactile Learning (LVTL) model for continuous robotic visual-tactile perception tasks, which fully explores the latent correlations in both intra-modality and cross-modality aspects. Specifically, a modality-specific knowledge library is developed for each modality to explore common intra-modality representations across different tasks, while narrowing intra-modality mapping divergence between semantic and feature spaces via an auto-encoder mechanism. Moreover, a sparse constraint based modality-invariant space is constructed to capture underlying cross-modality correlations and identify the contributions of each modality for new coming visual-tactile tasks. We further propose a modality consistency regularizer to efficiently align the heterogeneous visual and tactile samples, which ensures the semantic consistency between different modality-specific knowledge libraries. After deriving an efficient model optimization strategy, we conduct extensive experiments on several representative datasets to demonstrate the superiority of our LVTL model. Evaluation experiments show that our proposed model significantly outperforms existing state-of-the-art methods with about 1.16%∼15.36% improvement under different lifelong visual-tactile perception scenarios.}
}
@article{ZHANG2022108219,
title = {Learning sequentially diversified representations for fine-grained categorization},
journal = {Pattern Recognition},
volume = {121},
pages = {108219},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108219},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004003},
author = {Lianbo Zhang and Shaoli Huang and Wei Liu},
keywords = {Fine-grained visual categorization, Convolutional neural networks, Diversity learning, Object recognition},
abstract = {Learning representation carrying rich local information is essential for recognizing fine-grained objects. Existing methods to this task resort to multi-stage frameworks to capture fine-grained information. However, they usually require multiple forward passes of the backbone network, resulting in efficiency deterioration. In this paper, we propose Sequentially Diversified Networks (SDNs) that enrich representation by promoting their diversity while maintaining the extraction efficiency. Specifically, we construct multiple lightweight sub-networks to model mutually different scales of discriminative patterns. The design of these sub-networks follows the sequentially diversified constraint, encouraging them to be varied in spatial attention. By inserting these sub-networks into a single backbone network, SDNs enable information interaction among local regions of the fine-grained image. In this way, SDNs jointly promote diversity in terms of scale and spatial attention in the one-stage pipeline, thereby facilitating the learning of diversified representation efficiently. We evaluate our proposed method on three challenging datasets, namely CUB-200-2011, Stanford-Cars, and FGVC-Aircraft. Experiments demonstrate its effectiveness in learning diversified information. Moreover, our method achieves state-of-the-art performance, only requiring a single forward pass of the backbone network, which reduces inference time noticeably.}
}
@article{ROMEO2022108197,
title = {A Unified Hierarchical XGBoost model for classifying priorities for COVID-19 vaccination campaign},
journal = {Pattern Recognition},
volume = {121},
pages = {108197},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108197},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003794},
author = {Luca Romeo and Emanuele Frontoni},
keywords = {COVID-19, Vaccination, Machine learning, XGBoost, Clinical decision support system, Model interpretability},
abstract = {The current ML approaches do not fully focus to answer a still unresolved and topical challenge, namely the prediction of priorities of COVID-19 vaccine administration. Thus, our task includes some additional methodological challenges mainly related to avoiding unwanted bias while handling categorical and ordinal data with a highly imbalanced nature. Hence, the main contribution of this study is to propose a machine learning algorithm, namely Hierarchical Priority Classification eXtreme Gradient Boosting for priority classification for COVID-19 vaccine administration using the Italian Federation of General Practitioners dataset that contains Electronic Health Record data of 17k patients. We measured the effectiveness of the proposed methodology for classifying all the priority classes while demonstrating a significant improvement with respect to the state of the art. The proposed ML approach, which is integrated into a clinical decision support system, is currently supporting General Pracitioners in assigning COVID-19 vaccine administration priorities to their assistants.}
}
@article{FENG2022108119,
title = {Relation-aware dynamic attributed graph attention network for stocks recommendation},
journal = {Pattern Recognition},
volume = {121},
pages = {108119},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108119},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100306X},
author = {Shibo Feng and Chen Xu and Yu Zuo and Guo Chen and Fan Lin and Jianbing XiaHou},
keywords = {Financial market, Attributed graph attention network, Correlation coefficient, Chinese stock recommendation},
abstract = {The inherent properties of the graph structure of the financial market and the correlation attributes that actually exist in the system inspire us to introduce the concept of the graph to solve the problem of prediction and recommendation in the financial sector. In this paper, we are adhering to the idea of recommending high return ratio stocks and put forward an attributed graph attention network model based on the correlation information, with encoded timing characteristics derived from time series module and global information originating from the stacked graph neural network(GNN) based models, which we called Relation-aware Dynamic Attributed Graph Attention Network (RA-AGAT). On this basis, we have verified the practicality and applicability of the application of graph models in finance. Our innovative structure first captures the local correlation topology information and then introduce a stacked graph neural network structure to recommend Top-N return ratio of stock items. Experiments on the real China A-share market demonstrate that the RA-AGAT architecture is capable of surpassing the previously applicable methods in the prediction and recommendation of stock return ratio.}
}
@article{CHEN2022108156,
title = {Mixture factor analysis with distance metric constraint for dimensionality reduction},
journal = {Pattern Recognition},
volume = {121},
pages = {108156},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108156},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003435},
author = {Jian Chen and Leiyao Liao and Wei Zhang and Lan Du},
keywords = {Dimensionality reduction, Mixture factor analysis, Distance metric constraint, Classification},
abstract = {Dimensionality reduction (DR) is a key preprocessing stage in high-dimensional data classification. Traditional linear DR algorithms, e.g., Linear Discriminant Analysis, transform the original data into a low-dimensional subspace with a linear transformation matrix. However, these methods cannot handle complex nonlinearly separable data. Although some nonlinear DR methods, e.g., Locally Linear Embedding, are proposed to solve this problem, most of them are unsupervised, which only focus on the data structure hidden in the original high-dimensional space, rather than maximizing the inter-class separability of the transformed data, thus reducing the classification accuracy. To tackle this challenge, a novel supervised nonlinear DR algorithm, distance metric restricted mixture factor analysis (DMR-MFA), is proposed for high-dimensional data classification. In DMR-MFA, the original data is divided into several clusters, and the generation of original data in each cluster is described via a factor analysis model. Meanwhile, the distance metric constraint (DMC) is used for maximizing the separability of transformed low-dimensional data from different classes. Moreover, the optimal model parameters are learned via the joint optimization of log-likelihood function and DMC loss function, which makes the DMR-MFA possible to obtain the more separable low-dimensional embeddings while accurately describing the original data. Experimental results on synthetic data, benchmark datasets and high-resolution range profile data demonstrate that our method can handle nonlinearly separable data and improves the classification accuracy of data with high dimensionality.}
}
@article{ZAMBONI2022108252,
title = {Pedestrian trajectory prediction with convolutional neural networks},
journal = {Pattern Recognition},
volume = {121},
pages = {108252},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108252},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004325},
author = {Simone Zamboni and Zekarias Tilahun Kefato and Sarunas Girdzijauskas and Christoffer Norén and Laura {Dal Col}},
keywords = {Trajectory prediction, Pedestrian prediction, Convolutional neural networks},
abstract = {Predicting the future trajectories of pedestrians is a challenging problem that has a range of application, from crowd surveillance to autonomous driving. In literature, methods to approach pedestrian trajectory prediction have evolved, transitioning from physics-based models to data-driven models based on recurrent neural networks. In this work, we propose a new approach to pedestrian trajectory prediction, with the introduction of a novel 2D convolutional model. This new model outperforms recurrent models, and it achieves state-of-the-art results on the ETH and TrajNet datasets. We also present an effective system to represent pedestrian positions and powerful data augmentation techniques, such as the addition of Gaussian noise and the use of random rotations, which can be applied to any model. As an additional exploratory analysis, we present experimental results on the inclusion of occupancy methods to model social information, which empirically show that these methods are ineffective in capturing social interaction.}
}
@article{SAVCHENKO2022108248,
title = {Preference prediction based on a photo gallery analysis with scene recognition and object detection},
journal = {Pattern Recognition},
volume = {121},
pages = {108248},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108248},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004283},
author = {A.V. Savchenko and K.V. Demochkin and I.S. Grechikhin},
keywords = {Scene recognition, Event recognition, Object detection, Recognition of a set of images, CNN (Convolutional neural network), Mobile device},
abstract = {In this paper, a user modeling task is examined by processing mobile device gallery of photos and videos. We propose a novel engine for preferences prediction based on scene recognition, object detection and facial analysis. At first, all faces in a gallery are clustered, and all private photos and videos with faces from large clusters are processed on the embedded system in offline mode. Other photos may be sent to the remote server to be analyzed by very deep sophisticated neural networks. The visual features of each photo are obtained from scene recognition and object detection models. These features are aggregated into a single descriptor in the neural attention unit. The proposed pipeline is implemented in mobile Android application. Experimental results for the Photo Event Collection, Web Image Dataset for Event Recognition and Amazon Fashion data demonstrate the possibility to efficiently process images without significant accuracy degradation.}
}
@article{TAVAKKOL2022108223,
title = {Object-based cluster validation with densities},
journal = {Pattern Recognition},
volume = {121},
pages = {108223},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108223},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004040},
author = {Behnam Tavakkol and Jeongsub Choi and Myong Kee Jeong and Susan L. Albin},
keywords = {Clustering, Clustering validity index, Internal index, Density-based cluster validation, Unsupervised},
abstract = {Clustering validity indices are typically used as tools to find the correct number of clusters in a data set and/or to evaluate the quality of the clusters formed by clustering algorithms. Clustering validity indices measure separation and compactness of clusters. Typically, when applying a clustering algorithm, the input includes the number of clusters. After applying the algorithm with several different numbers of clusters, we determine the number of clusters to be the one with the best validity index. There are two types of clustering validity indices: external indices that are supervised, and internal indices that are unsupervised. The focus of this paper is on internal validity indices. Some existing internal validity indices capture the properties of the clusters by using representative statistics such as mean, variance, diameter, etc., however, these do not perform well when clusters have arbitrary shapes. One approach to overcome this issue is to use the density of the data objects in each cluster. That provides the advantage of capturing the full characteristics of the cluster which is most beneficial when there are clusters with arbitrary shapes. In the literature, a few density-based clustering validity indices have been proposed. However, some of them show poor performance when the clusters are not perfectly separated. Some others perform poorly because they use only representative objects from each cluster instead of all objects. The contribution of this paper is an internal validity index named the object-based clustering validity index with densities (OCVD). OCVD is a single number that averages the density-based contribution of individual data objects to both separation and compactness of clusters. The methodology behind calculating the density-based contributions of the objects is kernel density estimation. We show through several experiments that OCVD performs well in detecting the correct number of clusters in data sets with different cluster shapes including arbitrary shapes.}
}
@article{MOKOENA2022108227,
title = {Why is this an anomaly? Explaining anomalies using sequential explanations},
journal = {Pattern Recognition},
volume = {121},
pages = {108227},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108227},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004088},
author = {Tshepiso Mokoena and Turgay Celik and Vukosi Marivate},
keywords = {Outlier explanation, Sequential feature explanation, Sequential explanation, Anomaly validation, Explainable AI},
abstract = {In most applications, anomaly detection operates in an unsupervised mode by looking for outliers hoping that they are anomalies. Unfortunately, most anomaly detectors do not come with explanations about which features make a detected outlier point anomalous. Therefore, it requires human analysts to manually browse through each detected outlier point’s feature space to obtain the subset of features that will help them determine whether they are genuinely anomalous or not. This paper introduces sequential explanation (SE) methods that sequentially explain to the analyst which features make the detected outlier anomalous. We present two methods for computing SEs called the outlier and sample-based SE that will work alongside any anomaly detector. The outlier-based SE methods use an anomaly detector’s outlier scoring measure guided by a search algorithm to compute the SEs. Meanwhile, the sample-based SE methods employ sampling to turn the problem into a classical feature selection problem. In our experiments, we compare the performances of the different outlier- and sample-based SEs. Our results show that both the outlier and sample-based methods compute SEs that perform well and outperform sequential feature explanations.}
}
@article{HE2022108191,
title = {Creating synthetic minority class samples based on autoencoder extreme learning machine},
journal = {Pattern Recognition},
volume = {121},
pages = {108191},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108191},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003472},
author = {Yu-Lin He and Sheng-Sheng Xu and Joshua Zhexue Huang},
keywords = {Imbalanced classification, Minority class, Majority class, Synthetic samples creation, SMOTE},
abstract = {This paper reports a new method (simplified as AE-ELM-SynMin) to create the Synthetic Minority class samples for imbalanced classification based on AutoEncoder Extreme Learning Machine (AE-ELM). AE-ELM-SynMin first trains an AE-ELM which is a special ELM with the same input and output, i.e., the original minority class samples. Second, the crossover, mutation and filtration operations are conducted on the hidden-layer output of AE-ELM and then the synthetic hidden-layer output is obtained. Third, the synthetic minority class samples are created by decoding the synthetic hidden-layer output with output-layer weights of AE-ELM. AE-ELM-SynMin guarantees that the synthetic minority class has the higher information amount than original minority class and meanwhile keeps the consistent probability distribution with the original minority class. The experimental results demonstrate the better imbalanced classification performances of AE-ELM-SynMin in comparison with the regular synthetic minority over-sampling technique (Regular-SMOTE) and its variants, e.g., Borderline-SMOTE, Random-SMOTE, and SMOTE-IPF.}
}
@article{LI2022108256,
title = {A hierarchical model for learning to understand head gesture videos},
journal = {Pattern Recognition},
volume = {121},
pages = {108256},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108256},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004362},
author = {Jiachen Li and Songhua Xu and Xueying Qin},
keywords = {Video understanding, Head gesture videos, Stacked BLSTM, Multi-task learning, Transfer learning},
abstract = {Head gesture videos recorded of a person bear rich information about the individual. Automatically understanding these videos can empower many useful human-centered applications in areas such as smart health, education, work safety and security. To understand a video’s content, low-level head gesture signals carried in the video that capture characteristics of both human postures and motions need to be translated into high-level semantic labels. To meet this aim, we propose a hierarchical model for learning to understand head gesture videos. Given a head gesture video of an arbitrary length, the model first segments the full-length video into multiple short clips for clip-based feature extraction. Multiple base feature extraction procedures are then independently tuned via a set of peripheral learning tasks without consuming any labels of the goal task. These independently derived base features are subsequently aggregated through a multi-task learning framework, coupled with a feature dimensionality reduction module, to optimally learn to accomplish the end video understanding task in an weakly supervised manner, utilizing the limited amount of video labels available of the goal task. Experimental results show that the hierarchical model is superior to multiple state-of-the-art peer methods in tackling versatile video understanding tasks.}
}
@article{WU2022108204,
title = {Estimating fund-raising performance for start-up projects from a market graph perspective},
journal = {Pattern Recognition},
volume = {121},
pages = {108204},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108204},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100385X},
author = {Likang Wu and Zhi Li and Hongke Zhao and Qi Liu and Enhong Chen},
keywords = {Crowdfunding, Market environment modeling, Graph neural network},
abstract = {In the online innovation market, the fund-raising performance of the start-up project is a concerning issue for creators, investors and platforms. Unfortunately, existing studies always focus on modeling the fund-raising process after the publishment of a project but the predicting of a project attraction in the market before setting up is largely unexploited. Usually, this prediction is always with great challenges to making a comprehensive understanding of both the start-up project and market environment. To that end, in this paper, we present a focused study on this important problem from a market graph perspective. Specifically, we propose a Graph-based Market Environment (GME) model for predicting the fund-raising performance of the unpublished project by exploiting the market environment. In addition, we discriminatively model the project competitiveness and market preferences by designing two graph-based neural network architectures and incorporating them into a joint optimization stage. Furthermore, to explore the information propagation problem with dynamic environment in a large-scale market graph, we extend the GME model with parallelizing competitiveness quantification and hierarchical propagation algorithm. Finally, we conduct extensive experiments on real-world data. The experimental results clearly demonstrate the effectiveness of our proposed model.}
}
@article{ALAM2022108200,
title = {Unified learning approach for egocentric hand gesture recognition and fingertip detection},
journal = {Pattern Recognition},
volume = {121},
pages = {108200},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108200},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003824},
author = {Mohammad Mahmudul Alam and Mohammad Tariqul Islam and S.M. Mahbubur Rahman},
keywords = {Convolutional neural network, Fingertip detection, Gesture recognition, Human-computer interaction, Unified detection},
abstract = {Head-mounted device-based human-computer interaction often requires egocentric recognition of hand gestures and fingertips detection. In this paper, a unified approach of egocentric hand gesture recognition and fingertip detection is introduced. The proposed algorithm uses a single convolutional neural network to predict the probabilities of finger class and positions of fingertips in one forward propagation. Instead of directly regressing the positions of fingertips from the fully connected layer, the ensemble of the position of fingertips is regressed from the fully convolutional network. Subsequently, the ensemble average is taken to regress the final position of fingertips. Since the whole pipeline uses a single network, it is significantly fast in computation. Experimental results show that the proposed method outperforms the existing fingertip detection approaches including the Direct Regression and the Heatmap-based framework. The effectiveness of the proposed method is also shown in-the-wild scenario as well as in a use-case of virtual reality.}
}
@article{WANG2022108251,
title = {A novel GCN-based point cloud classification model robust to pose variances},
journal = {Pattern Recognition},
volume = {121},
pages = {108251},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108251},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004313},
author = {Huafeng Wang and Yaming Zhang and Wanquan Liu and Xianfeng Gu and Xin Jing and Zicheng Liu},
keywords = {Point cloud, Pose robust, Graph convolutional network, Classification},
abstract = {Point cloud data can be produced by many depth sensors, such as Light Detection and Ranging (LIDAR) and RGB-D cameras, and they are widely used in broad applications of robotic navigation and remote-sensing for the understanding of environment. Hence, new techniques for object representation and classification based on 3D point cloud are becoming increasingly in high demand. Due to the irregularity of the object shape, the point cloud-based object recognition is a very challenging task, especially the pose variances of a point cloud will impose many difficulties. In this paper, we tackle the challenge of pose variances in object classification based on point cloud by developing a novel end-to-end pose robust graph convolutional network. Technically, we first represent the point cloud using the spherical system instead of the traditional Cartesian system for simplicity of computation and representation. Then a pose auxiliary network is constructed with an aim to estimate the pose changes in terms of rotation angles. Finally, a graph convolutional network is constructed for object classification against the pose variations of point cloud. The experimental results show the new model outperforms the existing approaches (such as PointNet and PointNet++) on the classification task when conducting experiments on both the ModelNet40 and the ShapeNetCore dataset with a series of random rotations of a 3D point cloud. Specifically, we obtain 73.02% accuracy for classification task on the ModelNet40 with delaunay triangulation algorithm, which is much better than the state of the art algorithms, such as PointNet and PointCNN.}
}
@article{ZHANG2022108150,
title = {4D computed tomography super-resolution reconstruction based on tensor product and nuclear norm optimization},
journal = {Pattern Recognition},
volume = {121},
pages = {108150},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108150},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100337X},
author = {Shu Zhang and Youshen Xia},
keywords = {4D-CT, Super-resolution, Tensor product, Optimization, Nuclear norm},
abstract = {Four-dimensional computed tomography (4D-CT) has been widely used in preoperative evaluation and radiotherapy planning of lung tumors. To reduce the damage to healthy tissue, it is a better way to limit the scan time and the number of CT slices. Yet, it leads to the reduction of CT image resolution in the superior-inferior direction. To improve the resolution of the 4D-CT image, we propose a super-resolution (SR) algorithm based on tensor product and nuclear norm optimization. The proposed cost function includes a tensor fidelity term and a nuclear norm regularization term. The tensor fidelity term consists of low-resolution (LR) and high-resolution (HR) image tensors, as well as SR operators. The nuclear norm regularization term is used to preserve the operators’ low-rank. The optimization problem can be effectively solved by an alternative direction method of the multipliers (ADMM) technique. The SR operators can extract useful information from each dimension of LR image tensors to enhance the equality of 4D-CT SR reconstruction. Experimental results show that the proposed method can preserve the edge details of the 4D-CT image. Moreover, quantitative comparisons show that the proposed method increases peak signal-to-noise ratio from 1.5 dB to 5.5 dB, structural similarity index from 2% to 11%, visual information fidelity from 6% to 20%, edge model-based blur metric from 5% to 15%, and decreases the spatial-spectral entropy-based quality index from 1% to 5%, compared with conventional 4D-CT SR algorithms.}
}
@article{VAQUERO2022108205,
title = {Tracking more than 100 arbitrary objects at 25 FPS through deep learning},
journal = {Pattern Recognition},
volume = {121},
pages = {108205},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108205},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003861},
author = {Lorenzo Vaquero and Víctor M. Brea and Manuel Mucientes},
keywords = {Multiple visual object tracking, Motion estimation, Deep learning, Siamese networks},
abstract = {Most video analytics applications rely on object detectors to localize objects in frames. However, when real-time is a requirement, running the detector at all the frames is usually not possible. This is somewhat circumvented by instantiating visual object trackers between detector calls, but this does not scale with the number of objects. To tackle this problem, we present SiamMT, a new deep learning multiple visual object tracking solution that applies single-object tracking principles to multiple arbitrary objects in real-time. To achieve this, SiamMT reuses feature computations, implements a novel crop-and-resize operator, and defines a new and efficient pairwise similarity operator. SiamMT naturally scales up to several dozens of targets, reaching 25 fps with 122 simultaneous objects for VGA videos, or up to 100 simultaneous objects in HD720 video. SiamMT has been validated on five large real-time benchmarks, achieving leading performance against current state-of-the-art trackers.}
}
@article{2022108320,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {121},
pages = {108320},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(21)00500-8},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005008}
}
@article{GUARRASI2022108242,
title = {Pareto optimization of deep networks for COVID-19 diagnosis from chest X-rays},
journal = {Pattern Recognition},
volume = {121},
pages = {108242},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108242},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004234},
author = {Valerio Guarrasi and Natascha Claudia D’Amico and Rosa Sicilia and Ermanno Cordelli and Paolo Soda},
keywords = {COVID-19, X-ray, Deep-learning, Multi-expert systems, Optimization, Convolutional neural networks},
abstract = {The year 2020 was characterized by the COVID-19 pandemic that has caused, by the end of March 2021, more than 2.5 million deaths worldwide. Since the beginning, besides the laboratory test, used as the gold standard, many applications have been applying deep learning algorithms to chest X-ray images to recognize COVID-19 infected patients. In this context, we found out that convolutional neural networks perform well on a single dataset but struggle to generalize to other data sources. To overcome this limitation, we propose a late fusion approach where we combine the outputs of several state-of-the-art CNNs, introducing a novel method that allows us to construct an optimum ensemble determining which and how many base learners should be aggregated. This choice is driven by a two-objective function that maximizes, on a validation set, the accuracy and the diversity of the ensemble itself. A wide set of experiments on several publicly available datasets, accounting for more than 92,000 images, shows that the proposed approach provides average recognition rates up to 93.54% when tested on external datasets.}
}
@article{LI2022108259,
title = {Learning common and label-specific features for multi-Label classification with correlation information},
journal = {Pattern Recognition},
volume = {121},
pages = {108259},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108259},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004398},
author = {Junlong Li and Peipei Li and Xuegang Hu and Kui Yu},
keywords = {Multi-label classification, Label-specific features, Common features, Instance correlation},
abstract = {In multi-label classification, many existing works only pay attention to the label-specific features and label correlation while they ignore the common features and instance correlation, which are also essential for building a competitive classifier. Besides, existing works usually depend on the assumption that they tend to have the similar label-specific features if two labels are correlated. However, this assumption cannot always hold in some cases. Therefore, in this paper, we propose a new approach of learning common and label-specific features for multi-label classification using the correlation information from labels and instances. First, we introduce l2,1-norm and l1-norm regularizers to learn common and label-specific features simultaneously. Second, we use a regularizer to constrain label correlations on label outputs instead of coefficient matrix. Finally, instance correlations are also considered through the k-nearest neighbor mechanism. Comprehensive experiments manifest the superiority of our proposed approach against other well-established multi-label learning algorithms for label-specific features.}
}
@article{SI2022108196,
title = {Consistent and diverse multi-View subspace clustering with structure constraint},
journal = {Pattern Recognition},
volume = {121},
pages = {108196},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108196},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003782},
author = {Xiaomeng Si and Qiyue Yin and Xiaojie Zhao and Li Yao},
keywords = {Subspace self-representation, Multi-view clustering, Consistency, Diversity, Clustering structure},
abstract = {Multi-view subspace clustering algorithms have recently been developed to process multi-view dataset clustering by accurately depicting the essential characteristics of multi-view data. Most existing methods focus on conduct self-representation property using a consistent representation and a set of specific representations with well-designed regularization to learn the common and specific knowledge among different views. However, specific representations only contain the unique information of each individual view, which limits their ability to fully excavate the diversity of multi-view data to enhance the complementarity among different views. Moreover, when conducting multi-view subspace clustering, the learned subspace self-representation and clustering are sequential and independent, which lacks consideration of the interaction between representation learning and the final clustering calculation. In this paper, a novel method termed consistent and diverse multi-view subspace clustering with structure constraint (CDMSC2) is proposed to overcome the above-described deficiencies. (1) An exclusivity constraint term is employed to enhance the diversity of specific representations among different views for modeling consistency and diversity in a unified framework. (2) A clustering structure constraint is imposed on the subspace self-representation by factorizing the learned subspace self-representation into the cluster centroids and the cluster assignments with the goal of obtaining a clustering-oriented subspace self-representation. In addition, we carefully designed an efficient optimization algorithm to solve the objective function through relaxation and alternating minimization. Extensive experiments on five benchmark datasets in terms of six evaluation metrics demonstrate that our method outperforms the state-of-the-art methods.}
}
@article{SOGI2022108190,
title = {Constrained mutual convex cone method for image set based recognition},
journal = {Pattern Recognition},
volume = {121},
pages = {108190},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108190},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003502},
author = {Naoya Sogi and Rui Zhu and Jing-Hao Xue and Kazuhiro Fukui},
keywords = {Image-set based method, Convex cone representation, Multiple angles},
abstract = {In this paper, we propose convex cone-based frameworks for image-set classification. Image-set classification aims to classify a set of images, usually obtained from video frames or multi-view cameras, into a target object. To accurately and stably classify a set, it is essential to accurately represent structural information of the set. There are various image features, such as histogram-based features and convolutional neural network features. We should note that most of them have non-negativity and thus can be effectively represented by a convex cone. This leads us to introduce the convex cone representation to image-set classification. To establish a convex cone-based framework, we mathematically define multiple angles between two convex cones, and then use the angles to define the geometric similarity between them. Moreover, to enhance the framework, we introduce two discriminant spaces. We first propose a discriminant space that maximizes gaps between cones and minimizes the within-class variance. We then extend it to a weighted discriminant space by introducing weights on the gaps to deal with complicated data distribution. In addition, to reduce the computational cost of the proposed methods, we develop a novel strategy for fast implementation. The effectiveness of the proposed methods is demonstrated experimentally by using five databases.}
}
@article{GNANHA2022108222,
title = {The residual generator: An improved divergence minimization framework for GAN},
journal = {Pattern Recognition},
volume = {121},
pages = {108222},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108222},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004039},
author = {Aurele Tohokantche Gnanha and Wenming Cao and Xudong Mao and Si Wu and Hau-San Wong and Qing Li},
keywords = {Generative adversarial networks, Image synthesis, Deep learning},
abstract = {GAN is a generative modelling framework which has been proven as able to minimise various types of divergence measures under an optimal discriminator. However, there is a gap between the loss function of GAN used in theory and in practice. In theory, the proof of the Jensen divergence minimisation involves the min-max criterion, but in practice the non-saturating criterion is instead used to avoid gradient vanishing. We argue that the formulation of divergence minimization via GAN is biased and may yield a poor convergence of the algorithm. In this paper, we propose the Residual Generator for GAN (Rg-GAN), which is inspired by the closed-loop control theory, to bridge the gap between theory and practice. Rg-GAN minimizes the residual between the loss of the generated data to be real and the loss of the generated data to be fake from the perspective of the discriminator. In this setting, the loss terms of the generator depend only on the generated data and therefore contribute to the optimisation of the model. We formulate the residual generator for standard GAN and least-squares GAN and show that they are equivalent to the minimisation of reverse-KL divergence and a novel instance of f-divergence, respectively. Furthermore, we prove that Rg-GAN can be reduced to Integral Probability Metrics (IPMs) GANs (e.g., Wasserstein GAN) and bridge the gap between IPMs and f-divergence. Additionally, we further improve on Rg-GAN by proposing a loss function for the discriminator that has a better discrimination ability. Experiments on synthetic and natural images data sets show that Rg-GAN is robust to mode collapse, and improves the generation quality of GAN in terms of FID and IS scores.}
}
@article{SHAO2022108247,
title = {Towards open-set touchless palmprint recognition via weight-based meta metric learning},
journal = {Pattern Recognition},
volume = {121},
pages = {108247},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108247},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004271},
author = {Huikai Shao and Dexing Zhong},
keywords = {Biometrics, Palmprint recognition, Meta learning, Metric learning},
abstract = {Touchless biometrics has become significant in the wake of novel coronavirus 2019 (COVID-19). Due to the convenience, user-friendly, and high-accuracy, touchless palmprint recognition shows great potential when the hygiene issues are considered during COVID-19. However, previous palmprint recognition methods are mainly focused on close-set scenario. In this paper, a novel Weight-based Meta Metric Learning (W2ML) method is proposed for accurate open-set touchless palmprint recognition, where only a part of categories is seen during training. Deep metric learning-based feature extractor is learned in a meta way to improve the generalization ability. Multiple sets are sampled randomly to define support and query sets, which are further combined into meta sets to constrain the set-based distances. Particularly, hard sample mining and weighting are adopted to select informative meta sets to improve the efficiency. Finally, embeddings with obvious inter-class and intra-class differences are obtained as features for palmprint identification and verification. Experiments are conducted on four palmprint benchmarks including fourteen constrained and unconstrained palmprint datasets. The results show that our W2ML method is more robust and efficient in dealing with open-set palmprint recognition issue as compared to the state-of-the-arts, where the accuracy is increased by up to 9.11% and the Equal Error Rate (EER) is decreased by up to 2.97%.}
}
@article{YIN2022108238,
title = {Universal multi-Source domain adaptation for image classification},
journal = {Pattern Recognition},
volume = {121},
pages = {108238},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108238},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004192},
author = {Yueming Yin and Zhen Yang and Haifeng Hu and Xiaofu Wu},
keywords = {Universal domain adaptation, Multi-source domain adaptation, Universal multi-source domain adaptation, Universal multi-source adaptation network, Pseudo-margin vector},
abstract = {Unsupervised domain adaptation (DA) enables intelligent models to learn transferable knowledge from a labeled source domain and adapt to a similar but unlabeled target domain. Studies showed that knowledge could be transferred from one source domain to another unknown target domain, called Universal DA (UDA). However, there is often more than one source domain in the real-world application to be exploited for DA. In this paper, we formally propose a more general domain adaptation setting for image classification, universal multi-source DA (UMDA), where the label sets of multiple source domains can be different, and the label set of the target domain is completely unknown. The main challenge in UMDA is to identify the common label set among each source and target domain and keep the model scalable as the number of source domains increases. In the face of this challenge, we propose a universal multi-source adaptation network (UMAN) to solve the DA problem without increasing the complexity of the model in various UMDA settings. In UMAN, the reliability of each known class belonging to the common label set is estimated via a novel pseudo-margin vector and its weighted form, which helps adversarial training better align the distributions of multiple source domains and target domain. Moreover, the theoretical guarantee for UMAN is also provided. Massive experimental results show that existing UDA and multi-source DA (MDA) methods cannot be directly deployed to UMDA, and the proposed UMAN achieves the state-of-the-art performance in various UMDA settings.}
}
@article{ZHANG2022108234,
title = {Deep anomaly detection with self-supervised learning and adversarial training},
journal = {Pattern Recognition},
volume = {121},
pages = {108234},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108234},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004155},
author = {Xianchao Zhang and Jie Mu and Xiaotong Zhang and Han Liu and Linlin Zong and Yuangang Li},
keywords = {Deep anomaly detection, Self-supervised learning, Adversarial training},
abstract = {Deep anomaly detection, which utilizes neural networks to discover anomalies, is a vital research topic in pattern recognition. With the burgeoning of inference mechanism, inference-based methods show the promising performance. However, inference-based methods have two limitations: (1) they use an adversarial training way to learn data features. Such training way fails to learn task-specific features which can be conducive to capture the difference between normal and anomaly data. (2) The structure of detection network cannot capture the marginal distributions of normal data and corresponding features, which influences on the performance of anomaly detection. To overcome these limitations, this paper proposes a deep adversarial anomaly detection (DAAD) method. Specifically, an auxiliary task with self-supervised learning is first designed to learn task-specific features. Then a deep adversarial training (DAT) model is constructed to capture marginal distributions of normal data in different spaces. In addition, a majority voting strategy is applied to obtain reliable detection results. Experimental results on image and sequence datasets show that proposed method performs significantly better than many strong baselines.}
}
@article{JIA2022108226,
title = {Variable weight algorithm for convolutional neural networks and its applications to classification of seizure phases and types},
journal = {Pattern Recognition},
volume = {121},
pages = {108226},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108226},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004076},
author = {Guangyu Jia and Hak-Keung Lam and Kaspar Althoefer},
keywords = {Variable weight convolutional neural networks, Machine learning, Seizure phase classification, Seizure type classification},
abstract = {Deep learning techniques have recently achieved impressive results and raised expectations in the domains of medical diagnosis and physiological signal processing. The widely adopted methods include convolutional neural networks (CNNs) and recurrent neural networks (RNNs). However, the existing models possess static connection weights between layers, which might limit the generalization capability and the classification performance of the models as the weights of different layers are fixed after training. Furthermore, to deal with a large amount of data, a neural network with a sufficiently large size is required. This paper proposes the variable weight convolutional neural networks (VWCNNs), which are a type of network structure employing dynamic weights instead of static weights in their convolutional layers and fully-connected layers. VWCNNs are able to adapt to different characteristics of input data and can be viewed as an infinite number of traditional, fixed-weight CNNs. We will show that the proposed VWCNN structure outperforms the conventional CNN in terms of the classification accuracy, generalization capability, and robustness when the inputs are contaminated by noise. In this paper, VWCNNs are applied to the classification of three seizure phases (seizure-free, pre-seizure and seizure) based on measured electroencephalography (EEG) data. VWCNNs achieve 100% test accuracy and show strong robustness in the classification of the three seizure phases, and thus show the potential to be a useful classification tool for medical diagnosis. Furthermore, the classification of seven types of seizures is investigated in this paper using the world’s largest open source database of seizure recordings, TUH EEG seizure corpus. Comparisons with conventional CNNs, RNN, MobileNet, ResNet, DenseNet and traditional machine learning methods including random forest, decision tree, support vector machine, K-nearest neighbours, standard neural networks, and Naïve Bayes are being conducted using realistic test data sets. The results demonstrate that VWCNNs have advantages over other classifiers in terms of classification accuracy and robustness.}
}
@article{JIN2022108159,
title = {Delving deep into spatial pooling for squeeze-and-excitation networks},
journal = {Pattern Recognition},
volume = {121},
pages = {108159},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108159},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003460},
author = {Xin Jin and Yanping Xie and Xiu-Shen Wei and Bo-Rui Zhao and Zhao-Min Chen and Xiaoyang Tan},
keywords = {Convolutional neural networks, Squeeze-and-excitation, Spatial pooling, Base model},
abstract = {Squeeze-and-Excitation (SE) blocks have demonstrated significant accuracy gains for state-of-the-art deep architectures by re-weighting channel-wise feature responses. The SE block is an architecture unit that integrates two operations: a squeeze operation that employs global average pooling to aggregate spatial convolutional features into a channel feature, and an excitation operation that learns instance-specific channel weights from the squeezed feature to re-weight each channel. In this paper, we revisit the squeeze operation in SE blocks, and shed lights on why and how to embed rich (both global and local) information into the excitation module at minimal extra costs. In particular, we introduce a simple but effective two-stage spatial pooling process: rich descriptor extraction and information fusion. The rich descriptor extraction step aims to obtain a set of diverse (i.e., global and especially local) deep descriptors that contain more informative cues than global average-pooling. While, absorbing more information delivered by these descriptors via a fusion step can aid the excitation operation to return more accurate re-weight scores in a data-driven manner. We validate the effectiveness of our method by extensive experiments on ImageNet for image classification and on MS-COCO for object detection and instance segmentation. For these experiments, our method achieves consistent improvements over the SENets on all tasks, in some cases, by a large margin.}
}
@article{PENG2022108199,
title = {Context-aware co-supervision for accurate object detection},
journal = {Pattern Recognition},
volume = {121},
pages = {108199},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108199},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003812},
author = {Junran Peng and Haoquan Wang and Shaolong Yue and Zhaoxiang Zhang},
keywords = {Object detection, Co-supervised, Contextual cues},
abstract = {State-of-the-art object detection approaches are often composed of two stages, namely, proposing a number of regions on an image and classifying each of them into one class. Both stages share a network backbone which builds visual features in a bottom-up manner. In this paper, we advocate the importance of equipping two-stage detectors with top-down signals, in order to which provides high-level contextual cues to complement low-level features. In practice, this is implemented by adding a side path in the detection head to predict all object classes in the image, which is co-supervised by image-level semantics and requires little extra overheads. Our approach is easily applied to two popular object detection algorithms, and achieves consistent performance gain in the MS-COCO dataset.}
}
@article{WU2022108212,
title = {Recursive multi-model complementary deep fusion for robust salient object detection via parallel sub-networks},
journal = {Pattern Recognition},
volume = {121},
pages = {108212},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108212},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003939},
author = {Zhenyu Wu and Shuai Li and Chenglizhao Chen and Aimin Hao and Hong Qin},
keywords = {Salient object detection, Deep learning, Multi-model fusion},
abstract = {Fully convolutional networks have shown outstanding performance in the salient object detection (SOD) field. The state-of-the-art (SOTA) methods have a tendency to become deeper and more complex, which easily homogenize their learned deep features, resulting in a clear performance bottleneck. In sharp contrast to the conventional “deeper” schemes, this paper proposes a “wider” network architecture which consists of parallel sub-networks with totally different network architectures. In this way, those deep features obtained via these two sub-networks will exhibit large diversity, which will have large potential to be able to complement with each other. However, a large diversity may easily lead to the feature conflictions, thus we use the dense short-connections to enable a recursively interaction between the parallel sub-networks, pursuing an optimal complementary status between multi-model deep features. Finally, all these complementary multi-model deep features will be selectively fused to make high-performance salient object detections. Extensive experiments on several famous benchmarks clearly demonstrate the superior performance, good generalization, and powerful learning ability of the proposed wider framework.}
}
@article{SHEN2022108221,
title = {Learning scale awareness in keypoint extraction and description},
journal = {Pattern Recognition},
volume = {121},
pages = {108221},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108221},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004027},
author = {Xuelun Shen and Cheng Wang and Xin Li and Yifan Peng and Zijian He and Chenglu Wen and Ming Cheng},
keywords = {Keypoint detection, Keypoint description, Image matching, Structure from motion, 3D reconstruction},
abstract = {To recover relative camera motion accurately and robustly, establishing a set of point-to-point correspondences in the pixel space is an essential yet challenging task in computer vision. Even though multi-scale design philosophy has been used with significant success in computer vision tasks, such as object detection and semantic segmentation, learning-based image matching has not been fully exploited. In this work, we explore a scale awareness learning approach in finding pixel-level correspondences based on the intuition that keypoints need to be extracted and described on an appropriate scale. With that insight, we propose a novel scale-aware network and then develop a new fusion scheme that derives high-consistency response maps and high-precision descriptions. We also revise the Second Order Similarity Regularization (SOSR) to make it more effective for the end-to-end image matching network, which leads to significant improvement in local feature descriptions. Experimental results run on multiple datasets demonstrate that our approach performs better than state-of-the-art methods under multiple criteria.}
}
@article{SHI2022108170,
title = {Action recognition via pose-based graph convolutional networks with intermediate dense supervision},
journal = {Pattern Recognition},
volume = {121},
pages = {108170},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108170},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003575},
author = {Lei Shi and Yifan Zhang and Jian Cheng and Hanqing Lu},
keywords = {Action recognition, Skeleton},
abstract = {Pose-based action recognition has drawn considerable attention recently. Existing methods exploit the joint position to extract body-part features from the activation maps of the backbone CNN to assist human action recognition. However, there are two limitations: (1) the body-part features are independently used or simply concatenated to obtain a representation, where the prior knowledge about the structured correlations between body parts are not fully exploited; (2) the backbone CNN, from which the body-part features are extracted, is “lazy”. It always contents itself with identifying patterns from the most discriminative areas of the input, which causes no information on the features extracted from other areas. This consequently hampers the performance of the followed aggregation process and makes the model easy to be misled by the training data bias. To address these problems, we encode the body-part features into a human-based spatiotemporal graph and employ a light-weight graph convolutional module to explicitly model the dependencies between body parts. Besides, we introduce a novel intermediate dense supervision to promote the backbone CNN to treat all regions equally, which is simple and effective, without extra parameters and computations. The proposed approach, namely, the pose-based graph convolutional network (PGCN), is evaluated on three popular benchmarks, where our approach significantly outperforms the state-of-the-art methods.}
}
@article{WAN2022108146,
title = {Edge computing enabled video segmentation for real-time traffic monitoring in internet of vehicles},
journal = {Pattern Recognition},
volume = {121},
pages = {108146},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108146},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003332},
author = {Shaohua Wan and Songtao Ding and Chen Chen},
keywords = {Video segmentation, Key frames extraction, Edge computing, YOLOv3},
abstract = {In the Internet of Things enabled intelligent transportation systems, a huge amount of vehicle video data has been generated and real-time and accurate video analysis are very important and challenging work, especially in situations with complex street scenes. Therefore, we propose edge computing based video pre-processing to eliminate the redundant frames, so that we migrate the partial or all the video processing task to the edge, thereby diminishing the computing, storage and network bandwidth requirements of the cloud center, and enhancing the effectiveness of video analyzes. To eliminate the redundancy of the traffic video, the magnitude of motion detection based on spatio-temporal interest points (STIP) and the multi-modal linear features combination are presented which splits a video into super frame segments of interests. After that, we select the key frames from these interesting segments of the long videos with the design and detection of the prominent region. Finally, the extensive numerical experimental verification results show our methods are superior to the previous algorithms for different stages of the redundancy elimination, video segmentation, key frame selection and vehicle detection.}
}
@article{VYUGIN2022108193,
title = {Online aggregation of probability forecasts with confidence},
journal = {Pattern Recognition},
volume = {121},
pages = {108193},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108193},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003496},
author = {Vladimir V’yugin and Vladimir Trunov},
keywords = {On-line learning, Prediction with expert advice, Aggregating algorithm, Probabilistic prediction, Continuous ranked probability score (), Smooth confidence levels for experts},
abstract = {The paper presents numerical experiments and some theoretical developments in prediction with expert advice (PEA). One experiment deals with predicting electricity consumption depending on temperature and uses real data. As the pattern of dependence can change with season and time of the day, the domain naturally admits PEA formulation with experts having different “areas of expertise”. We consider the case where several competing methods produce online predictions in the form of probability distribution functions. The dissimilarity between a probability forecast and an outcome is measured by a loss function (scoring rule). A popular example of scoring rule for continuous outcomes is Continuous Ranked Probability Score (CRPS). In this paper the problem of combining probabilistic forecasts is considered in the PEA framework. We show that CRPS is a mixable loss function and then the time-independent upper bound for the regret of the Vovk aggregating algorithm using CRPS as a loss function can be obtained. Also, we incorporate a “smooth” version of the method of specialized experts in this scheme which allows us to combine the probabilistic predictions of the specialized experts with overlapping domains of their competence.}
}
@article{MA2022108216,
title = {Joint multi-label learning and feature extraction for temporal link prediction},
journal = {Pattern Recognition},
volume = {121},
pages = {108216},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108216},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003976},
author = {Xiaoke Ma and Shiyin Tan and Xianghua Xie and Xiaoxiong Zhong and Jingjing Deng},
keywords = {Temporal link prediction, Non-negative matrix factorization, Multi-label learning, Dynamic networks},
abstract = {Networks derived from various disciplinary of sociality and nature are dynamic and incomplete, and temporal link prediction has wide applications in recommendation system and data mining system, etc. The current algorithms first obtain features by exploiting the topological or latent structure of networks, and then predict temporal links based on the obtained features. These algorithms are criticized by the separation of feature extraction and link prediction, which fails to fully characterize the dynamics of networks, resulting in undesirable performance. To overcome this problem, we propose a novel algorithm by joint multi-label learning and feature extraction (called MLjFE), where temporal link prediction and feature extraction are integrated into an overall objective function. The main advantage of MLjFE is that the features and parameter matrix for temporal link prediction are simultaneously learned during optimization procedure, which is more precise to capture dynamics of networks, improving the performance of algorithms. The experimental results on a number of artificial and real-world temporal networks demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods, showing joint learning with feature extraction and temporal link prediction is promising.}
}
@article{SHENG2022108254,
title = {Learning to schedule multi-NUMA virtual machines via reinforcement learning},
journal = {Pattern Recognition},
volume = {121},
pages = {108254},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108254},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004349},
author = {Junjie Sheng and Yiqiu Hu and Wenli Zhou and Lei Zhu and Bo Jin and Jun Wang and Xiangfeng Wang},
keywords = {Dynamic virtual machine scheduling, Multi-NUMA, Reinforcement learning, Cloud computing},
abstract = {With the rapid development of cloud computing, the importance of dynamic virtual machine scheduling is increasing. Existing works formulate the VM scheduling as a bin-packing problem and design greedy methods to solve it. However, cloud service providers widely adopt multi-NUMA architecture servers in recent years, and existing methods do not consider the architecture. This paper formulates the multi-NUMA VM scheduling into a novel structured combinatorial optimization and transforms it into a reinforcement learning problem. We propose a reinforcement learning algorithm called SchedRL with a delta reward scheme and an episodic guided sampling strategy to solve the problem efficiently. Evaluating on a public dataset of Azure under two different scenarios, our SchedRL outperforms FirstFit and BestFit on the fulfill number and allocation rate.}
}
@article{2022108134,
title = {Expression of concern: “What-and-where to match: Deep spatially multiplicative integration networks for person re-identification” [Pattern Recognition, Volume 76, April 2018, Pages 727-738]},
journal = {Pattern Recognition},
volume = {121},
pages = {108134},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108134},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003216}
}
@article{ZHAO2022108229,
title = {Learning discriminative region representation for person retrieval},
journal = {Pattern Recognition},
volume = {121},
pages = {108229},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108229},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004106},
author = {Yang Zhao and Xiaohan Yu and Yongsheng Gao and Chunhua Shen},
keywords = {Person retrieval, Region representation},
abstract = {Region-level representation learning plays a key role in providing discriminative information for person retrieval. Current methods rely on heuristically coarse-grained region strips or directly borrow pixel-level annotations from pretrained human parsing models for region representation learning. How to learn a discriminative region representation within fine-grained segments while avoiding expensive pixel-level annotations is rarely discussed. To that end, we introduce a novel identity-guided human region segmentation (HRS) method for person retrieval. Via learning a set of distinct region bases that are consistent across a given dataset, HRS can predict informative region segments by grouping intermediate feature vectors based on their similarity to these bases. The predicted segments are iteratively refined for discriminative region representation learning. HRS enjoys two advantages: (1) HRS learns region segmentation using only identity labels, making it a much more practical solution to person retrieval. (2) By jointly learning global appearance and local granularity cues, HRS enables a comprehensive feature representation learning. We verify the effectiveness of the proposed HRS on four challenging benchmark datasets of Market1501, DukeMTMC-reID, CUHK03, and Occluded-DukeMTMC. Extensive experiments demonstrate superior performance over the state-of-the-art region-based methods. For instance, on the CUHK03-labeled dataset, the performance increases from 74.1% mAP and 76.5% rank-1 accuracy to 81.5% (+7.4%) mAP and 83.2% (+6.7%) rank-1 accuracy.}
}
@article{YANG2022108208,
title = {SAR-to-optical image translation based on improved CGAN},
journal = {Pattern Recognition},
volume = {121},
pages = {108208},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108208},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003897},
author = {Xi Yang and Jingyi Zhao and Ziyu Wei and Nannan Wang and Xinbo Gao},
keywords = {SAR-to-optical image translation, Chromatic aberration loss, ICGAN},
abstract = {SAR images have the advantages of being less susceptible to clouds and light, while optical images conform to the human vision system. Both of them are widely applied in the field of scene classification, natural environment monitoring, disaster warning, etc. However, due to the speckle noise caused by the SAR imaging principle, it is difficult for people to distinguish the ground objects from complex background without professional knowledge. One commonly used solution is to exploit Generative Adversarial Networks (GAN) to translate SAR images to optical images which is able to clearly present ground objects with rich color information, i.e., SAR-to-optical image translation. Traditional GAN-based translation methods are apt to cause blurring of contour, disappearance of texture and inconsistency of color. To this end, we propose an improved conditional GAN (ICGAN) method. Compared with the basic CGAN model, the translation ability of our method is improved in the following three aspects. (1) Contour sharpness. We utilize the parallel branches to combine low-level and high-level features, and thus the image contour information is improved without the influence of noise. (2) Texture fine-grainedness. We discriminate the image using multi-scale receptive fields to enrich the local and global texture features of the image. (3) Color fidelity. We use the chromatic aberration loss which is based on Gaussian blur convolution to reduce the color gap between the generated image and the real optical image. Our method considers both the visual layer and the conceptual layer of the image to complete the SAR-to-optical image translation task. The model is able to preserve the contours and textures of the SAR image, while more closely approximates the colors of the ground truth. The experimental results show that the generated image not only has preferable results in visual effects and favorable evaluation metrics (subjective and objective), but also achieves outstanding classification accuracy, which proves the superiority of our method over the state-of-the-arts in the SAR-to-optical image translation task.}
}
@article{AN2022108195,
title = {Indefinite twin support vector machine with DC functions programming},
journal = {Pattern Recognition},
volume = {121},
pages = {108195},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108195},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003770},
author = {Yuexuan An and Hui Xue},
keywords = {SVM, TWSVM, Indefinite kernel, DC Programming, Structural risk minimization principle},
abstract = {Twin support vector machine (TWSVM) is an efficient algorithm for binary classification. However, the lack of the structural risk minimization principle restrains the generalization of TWSVM and the guarantee of convex optimization constraints TWSVM to only use positive semi-definite kernels (PSD). In this paper, we propose a novel TWSVM for indefinite kernel called indefinite twin support vector machine with difference of convex functions programming (ITWSVM-DC). The indefinite TWSVM (ITWSVM) leverages a maximum margin regularization term to improve the generalization of TWSVM and a smooth quadratic hinge loss function to make the model continuously differentiable. The representer theorem is applied to the ITWSVM and the convexity of the ITWSVM is analyzed. In order to address the non-convex optimization problem when the kernel is indefinite, a difference of convex functions (DC) is used to decompose the non-convex objective function into the subtraction of two convex functions and a line search method is applied in the DC algorithm to accelerate the convergence rate. A theoretical analysis illustrates that ITWSVM-DC can converge to a local optimum and extensive experiments on indefinite and positive semi-definite kernels show the superiority of ITWSVM-DC.}
}
@article{FAN2022108225,
title = {Blitz-SLAM: A semantic SLAM in dynamic environments},
journal = {Pattern Recognition},
volume = {121},
pages = {108225},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108225},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004064},
author = {Yingchun Fan and Qichi Zhang and Yuliang Tang and Shaofen Liu and Hong Han},
keywords = {Semantic SLAM, Dynamic environments, Noise block, Local point cloud, Global point cloud map},
abstract = {Static environment is a prerequisite for most of visual simultaneous localization and mapping systems. Such a strong assumption limits the practical application of most existing SLAM systems. When moving objects enter the camera’s view field, dynamic matching points will directly interrupt the camera localization, and the noise blocks formed by moving objects will contaminate the constructed map. In this paper, a semantic SLAM system working in indoor dynamic environments named Blitz-SLAM is proposed. The noise blocks in the local point cloud are removed by combining the advantages of semantic and geometric information of mask, RGB and depth images. The global point cloud map can be obtained by merging the local point clouds. We evaluate Blitz-SLAM on the TUM RGB-D dataset and in the real-world environment. The experimental results demonstrate that Blitz-SLAM can work robustly in dynamic environments and generate a clean and accurate global point cloud map simultaneously.}
}