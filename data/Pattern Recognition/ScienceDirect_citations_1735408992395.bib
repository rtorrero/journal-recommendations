@article{TOLOSANA2022108609,
title = {SVC-onGoing: Signature verification competition},
journal = {Pattern Recognition},
volume = {127},
pages = {108609},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108609},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000905},
author = {Ruben Tolosana and Ruben Vera-Rodriguez and Carlos Gonzalez-Garcia and Julian Fierrez and Aythami Morales and Javier Ortega-Garcia and Juan {Carlos Ruiz-Garcia} and Sergio Romero-Tapiador and Santiago Rengifo and Miguel Caruana and Jiajia Jiang and Songxuan Lai and Lianwen Jin and Yecheng Zhu and Javier Galbally and Moises Diaz and Miguel {Angel Ferrer} and Marta Gomez-Barrero and Ilya Hodashinsky and Konstantin Sarin and Artem Slezkin and Marina Bardamova and Mikhail Svetlakov and Mohammad Saleem and Cintia {Lia Szcs} and Bence Kovari and Falk Pulsmeyer and Mohamad Wehbi and Dario Zanca and Sumaiya Ahmad and Sarthak Mishra and Suraiya Jabin},
keywords = {SVC-onGoing, SVC 2021, Biometrics, Handwriting, Signature verification, DeepSignDB, SVC2021_EvalDB},
abstract = {This article presents SVC-onGoing11https://competitions.codalab.org/competitions/27295., an on-going competition for on-line signature verification where researchers can easily benchmark their systems against the state of the art in an open common platform using large-scale public databases, such as DeepSignDB22https://github.com/BiDAlab/DeepSignDB. and SVC2021_EvalDB33https://github.com/BiDAlab/SVC2021_EvalDB., and standard experimental protocols. SVC-onGoing is based on the ICDAR 2021 Competition on On-Line Signature Verification (SVC 2021), which has been extended to allow participants anytime. The goal of SVC-onGoing is to evaluate the limits of on-line signature verification systems on popular scenarios (office/mobile) and writing inputs (stylus/finger) through large-scale public databases. Three different tasks are considered in the competition, simulating realistic scenarios as both random and skilled forgeries are simultaneously considered on each task. The results obtained in SVC-onGoing prove the high potential of deep learning methods in comparison with traditional methods. In particular, the best signature verification system has obtained Equal Error Rate (EER) values of 3.33% (Task 1), 7.41% (Task 2), and 6.04% (Task 3). Future studies in the field should be oriented to improve the performance of signature verification systems on the challenging mobile scenarios of SVC-onGoing in which several mobile devices and the finger are used during the signature acquisition.}
}
@article{NGUYEN2022108667,
title = {Reflection symmetry detection of shapes based on shape signatures},
journal = {Pattern Recognition},
volume = {128},
pages = {108667},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108667},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001480},
author = {Thanh Phuong Nguyen and Hung Phuoc Truong and Thanh Tuan Nguyen and Yong-Guk Kim},
keywords = {Symmetry detection, Reflection symmetry, LIP-signature, -signature, Radon},
abstract = {We present two novel shape signature-based reflection symmetry detection methods with their theoretical underpinning and empirical evaluation. LIP-signature and R-signature share similar beneficial properties allowing to detect reflection symmetry directions in a high-performing manner. For the shape signature of a given shape, its merit profile is constructed to detect candidates of symmetry direction. A verification process is utilized to eliminate the false candidates by addressing Radon projections. The proposed methods can effectively deal with compound shapes which are challenging for traditional contour-based methods. To quantify the symmetric efficiency, a new symmetry measure is proposed over the range [0, 1]. Furthermore, we introduce two symmetry shape datasets with a new evaluation protocol and a lost measure for evaluating symmetry detectors. Experimental results using standard and new datasets suggest that the proposed methods prominently perform compared to state of the art.}
}
@article{HU2022108623,
title = {A novel hybrid model for short-term prediction of wind speed},
journal = {Pattern Recognition},
volume = {127},
pages = {108623},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108623},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001042},
author = {Haize Hu and Yunyi Li and Xiangping Zhang and Mengge Fang},
keywords = {Short-term, Wind speed, Hybrid model, GWO, SVM},
abstract = {Due to the randomness and contingency of wind speed size and direction, it is difficult to predict the wind speed accurately, which seriously affects the stable operation of the power system. To improve the operation stability of power system, the accurate prediction of wind speed is very important. In this paper, a new hybrid model based on gray wolf algorithm (GWO) and support vector machine (SVM) for wind speed prediction is proposed. Firstly, Neo4j(NE) is utilized to identify the data and preprocess the data. Secondly, k-means clustering(KC) is utilized to analyze data and eliminate invalid data. Thirdly, GWO is utilized to optimize the kernel function parameters and penalty factors of SVM to improve the prediction results. Fourthly, The four modules are combined into NE-KC-GWO-SVM model to predict the wind speed accurately. Finally, to verify the effectiveness of the proposed model, the prediction accuracy of the model is experimentally analyzed from two parts. One is to analyze the superiority of the model itself by using the method of single model removed. The results show that the proposed model is the best, and has high accuracy, and can reflect the characteristics of wind speed well and truly. The other one is that models similar to those proposed in the literature are selected for comparative analysis. The experimental results show that compared with the other two models, the proposed model has the best accuracy. At the same time, the proposed model has good prediction stability and acceptable time complexity. Based on all the experimental results, it can be obtained that the proposed model has better prediction effect, which can provide a scientific basis for the macro-control of power system and improve the operation security and stability of power system.}
}
@article{JIN2022108655,
title = {Sparse matrix factorization with L2,1 norm for matrix completion},
journal = {Pattern Recognition},
volume = {127},
pages = {108655},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108655},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001364},
author = {Xiaobo Jin and Jianyu Miao and Qiufeng Wang and Guanggang Geng and Kaizhu Huang},
keywords = {Matrix Completion, Matrix Factorization,  Norm Regularization, Alternative Optimization, Sparse Property},
abstract = {Matrix factorization is a popular matrix completion method, however, it is difficult to determine the ranks of the factor matrices. We propose two new sparse matrix factorization methods with l2,1 norm to explicitly force the row sparseness of the factor matrices, where the rank of the factor matrices is adaptively controlled by the regularization coefficient. We further theoretically prove the convergence property of our algorithms. The experimental results on the simulation and the benchmark datasets show that our methods achieve superior performance than its counterparts. Moreover our proposed methods can attain comparable performance with the deep learning-based matrix completion methods.}
}
@article{CHENG2022108658,
title = {A spatially constrained skew Student’s-t mixture model for brain MR image segmentation and bias field correction},
journal = {Pattern Recognition},
volume = {128},
pages = {108658},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108658},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200139X},
author = {Ning Cheng and Chunzheng Cao and Jianwei Yang and Zhichao Zhang and Yunjie Chen},
keywords = {Bias field, EM Algorithm, Skew student’s-t distribution, Two-level spatial information},
abstract = {Accurate segmentation of brain magnetic resonance images is a key step in quantitative analysis of brain images. Finite mixture model is one of the most widely used methods in brain magnetic resonance image segmentation. However, due to the presence of intensity inhomogeneity artifact and noise, the image histogram distribution of brain MR images may follow a heavy tailed distribution or asymmetric distribution, which makes traditional finite mixture model, such as Gaussian mixture model, hard to achieve accurate segmentation results. To alleviate these problems, a novel spatially constrained finite skew student’s-t mixture model is proposed in this paper. Firstly, we propose anisotropic two-level spatial information, which combines the prior and posterior probabilities, to reduce the impact of noise. The proposed spatial information can preserve rich details, such as edges and corners. Secondly, we couple the anisotropic spatial information into the skew student’s-t distribution to fit the intensity distribution of observation data with heavy tail distribution or asymmetric distribution. Thirdly, we use a linear combination of a set of orthogonal basis functions to model the intensity inhomogeneities. Finally, the objective function integrates both tissue segmentation and the bias field estimation. In the implementation, we used an improved expectation maximization (EM) algorithm to estimate the model parameters. The experimental results of our model on synthetic data and brain magnetic resonance images are better than other state-of-the-art segmentation methods.}
}
@article{LEE2022108639,
title = {Convergence analysis of connection center evolution and faster clustering},
journal = {Pattern Recognition},
volume = {127},
pages = {108639},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108639},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001200},
author = {Jaemin Lee and Minseok Han and Jong-Seok Lee},
keywords = {Clustering, Center evolution, Convergence analysis, Ergodic Markov chain, Faster algorithm},
abstract = {Clustering is a subjective task, that is, several different results can be obtained from a single clustering hierarchy, depending on the observation scale. A local view of the data may necessitate more clusters, whereas a global view requires fewer clusters. It is, therefore, important to provide users with an appropriate clustering hierarchy and let them select the final clustering result based on their own observation scale. Thus, a new clustering method, named connection center evolution (CCE), was recently developed by Geng and Tang (2020). CCE provides gradual clustering results by iteratively merging cluster centers. However, theoretical evidence for its convergence is missing, and the center evolution requires a connectivity matrix of a significantly higher order as iterations proceed, resulting in higher computational costs. Accordingly, we present a convergence analysis of CCE using the properties of ergodic Markov chains and propose a faster algorithm using the enhanced connectivity graph derived from the convergence analysis. Empirical evidence from numerical experiments and theoretical proofs demonstrate the advantages of the proposed method.}
}
@article{ZHAO2022108618,
title = {A feature consistency driven attention erasing network for fine-grained image retrieval},
journal = {Pattern Recognition},
volume = {128},
pages = {108618},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108618},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000991},
author = {Qi Zhao and Xu Wang and Shuchang Lyu and Binghao Liu and Yifan Yang},
keywords = {Fine-grained image retrieval, Deep hashing learning, Selective region erasing module, Feature consistency},
abstract = {Large-scale fine-grained image retrieval based hashing learning method has two main problems. First, low dimension feature embedding can fasten the retrieval process but bring accuracy decrease due to much information loss. Second, fine-grained images lead to the same category query hash codes mapping into the different cluster in database hash latent space. To handle these issues, we propose a feature consistency driven attention erasing network (FCAENet) for fine-grained image retrieval. For the first issue, we propose an adaptive augmentation module in FCAENet, which is the selective region erasing module (SREM). SREM makes the network more robust on subtle differences of fine-grained task by adaptively covering some regions of raw images. The feature extractor and hash layer can learn more representative hash codes for fine-grained images by SREM. With regard to the second issue, we fully exploit the pair-wise similarity information and add the enhancing space relation loss (ESRL) in FCAENet to make the vulnerable relation stabler between the query hash code and database hash code. We conduct extensive experiments on five fine-grained benchmark datasets (CUB2011, Aircraft, NABirds, VegFru, Food101) for 12bits, 24bits, 32bits, 48bits hash codes. The results show that FCAENet achieves the state-of-the-art (SOTA) fine-grained image retrieval performance based on the hashing learning method.}
}
@article{ZHANG2022108663,
title = {End-to-end weakly supervised semantic segmentation with reliable region mining},
journal = {Pattern Recognition},
volume = {128},
pages = {108663},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108663},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001443},
author = {Bingfeng Zhang and Jimin Xiao and Yunchao Wei and Kaizhu Huang and Shan Luo and Yao Zhao},
keywords = {Weakly supervised, Semantic segmentation, End-to-end, Attention},
abstract = {Weakly supervised semantic segmentation is a challenging task that only takes image-level labels as supervision but produces pixel-level predictions for testing. To address such a challenging task, most current approaches generate pseudo pixel masks first that are then fed into a separate semantic segmentation network. However, these two-step approaches suffer from high complexity and being hard to train as a whole. In this work, we harness the image-level labels to produce reliable pixel-level annotations and design a fully end-to-end network to learn to predict segmentation maps. Concretely, we firstly leverage an image classification branch to generate class activation maps for the annotated categories, which are further pruned into tiny reliable object/background regions. Such reliable regions are then directly served as ground-truth labels for the segmentation branch, where both global information and local information sub-branches are used to generate accurate pixel-level predictions. Furthermore, a new joint loss is proposed that considers both shallow and high-level features. Despite its apparent simplicity, our end-to-end solution achieves competitive mIoU scores (val: 65.4%, test: 65.3%) on Pascal VOC compared with the two-step counterparts. By extending our one-step method to two-step, we get a new state-of-the-art performance on the Pascal VOC 2012 dataset(val: 69.3%, test: 69.2%). Code is available at: https://github.com/zbf1991/RRM.}
}
@article{STRAZZERI2022108687,
title = {Possibility results for graph clustering: A novel consistency axiom},
journal = {Pattern Recognition},
volume = {128},
pages = {108687},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108687},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001686},
author = {Fabio Strazzeri and Rubén J. Sánchez-García},
keywords = {Data clustering, Graph clustering, Axiomatic clustering, Morse theory, Morse flow},
abstract = {Kleinberg introduced three natural clustering properties, or axioms, and showed they cannot be simultaneously satisfied by any clustering algorithm. We present a new clustering property, Monotonic Consistency, which avoids the well-known problematic behaviour of Kleinberg’s Consistency axiom, and the impossibility result. Namely, we describe a clustering algorithm, Morse Clustering, inspired by Morse Theory in Differential Topology, which satisfies Kleinberg’s original axioms with Consistency replaced by Monotonic Consistency. Morse clustering uncovers the underlying flow structure on a set or graph and returns a partition into trees representing basins of attraction of critical vertices. We also generalise Kleinberg’s axiomatic approach to sparse graphs, showing an impossibility result for Consistency, and a possibility result for Monotonic Consistency and Morse clustering.}
}
@article{ABANDA2022108671,
title = {Time series classifier recommendation by a meta-learning approach},
journal = {Pattern Recognition},
volume = {128},
pages = {108671},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108671},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001522},
author = {A. Abanda and U. Mori and Jose A. Lozano},
keywords = {Time series classification, Meta-learning, Landmarkers, Hierarchical inference, Meta-targets},
abstract = {This work addresses time series classifier recommendation for the first time in the literature by considering several recommendation forms or meta-targets: classifier accuracies, complete ranking, top-M ranking, best set and best classifier. For this, an ad-hoc set of quick estimators of the accuracies of the candidate classifiers (landmarkers) are designed, which are used as predictors for the recommendation system. The performance of our recommender is compared with the performance of a standard method for non-sequential data and a set of baseline methods, which our method outperforms in 7 of the 9 considered scenarios. Since some meta-targets can be inferred from the predictions of other more fine-grained meta-targets, the last part of the work addresses the hierarchical inference of meta-targets. The experimentation suggests that, in many cases, a single model is sufficient to output many types of meta-targets with competitive results.}
}
@article{SUAREZ2022108619,
title = {ELSED: Enhanced line SEgment drawing},
journal = {Pattern Recognition},
volume = {127},
pages = {108619},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108619},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001005},
author = {Iago Suárez and José M. Buenaposada and Luis Baumela},
keywords = {Image edge detection, Efficient line segment detection, Line segment detection evaluation},
abstract = {Detecting local features, such as corners, segments or blobs, is the first step in the pipeline of many Computer Vision applications. Its speed is crucial for real-time applications. In this paper we present ELSED, the fastest line segment detector in the literature. The key for its efficiency is a local segment growing algorithm that connects gradient-aligned pixels in presence of small discontinuities. The proposed algorithm not only runs in devices with very low end hardware, but may also be parametrized to foster the detection of short or longer segments, depending on the task at hand. We also introduce new metrics to evaluate the accuracy and repeatability of segment detectors. In our experiments with different public benchmarks we prove that our method accounts the highest repeatability and it is the most efficient in the literature.11Source code: https://github.com/iago-suarez/ELSED In the experiments we quantify the accuracy traded for such gain.}
}
@article{ZHOU2022108695,
title = {Interpolation-based nonrigid deformation estimation under manifold regularization constraint},
journal = {Pattern Recognition},
volume = {128},
pages = {108695},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108695},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001765},
author = {Huabing Zhou and Zhichao Xu and Yulu Tian and Zhenghong Yu and Yanduo Zhang and Jiayi Ma},
keywords = {Nonrigid deformation, Manifold regularization, Gaussian kernel, TPS kernel, Medical image registration},
abstract = {This paper addresses the image/surface deformation problem by estimating interpolation functions pixel by pixel(or voxel by voxel) between control point pairs using labeled control points and unlabeled feature points as input. The labeled control points are usually selected by users and labeled through user operations; the unlabeled feature points are extracted from the source image. We formulate the interpolation function estimation at each pixel as a weighted semi-supervised learning problem. Specially, we employ moving least squares to estimate the nonrigid deformation function according to the weights between each pixel and the labeled control points and exploit manifold regularization to preserve the intrinsic geometric information of the unlabeled feature points contained in the object. Moreover, we define the nonrigid deformation function in a reproducing kernel Hilbert space to derive a closed-form solution. To reduce the computational complexity, we also adopt a sparse approximation to realize a fast implementation. It is worth mentioning that our proposed method is a unified framework with two different basis functions. Both basis-function-based methods are applied to 2D image deformation, 3D surface deformation, and medical image registration. Extensive experiments on the data and the resulting mean opinion score (MOS) on the 2D deformation demonstrate that our methods are superior to state-of-the-art ones.}
}
@article{ACIEN2022108643,
title = {BeCAPTCHA-Mouse: Synthetic mouse trajectories and improved bot detection},
journal = {Pattern Recognition},
volume = {127},
pages = {108643},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108643},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001248},
author = {Alejandro Acien and Aythami Morales and Julian Fierrez and Ruben Vera-Rodriguez},
keywords = {CAPTCHA, Bot detection, Behavior, Biometrics, Mouse, Neuromotor},
abstract = {We first study the suitability of behavioral biometrics to distinguish between computers and humans, commonly named as bot detection. We then present BeCAPTCHA-Mouse, a bot detector based on: i) a neuromotor model of mouse dynamics to obtain a novel feature set for the classification of human and bot samples; and ii) a learning framework involving real and synthetically generated mouse trajectories. We propose two new mouse trajectory synthesis methods for generating realistic data: a) a function-based method based on heuristic functions, and b) a data-driven method based on Generative Adversarial Networks (GANs) in which a Generator synthesizes human-like trajectories from a Gaussian noise input. Experiments are conducted on a new testbed also introduced here and available in GitHub: BeCAPTCHA-Mouse Benchmark; useful for research in bot detection and other mouse-based HCI applications. Our benchmark data consists of 15,000 mouse trajectories including real data from 58 users and bot data with various levels of realism. Our experiments show that BeCAPTCHA-Mouse is able to detect bot trajectories of high realism with 93% of accuracy in average using only one mouse trajectory. When our approach is fused with state-of-the-art mouse dynamic features, the bot detection accuracy increases relatively by more than 36%, proving that mouse-based bot detection is a fast, easy, and reliable tool to complement traditional CAPTCHA systems.}
}
@article{WANG2022108606,
title = {Complex shearlets and rotary phase congruence tensor for corner detection},
journal = {Pattern Recognition},
volume = {128},
pages = {108606},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108606},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000875},
author = {Mingzhe Wang and Changming Sun and Arcot Sowmya},
keywords = {Complex shearlets, Phase congruence, Rotary structure tensor, Multi-scale analysis, Corner detection},
abstract = {Corner detection algorithms based on multi-scale analysis attract more attention due to their promising performance. However, they only consider amplitude information, neglect phase information and partially utilize multi-scale decomposition coefficients to detect corners. This limits their detection accuracy, repeatability and localization ability. This paper describes a new multi-scale analysis based corner detector. To overcome the problems of bilateral margin responses, edge extension and lack of phase information in traditional shearlets, a novel complex shearlet transform is proposed to better localize distributed discontinuities and especially to extract phase information from geometrical features. Moreover, a new rotary phase congruence tensor is proposed to utilize all amplitude and phase information for corner detection. Its tolerances to noise and ability for corner localization are improved further by screening and normalizing the amplitude information. Experimental results demonstrate that the localization ability and detection accuracy of the proposed method are superior to current detectors, and its repeatability is generally higher than current detectors and recent machine learning based interest point detectors.}
}
@article{YUAN2022108651,
title = {Exploring interactive attribute reduction via fuzzy complementary entropy for unlabeled mixed data},
journal = {Pattern Recognition},
volume = {127},
pages = {108651},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108651},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001327},
author = {Zhong Yuan and Hongmei Chen and Tianrui Li},
keywords = {Fuzzy rough set theory, Unsupervised attribute reduction, Complementary entropy, Maximal information, Minimal redundancy, Maximal interactivity, Mixed data},
abstract = {Attribute reduction is one of the important applications in fuzzy rough set theory. However, most attribute reduction methods in fuzzy rough theory mainly focus on removing irrelevant or redundant attributes. There are few reports about the method of considering attribute interaction. For this reason, this paper proposes an interactive attribute reduction method for unlabeled mixed data. First, some uncertainty measures based on fuzzy complementary entropy are further defined. Then, based on the proposed uncertainty measure, the attribute evaluation criteria of maximal information, minimal redundancy, and maximal interactivity are developed respectively. As a result, the evaluation index of the attribute importance is established by using the idea of unsupervised maximal information-minimal redundancy-maximal interactivity. Finally, a corresponding algorithm is designed to select attributes. The experimental results show that the proposed algorithm has better performance.}
}
@article{LI2022108614,
title = {Learning residue-aware correlation filters and refining scale for real-time UAV tracking},
journal = {Pattern Recognition},
volume = {127},
pages = {108614},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108614},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000954},
author = {Shuiwang Li and Yuting Liu and Qijun Zhao and Ziliang Feng},
keywords = {Residue-aware correlation filters, Discriminative scale estimation, GrabCut, Unmanned aerial vehicle (UAV) tracking},
abstract = {Unmanned aerial vehicle (UAV)-based tracking finds its applications in agriculture, aviation, navigation, transportation and public security, etc and develops rapidly recently. However, due to limitations of computing resources, battery capacity, requirement of low power and maximum load of UAV, the deployment of deep learning-based tracking algorithms in UAV is currently not feasible and therefore discriminative correlation filters (DCF)-based trackers have stood out in UAV tracking community for their high efficiency and appealing robustness on a single CPU. But confronted with difficult challenges the efficiency and accuracy of existing DCF-based approaches is still not satisfying. Inspired by the good optimization properties associated with residue representation, in this paper we exploit the residue nature inherent to videos and propose residue-aware correlation filters which demonstrate better convergence properties in filter learning. In addition, we propose a scale refinement strategy to improve the wildly adopted discriminative scale estimation in DCF-based trackers, which, in fact, greatly impacts the precision and accuracy of the trackers since accumulated scale error degrades the appearance model as online updating goes on. Extensive experiments are conducted on four UAV benchmarks, namely, UAV123@10fps, DTB70, UAVDT and Vistrone2018 (VisDrone2018-test-dev). The results show that our method achieves state-of-the-art performance in UAV tracking.}
}
@article{YE2022108659,
title = {Molecular substructure graph attention network for molecular property identification in drug discovery},
journal = {Pattern Recognition},
volume = {128},
pages = {108659},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108659},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001406},
author = {Xian-bin Ye and Quanlong Guan and Weiqi Luo and Liangda Fang and Zhao-Rong Lai and Jun Wang},
keywords = {Molecular substructure, Graph attention, Molecular property identification},
abstract = {Molecular machine learning based on graph neural network has a broad prospect in molecular property identification in drug discovery. Molecules contain many types of substructures that may affect their properties. However, conventional methods based on graph neural networks only consider the interaction information between nodes, which may lead to the oversmoothing problem in the multi-hop operations. These methods may not efficiently express the interacting information between molecular substructures. Hence, We develop a Molecular SubStructure Graph ATtention (MSSGAT) network to capture the interacting substructural information, which constructs a composite molecular representation with multi-substructural feature extraction and processes such features effectively with a nested convolution plus readout scheme. We evaluate the performance of our model on 13 benchmark data sets, in which 9 data sets are from the ChEMBL data base and 4 are the SIDER, BBBP, BACE, and HIV data sets. Extensive experimental results show that MSSGAT achieves the best results on most of the data sets compared with other state-of-the-art methods.}
}
@article{ZHANG2022108594,
title = {Unabridged adjacent modulation for clothing parsing},
journal = {Pattern Recognition},
volume = {127},
pages = {108594},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108594},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000759},
author = {Dong Zhang and Chengting Zuo and Qianhao Wu and Liyong Fu and Xinguang Xiang},
keywords = {Encoder-decoder network, Clothing parsing, Attention learning, Features modulation, Self-supervised learning},
abstract = {Clothing parsing has made tremendous progress in the domain of computer vision recently. Most state-of-the-art methods are based on the encoder-decoder architecture. However, the existing methods mainly neglect problems of feature uncalibration within blocks and semantics dilution between blocks. In this work, we propose an unabridged adjacent modulation network (UAM-Net) to aggregate multi-level features for clothing parsing. We first build an unabridged channel attention (UCA) mechanism on feature maps within each block for feature recalibration. We further design a top-down adjacent modulation (TAM) for decoder blocks. By deploying TAM, high-level semantic information and visual contexts can be gradually transferred into lower-level layers without loss. The joint implementation of UCA and TAM ensures that the encoder has an enhanced feature representation ability, and the low-level features of the decoders contain abundant semantic contexts. Quantitative and qualitative experimental results on two challenging benchmarks (i.e., colorful fashion parsing and the modified fashion clothing) declare that our proposed UAM-Net can achieve competitive high-accurate performance with the state-of-the-art methods. The source codes are available at: https://github.com/ctzuo/UAM-Net.}
}
@article{YU2022108691,
title = {SPARE: Self-supervised part erasing for ultra-fine-grained visual categorization},
journal = {Pattern Recognition},
volume = {128},
pages = {108691},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108691},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001728},
author = {Xiaohan Yu and Yang Zhao and Yongsheng Gao},
keywords = {Self-Supervised part erasing, Ultra-fine-grained visual categorization, Fine-grained visual categorization, Random part erasing, Weakly-supervised part segmentation},
abstract = {This paper presents SPARE, a self-supervised part erasing framework for ultra-fine-grained visual categorization. The key insight of our model is to learn discriminative representations by encoding a self-supervised module that performs random part erasing and prediction on the contextual position of the erased parts. This drives the network to exploit intrinsic structure of data, i.e., understanding and recognizing the contextual information of the objects, thus facilitating more discriminative part-level representation. This also enhances the learning capability of the model by introducing more diversified training part segments with semantic meaning. We demonstrate that our approach is able to achieve strong performance on seven publicly available datasets covering ultra-fine-grained visual categorization and fine-grained visual categorization tasks.}
}
@article{BEECHE2022108669,
title = {Super U-Net: A modularized generalizable architecture},
journal = {Pattern Recognition},
volume = {128},
pages = {108669},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108669},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001509},
author = {Cameron Beeche and Jatin P Singh and Joseph K Leader and Naciye S Gezer and Amechi P Oruwari and Kunal K Dansingani and Jay Chhablani and Jiantao Pu},
keywords = {Image segmentation, U-Net, Dynamic receptive field, Fusion upsampling},
abstract = {Objective
To develop and validate a novel convolutional neural network (CNN) termed “Super U-Net” for medical image segmentation.
Methods
Super U-Net integrates a dynamic receptive field module and a fusion upsampling module into the classical U-Net architecture. The model was developed and tested to segment retinal vessels, gastrointestinal (GI) polyps, skin lesions on several image types (i.e., fundus images, endoscopic images, dermoscopic images). We also trained and tested the traditional U-Net architecture, seven U-Net variants, and two non-U-Net segmentation architectures. K-fold cross-validation was used to evaluate performance. The performance metrics included Dice similarity coefficient (DSC), accuracy, positive predictive value (PPV), and sensitivity.
Results
Super U-Net achieved average DSCs of 0.808±0.0210, 0.752±0.019, 0.804±0.239, and 0.877±0.135 for segmenting retinal vessels, pediatric retinal vessels, GI polyps, and skin lesions, respectively. The Super U-net consistently outperformed U-Net, seven U-Net variants, and two non-U-Net segmentation architectures (p < 0.05).
Conclusion
Dynamic receptive fields and fusion upsampling can significantly improve image segmentation performance.}
}
@article{TANG2022108638,
title = {Unsupervised domain adaptation via distilled discriminative clustering},
journal = {Pattern Recognition},
volume = {127},
pages = {108638},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108638},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001194},
author = {Hui Tang and Yaowei Wang and Kui Jia},
keywords = {Deep learning, Unsupervised domain adaptation, Image classification, Knowledge distillation, Deep discriminative clustering, Implicit domain alignment},
abstract = {Unsupervised domain adaptation addresses the problem of classifying data in an unlabeled target domain, given labeled source domain data that share a common label space but follow a different distribution. Most of the recent methods take the approach of explicitly aligning feature distributions between the two domains. Differently, motivated by the fundamental assumption for domain adaptability, we re-cast the domain adaptation problem as discriminative clustering of target data, given strong privileged information provided by the closely related, labeled source data. Technically, we use clustering objectives based on a robust variant of entropy minimization that adaptively filters target data, a soft Fisher-like criterion, and additionally the cluster ordering via centroid classification. To distill discriminative source information for target clustering, we propose to jointly train the network using parallel, supervised learning objectives over labeled source data. We term our method of distilled discriminative clustering for domain adaptation as DisClusterDA. We also give geometric intuition that illustrates how constituent objectives of DisClusterDA help learn class-wisely pure, compact feature distributions. We conduct careful ablation studies and extensive experiments on five popular benchmark datasets, including a multi-source domain adaptation one. Based on commonly used backbone networks, DisClusterDA outperforms existing methods on these benchmarks. It is also interesting to observe that in our DisClusterDA framework, adding an additional loss term that explicitly learns to align class-level feature distributions across domains does harm to the adaptation performance, though more careful studies in different algorithmic frameworks are to be conducted.}
}
@article{HUANG2022108622,
title = {Unsupervised feature selection via adaptive graph and dependency score},
journal = {Pattern Recognition},
volume = {127},
pages = {108622},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108622},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001030},
author = {Pei Huang and Xiaowei Yang},
keywords = {Unsupervised feature selection, Adaptive graph, Mutual information, Entropy},
abstract = {Unsupervised feature selection is an important topic in the fields of machine learning, pattern recognition and data mining. The representation methods include adaptive-graph-based methods and self-representation-based methods. The former methods have a longstanding and undiscovered problem about imbalanced neighbors, and the latter ones do not perform well when features are not linearly dependent. To deal with these problems, a novel unsupervised feature selection method is proposed to ensure k connectivity and eliminate more redundant features based on adaptive graph and dependency score (AGDS). Extensive experiments conducted on 13 benchmark datasets show the effectiveness of AGDS.}
}
@article{QUACH2022108646,
title = {Non-volume preserving-based fusion to group-level emotion recognition on crowd videos},
journal = {Pattern Recognition},
volume = {128},
pages = {108646},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108646},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001273},
author = {Kha Gia Quach and Ngan Le and Chi Nhan Duong and Ibsa Jalata and Kaushik Roy and Khoa Luu},
keywords = {Group-level emotion recognition, Facial features, Feature extraction, Feature fusion, Crowd videos},
abstract = {Group-level emotion recognition (ER) is a growing research area as the demands for assessing crowds of all sizes are becoming an interest in both the security arena as well as social media. This work extends the earlier ER investigations, which focused on either group-level ER on single images or within a video, by fully investigating group-level expression recognition on crowd videos. In this paper, we propose an effective deep feature level fusion mechanism to model the spatial-temporal information in the crowd videos. In our approach, the fusing process is performed on the deep feature domain by a generative probabilistic model, Non-Volume Preserving Fusion (NVPF), that models spatial information relationships. Furthermore, we extend our proposed spatial NVPF approach to the spatial-temporal NVPF approach to learn the temporal information between frames. To demonstrate the robustness and effectiveness of each component in the proposed approach, three experiments were conducted: (i) evaluation on AffectNet database to benchmark the proposed EmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii) examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos (GECV) dataset composed of 627 videos collected from publicly available sources. GECV dataset is a collection of videos containing crowds of people. Each video is labeled with emotion categories at three levels: individual faces, group of people, and the entire video frame.}
}
@article{LIU2022108654,
title = {Making person search enjoy the merits of person re-identification},
journal = {Pattern Recognition},
volume = {127},
pages = {108654},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108654},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001352},
author = {Chuang Liu and Hua Yang and Qin Zhou and Shibao Zheng},
keywords = {Person search, Person re-identification, Knowledge transfer, Teacher-guided disentangling network, Context ranking},
abstract = {Person search is an extended task of person re-identification (Re-ID). However, most existing one-step person search works do not study how to employ existing Re-ID models to improve the one-step person search. To address this issue, we propose a Teacher-guided Disentangling Network (TDN) to make the one-step person search enjoy the merits of existing Re-ID research. The proposed TDN can significantly boost person search performance by transferring the advanced person Re-ID knowledge to the person search model. In the proposed TDN, for better knowledge transfer from the Re-ID teacher model to the one-step person search model, we design a new one-step person search base framework by partially disentangling the two subtasks. Besides, we propose a Knowledge Transfer Bridge module to bridge the scale gap caused by different input formats between the Re-ID model and the one-step person search model. Moreover, we also propose a Ranking with Context Persons strategy to exploit the context information in panoramic images for better ranking. Experiments on two public person search datasets demonstrate the favorable performance of the proposed method.}
}
@article{ZHAO2022108626,
title = {Rotation invariant point cloud analysis: Where local geometry meets global topology},
journal = {Pattern Recognition},
volume = {127},
pages = {108626},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108626},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001078},
author = {Chen Zhao and Jiaqi Yang and Xin Xiong and Angfan Zhu and Zhiguo Cao and Xin Li},
keywords = {Point cloud analysis, Rotation invariance, Deep learning, Classification, Segmentation},
abstract = {Point cloud analysis is a fundamental task in 3D computer vision. Most previous works have conducted experiments on synthetic datasets with well-aligned data; while real-world point clouds are often not pre-aligned. How to achieve rotation invariance remains an open problem in point cloud analysis. To meet this challenge, we propose an approach toward achieving rotation-invariant (RI) representations by combining local geometry with global topology. In our local-global-representation (LGR)-Net, we have designed a two-branch network where one stream encodes local geometric RI features and the other encodes global topology-preserving RI features. Motivated by the observation that local geometry and global topology have different yet complementary RI responses in varying regions, two-branch RI features are fused by an innovative multi-layer perceptron (MLP) based attention module. To the best of our knowledge, this work is the first principled approach toward adaptively combining global and local information under the context of RI point cloud analysis. Extensive experiments have demonstrated that our LGR-Net achieves the state-of-the-art performance on various rotation-augmented versions of ModelNet40, ShapeNet, ScanObjectNN, and S3DIS.}
}
@article{LIANG2022108662,
title = {Learning multi-level weight-centric features for few-shot learning},
journal = {Pattern Recognition},
volume = {128},
pages = {108662},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108662},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001431},
author = {Mingjiang Liang and Shaoli Huang and Shirui Pan and Mingming Gong and Wei Liu},
keywords = {Fewshot learning, Low-shot learning, Multi-level features, Image classification},
abstract = {Few-shot learning is currently enjoying a considerable resurgence of interest, aided by the recent advance of deep learning. Contemporary approaches based on weight-generation scheme delivers a straightforward and flexible solution to the problem. However, they did not fully consider both the representation power for unseen categories and weight generation capacity in feature learning, making it a significant performance bottleneck. This paper proposes a multi-level weight-centric feature learning to give full play to feature extractor’s dual roles in few-shot learning. Our proposed method consists of two essential techniques: a weight-centric training strategy to improve the features’ prototype-ability and a multi-level feature incorporating a mid- and relation-level information. The former increases the feasibility of constructing a discriminative decision boundary based on a few samples. Simultaneously, the latter helps improve the transferability for characterizing novel classes and preserve classification capability for base classes. We extensively evaluate our approach to low-shot classification benchmarks. Experiments demonstrate our proposed method significantly outperforms its counterparts in both standard and generalized settings and using different network backbones.}
}
@article{WU2022108686,
title = {Total Bregman divergence-driven possibilistic fuzzy clustering with kernel metric and local information for grayscale image segmentation},
journal = {Pattern Recognition},
volume = {128},
pages = {108686},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108686},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001674},
author = {Chengmao Wu and Xue Zhang},
keywords = {Image segmentation, Fuzzy clustering, Total Bregman divergence, Polynomial kernel function, Possibilistic typicality},
abstract = {Kernel possibilistic fuzzy C-means with local information (KWPFLICM) has important research significance of image segmentation, but it is very sensitive to high noise or outliers. To enhance the segmentation performance of the algorithm, this paper proposes a kernelized total Bregman divergence-driven possibilistic fuzzy clustering with local information (TKWPFLICM). Firstly, a polynomial kernel function is introduced to kernelize total Bregman divergence (TBD), and local neighborhood information of the pixel is used to modify it, which overcomes the shortcomings of Bregman divergence (BD) with rotation variability; Secondly, the modified kernelized TBD and possibilistic typicality are combined to further enhance the anti-noise ability of the algorithm; Finally, the modified kernelized TBD is introduced into the objective function of KWPFLICM algorithm, then a novel robust fuzzy clustering algorithm is derived by optimization theory. Experimental results show that compared with existing fuzzy clustering-related algorithms, the average SA improvement on TKWPFLICM algorithm is in the range of 0.791% to 33.237%. Therefore, TKWPFLICM algorithm has better anti-noise robustness and segmentation accuracy.}
}
@article{WANG2022108670,
title = {Preserving similarity order for unsupervised clustering},
journal = {Pattern Recognition},
volume = {128},
pages = {108670},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108670},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001510},
author = {Jinghua Wang and Li Wang and Jianmin Jiang},
keywords = {Image clustering, Order preserving, Deep representation learning, Score function learning},
abstract = {Unsupervised clustering categorizes a sample set into several groups, where the samples in the same group share high-level concepts. As the clustering performances are heavily determined by the metric to assess the similarity between sample pairs, we propose to learn a deep similarity score function and use it to capture the correlations between sample pairs for improved clustering. We formulate the learning procedure in a ranking framework and introduce two new supervisory signals to train our model. Specifically, we train the similarity score function to guarantee 1) a sample should have a higher level of similarity with its nearest neighbors than others in order to achieve correct clustering, and 2) the ordering of the similarity between neighboring sample pairs should be preserved in order to achieve robust clustering. To this end, we not only study the relevance between neighboring sample pairs for local structure learning, but also study the relevance between each sample and the boundary samples for global structure learning. Extensive experiments on seven public available datasets validate the effectiveness of our proposed framework, including face image clustering, object image clustering, and real-world image clustering.}
}
@article{BAI2022108694,
title = {A categorical data clustering framework on graph representation},
journal = {Pattern Recognition},
volume = {128},
pages = {108694},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108694},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001753},
author = {Liang Bai and Jiye Liang},
keywords = {Cluster analysis, Categorical data clustering, Data representation, Graph embedding},
abstract = {Clustering categorical data is an important task of machine learning, since the type of data widely exists in real world. However, the lack of an inherent order on the domains of categorical features prevents most of classical clustering algorithms from being directly applied for the type of data. Therefore, it is very key issue to learn an appropriate representation of categorical data for the clustering task. In order to address this issue, we develop a categorical data clustering framework based on graph representation. In this framework, a graph-based representation method for categorical data is proposed, which learns the representation of categorical values from their similar graph to provide similar representations for similar categorical values. We compared the proposed framework with other representation methods for categorical data clustering on benchmark data sets. The experiment results illustrate the proposed framework is very effective, compared to other methods.}
}
@article{DUFRENOIS2022108642,
title = {Incremental and compressible kernel null discriminant analysis},
journal = {Pattern Recognition},
volume = {127},
pages = {108642},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108642},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001236},
author = {F. Dufrenois},
keywords = {Incremental kernel discriminant analysis, Null space, Compression mechanism, Multi-class learning, One-class learning, Novelty detection},
abstract = {Kernel discriminant analysis (KDA), the nonlinear extension of linear Discriminant Analysis (LDA), is a popular tool for learning one or multiple categories in nonlinear data sets. However, in most modern pattern recognition applications such as video surveillance, data are collected in flow and require sequential processing. In this context, KDA is faced two critical issues: an original formulation unsuited to the dynamic nature of the data and an increasing memory requirement for the kernel matrix storage. Motivated by the state-of-the-art performance reported by the null KDA, we propose in this paper a new solution to solve the null KDA (NKDA) in the context of data streams. Compared to previous works, our contribution is based on three points: first, we develop an exact incremental scheme which guarantees accurate solutions. Secondly, we develop a compression mechanism based on the following observation: rger the size of the training data set more the distances in the null space contract This property of the null space leads to formulate an indicator of redundancy in the training data set. This criterion is the cornerstone of our incremental KNDA because it authorizes incremental learning on large-scale data sets. Third, the problem of novelty detection in multi-class and one-class scenarios is addressed. More precisely, the fact that distances in the null space change over the training period leads us to define adjustable novelty thresholds. Lastly, numerous experiments based on various publicly available data sets and state-of-the-art classifiers show that the proposed method is effective both for multi-class and one-class real applications.}
}
@article{FENG2022108666,
title = {Encoder deep interleaved network with multi-scale aggregation for RGB-D salient object detection},
journal = {Pattern Recognition},
volume = {128},
pages = {108666},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108666},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001479},
author = {Guang Feng and Jinyu Meng and Lihe Zhang and Huchuan Lu},
keywords = {RGB-D salient object detection, Deep interleaved encoder, Cross-modal mutual guidance, Residual multi-scale feature aggregation, Real-time},
abstract = {Recently, RGB-D salient object detection (SOD) has aroused widespread research interest. Existing RGB-D SOD approaches mainly consider the cross-modal information fusion in the decoder. And their multi-modal interaction mainly concentrates on the same level of features between RGB stream and depth stream. They do not deeply explore the coherence of multi-model features at different levels. In this paper, we design a two-stream deep interleaved encoder network to extract RGB and depth information and realize their mixing simultaneously. This network allows us to gradually learn multi-modal representation at different levels from shallow to deep. Moreover, to further fuse multi-modal features in the decoding stage, we propose a cross-modal mutual guidance module and a residual multi-scale aggregation module to implement the global guidance and local refinement of the salient region. Extensive experiments on six benchmark datasets demonstrate that the proposed approach performs favorably against most state-of-the-art methods under different evaluation metrics. During the testing stage, this model can run at a real-time speed of 93 FPS.}
}
@article{WANG2022108605,
title = {High quality proposal feature generation for crowded pedestrian detection},
journal = {Pattern Recognition},
volume = {128},
pages = {108605},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108605},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000863},
author = {Jing Wang and Cailing Zhao and Zhanqiang Huo and Yingxu Qiao and Haifeng Sima},
keywords = {Crowded pedestrian, Pedestrian detection, Visible proposal, Feature fusion, Paired prediction},
abstract = {Occlusion is a severe problem for pedestrian detection in crowded scenes. Due to the diversity of pedestrian postures and occlusion forms, leading to false detection and missed detection. In this paper, we propose a high quality proposal feature generation pedestrian detection algorithm to improve detection performance. Firstly, Dual-Region Feature Generation (DRFG) is proposed to generate high quality proposal features. Specifically, visible regions with less occlusion are introduced and low-precision proposals are generated for both the full-body and visible regions respectively. Then, proposals are respectively selected from the two kinds of proposals mentioned above to match in pairs, so as to guarantee a strong correspondence in information between the two proposals. Afterwards, the successfully matched proposal features are fused by Selective Kernel Feature Fusion (SKFF) to generate high quality proposal features. Secondly, Paired Multiple Instance Prediction(PMIP) is performed on the fused features to generate multiple prediction branches, and each prediction branch generates full-body and visible prediction box. Finally, Paired Non-Maximum Suppression(PNMS) is applied to the prediction boxes to reduce the false positives. Experiments have been conducted on CrowdHuman [1] and CityPersons [2] datasets. Comparing with baseline, our methods have achieved 5.9% AP and 1.5% MR−2 improvement on the above two datasets, sufficiently verifying the effectiveness of our methods in crowded pedestrian detection.}
}
@article{SUN2022108650,
title = {Iterative brain tumor retrieval for MR images based on user’s intention model},
journal = {Pattern Recognition},
volume = {127},
pages = {108650},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108650},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001315},
author = {Mengli Sun and Wei Zou and Nan Hu and Jiajun Wang and Zheru Chi},
keywords = {CBIR, Brain tumor images, Eye-tracking, Intention similarity, Iterative retrieval, Relevance feedback},
abstract = {Generally, medical content-based image retrieval (CBIR) systems select low-level visual features as image descriptors. However, these descriptors fail to provide clues for understanding the content of medical images in a similar way as a human expert, which makes the retrieval results inconsistent with the user’s intention. To solve this problem, we propose a closed-loop brain tumor retrieval system for MR images with an eye-tracking based relevance feedback mechanism. In our method, we first model the intention of the user by training a convolutional neural network based on the temporal and spatial features extracted from his/her eye-tracking data collected when inspecting the relevance between different images. Upon using visual features as a bridge, the relevancy degree to the query image of any of the database images is computed with our user’s intention model by transferring to it the eye movement data from the most visually similar image amongst images iteratively accumulated in the canvas. Our proposed retrieval system is implemented in an iterative manner. In each round of iteration, user’s eye movement data when inspecting the system returns are collected and the canvas collection of images is also updated by appending to it the user inspected system returns. With the updated canvas collections, the relevancy degree of database images can be recomputed and the system can begin a new round search of the most relevant images. Extensive experiments have been performed on a publicly available T1-weighted contrast-enhanced magnetic resonance image (CE-MRI) dataset that consists of three types of brain tumors (glioma, meningioma, and pituitary tumor) collected from 233 patients with a total of 3064 images across the axial, coronal, and sagittal views. Experimental results of 22 volunteers (11 males and 11 females, with an average age of 24.4 years) from our medical school show that upon implicit involvement of users in the brain tumor retrieving process, our proposed system significantly outperforms state-of-the-art methods and achieves Prec@10 to 99.94%, mAP to 97.95% after the third round of iteration.}
}
@article{GUPTA2022108674,
title = {HandyPose: Multi-level framework for hand pose estimation},
journal = {Pattern Recognition},
volume = {128},
pages = {108674},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108674},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001558},
author = {Divyansh Gupta and Bruno Artacho and Andreas Savakis},
keywords = {Hand pose estimation, Feature representations, Computer vision},
abstract = {Hand pose estimation is a challenging task due to the large number of degrees of freedom and the frequent occlusions of joints. To address these challenges, we propose HandyPose, a single-pass, end-to-end trainable architecture for 2D hand pose estimation using a single RGB image as input. Adopting an encoder-decoder framework with multi-level features, along with a novel multi-level waterfall atrous spatial pooling module for multi-scale representations, our method achieves high accuracy in hand pose while maintaining manageable size complexity and modularity of the network. HandyPose takes a multi-scale approach to representing context by incorporating spatial information at various levels of the network to mitigate the loss of resolution due to pooling. Our advanced multi-level waterfall module leverages the efficiency of progressive cascade filtering while maintaining larger fields-of-view through the concatenation of multi-level features from different levels of the network in the waterfall module. The decoder incorporates both the waterfall and multi-scale features for the generation of accurate joint heatmaps in a single stage. Our results demonstrate state-of-the-art performance on popular datasets and show that HandyPose is a robust and efficient architecture for 2D hand pose estimation.}
}
@article{LI2022108613,
title = {From general to specific: Online updating for blind super-resolution},
journal = {Pattern Recognition},
volume = {127},
pages = {108613},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108613},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000942},
author = {Shang Li and Guixuan Zhang and Zhengxiong Luo and Jie Liu and Zhi Zeng and Shuwu Zhang},
keywords = {Blind super-resolution, Online updating, Internal learning, External learning},
abstract = {Most deep learning-based super-resolution (SR) methods are not image-specific: 1) They are trained on samples synthesized by predefined degradations (e.g.bicubic downsampling), regardless of the domain gap between training and testing data. 2) During testing, they super-resolve all images by the same set of model weights, ignoring the degradation variety. As a result, most previous methods may suffer a performance drop when the degradations of test images are unknown and various (i.e.the case of blind SR). To address these issues, we propose an online SR (ONSR) method. It does not rely on predefined degradations and allows the model weights to be updated according to the degradation of the test image. Specifically, ONSR consists of two branches, namely internal branch (IB) and external branch (EB). IB could learn the specific degradation of the given test LR image, and EB could learn to super resolve images degraded by the learned degradation. In this way, ONSR could customize a specific model for each test image, and thus get more robust to various degradations. Extensive experiments on both synthesized and real-world images show that ONSR can generate more visually favorable SR results and achieve state-of-the-art performance in blind SR.}
}
@article{ZHANG2022108661,
title = {Node-Feature Convolution for Graph Convolutional Networks},
journal = {Pattern Recognition},
volume = {128},
pages = {108661},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108661},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200142X},
author = {Li Zhang and Heda Song and Nikolaos Aletras and Haiping Lu},
keywords = {Graph, Representation learning, Graph convolutional networks, Convolutional neural networks},
abstract = {Graph convolutional network (GCN) is an effective neural network model for graph representation learning. However, standard GCN suffers from three main limitations: (1) most real-world graphs have no regular connectivity and node degrees can range from one to hundreds or thousands, (2) neighboring nodes are aggregated with fixed weights, and (3) node features within a node feature vector are considered equally important. Several extensions have been proposed to tackle the limitations respectively. This paper focuses on tackling all the proposed limitations. Specifically, we propose a new node-feature convolutional (NFC) layer for GCN. The NFC layer first constructs a feature map using features selected and ordered from a fixed number of neighbors. It then performs a convolution operation on this feature map to learn the node representation. In this way, we can learn the usefulness of both individual nodes and individual features from a fixed-size neighborhood. Experiments on three benchmark datasets show that NFC-GCN consistently outperforms state-of-the-art methods in node classification.}
}
@article{HAO2022108593,
title = {Multilabel learning based adaptive graph convolutional network for human parsing},
journal = {Pattern Recognition},
volume = {127},
pages = {108593},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108593},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000747},
author = {Huaqing Hao and Weibin Liu and Weiwei Xing and Shunli Zhang},
keywords = {Human parsing, Multilabel learning based adaptive graph convolutional network, Adaptive graph},
abstract = {In human parsing, graph convolutional networks (GCNs), which naturally model the skeleton of the human body as a fixed graph, have been witnessed to obtain remarkable performance. However, the existing methods perform the fixed graph modeling over all the training samples. This may not be an optimal graph for the diversity of the samples that contain various shapes of human parts, complex body postures, severe occlusions and dense crowd, etc. Focusing on this, we propose a new Multilabel Learning based Adaptive Graph Convolutional Network (ML-AGCN) for human parsing. The ML-AGCN includes three modules: adaptive graph generation module, semantic parts based attention module and label consistency loss. Concretely, to effectively deal with the different sizes and connectivities of the optimal graph for different samples, we first propose an adaptive graph generation module based on multilabel learning that contains graph node adaptation (GNA) and graph connection adaptation (GCA). Then, for a more comprehensive node embedding, we design a semantic parts based attention module to optimally fuse fixed graph embeddings and adaptive graph embeddings. Besides, to further explicitly constraint the consistency between the predicted multilabel and the predicted human parsing results, we propose a label consistency loss that can simultaneously refine the human parsing results and optimize the accuracy of the adaptive graph. Extensive experiments on four challenging datasets, including PASCAL-Person-Part, ATR, LIP and CIHP, well demonstrate the effectiveness of our model, and it outperforms other state-of-the-art methods in human parsing.}
}
@article{LENG2022108601,
title = {Incorporating global and local social networks for group recommendations},
journal = {Pattern Recognition},
volume = {127},
pages = {108601},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108601},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000826},
author = {Youfang Leng and Li Yu},
keywords = {Group recommendation, Recommendation systems, Graph neural network, Social network analysis, Graph-based method},
abstract = {Due to the social nature of human beings, group activities have become an integral part of daily life. This creates the need for an in-depth study of the group-recommendation task: recommending items to a group of users. Unlike individual decision-making, which relies primarily on personal preferences, group decision-making is a process of negotiation and agreement among group members, in which social characteristics are a critical factor in achieving positive recommendation results. Therefore, in this paper, we propose a new model to solve the group recommendation problem from both global and local social networks. In a global network, a user’s social influence spreads through social connections and affects the preferences of others. In a local network, group members may contribute differently to the final decision, forming a dynamic negotiation and consensus process. We propose to model global and local networks with two components: 1) an attentive graph convolutional network based global network diffusion (GND) module to simulate the spread of social influence and capture the social gate of each user, and 2) a multi-channel attention-based local network fusion (LNF) module to learn the complex decision-making process among group members and integrate them into a final representation of the group. Finally, two separate neural collaborative filtering (NCF) modules are presented to model group-item and user-item interactions, respectively, to enhance each other. Extensive experimental results from two real-world datasets show the effectiveness of our proposed model.}
}
@article{LU2022108611,
title = {Improved deep convolutional embedded clustering with re-selectable sample training},
journal = {Pattern Recognition},
volume = {127},
pages = {108611},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108611},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000929},
author = {Hu Lu and Chao Chen and Hui Wei and Zhongchen Ma and Ke Jiang and Yingquan Wang},
keywords = {Unsupervised clustering, Deep embedded clustering, Autoencoder, Reliable samples},
abstract = {The deep clustering algorithm can learn the latent features of the embedded subspace, and further realize the clustering of samples in the feature space. The existing deep clustering algorithms mostly integrate neural networks and traditional clustering algorithms. However, for sample sets with many noise points, the effect of the clustering remains unsatisfactory. To address this issue, we propose an improved deep convolutional embedded clustering algorithm using reliable samples (IDCEC) in this paper. The algorithm first uses the convolutional autoencoder to extract features and cluster the samples. Then we select reliable samples with pseudo-labels and pass them to the convolutional neural network for training to get a better clustering model. We construct a new loss function for backpropagation training and implement an unsupervised deep clustering method. To verify the performance of the method proposed in this paper, we conducted experimental tests on standard data sets such as MNIST and USPS. Experimental results show that our method has better performance compared to traditional clustering algorithms and the state-of-the-art deep clustering algorithm under four clustering metrics.}
}
@article{CHINBAT2022108637,
title = {GA3N: Generative adversarial AutoAugment network},
journal = {Pattern Recognition},
volume = {127},
pages = {108637},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108637},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001182},
author = {Vanchinbal Chinbat and Seung-Hwan Bae},
keywords = {Data augmentation, AutoAugment, Generative adversarial network, Classification, Deep learning, Adversarial learning},
abstract = {Data augmentation is beneficial for improving robustness of deep meta-learning. However, data augmentation methods for the recent deep meta-learning are still based on photometric or geometric manipulations or combinations of images. This paper proposes a generative adversarial autoaugment network (GA3N) for enlarging the augmentation search space and improving classification accuracy. To achieve, we first extend the search space of image augmentation by using GANs. However, the main challenge is to generate images suitable for the task. For solution, we find the best policy by optimizing a target and GAN losses alternatively. We then use the manipulated and generated samples determined by the policy network as augmented samples for improving the target tasks. To show the effects of our method, we implement classification networks by combining our GA3N and evaluate them on CIFAR-100 and Tiny-ImageNet datasets. As a result, we achieve better accuracy than the recent AutoAugment methods on each dataset.}
}
@article{LEE2022108645,
title = {Human interaction recognition framework based on interacting body part attention},
journal = {Pattern Recognition},
volume = {128},
pages = {108645},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108645},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001261},
author = {Dong-Gyu Lee and Seong-Whan Lee},
keywords = {Human activity recognition, Human-human interaction, Interacting body part attention},
abstract = {Human activity recognition in videos has been widely studied and has recently gained significant advances with deep learning approaches; however, it remains a challenging task. In this paper, we propose a novel framework that simultaneously considers both implicit and explicit representations of human interactions by fusing information of local image where the interaction actively occurred, primitive motion with the posture of individual subject’s body parts, and the co-occurrence of overall appearance change. Human interactions change, depending on how the body parts of each human interact with the other. The proposed method captures the subtle difference between different interactions using interacting body part attention. Semantically important body parts that interact with other objects are given more weight during feature representation. The combined feature of interacting body part attention-based individual representation and the co-occurrence descriptor of the full-body appearance change is fed into long short-term memory to model the temporal dynamics over time in a single framework. The experimental results on five widely used public datasets demonstrate the effectiveness of the proposed method to recognize human interactions from videos.}
}
@article{HUANG2022108653,
title = {Cross-modality person re-identification via multi-task learning},
journal = {Pattern Recognition},
volume = {128},
pages = {108653},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108653},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001340},
author = {Nianchang Huang and Kunlong Liu and Yang Liu and Qiang Zhang and Jungong Han},
keywords = {Cross-modality person re-identification, Person body information, Multi-task learning},
abstract = {Despite its promising preliminary results, existing cross-modality Visible-Infrared Person Re-IDentification (VI-PReID) models incorporating semantic (person) masks simply use these person masks as selection maps to separate person features from background regions. Such models do not dedicate to extracting more modality-invariant person body features in the VI-PReID network itself, thus leading to suboptimal results in VI-PReID. Differently, we aim to better capture person body information in the VI-PReID network itself for VI-PReID by exploiting the inner relations between person mask prediction and VI-PReID. To this end, a novel multi-task learning model is presented in this paper, where person body features obtained by person mask prediction potentially facilitate the extraction of discriminative modality-shared person body information for VI-PReID. On top of that, considering the task difference between person mask prediction and VI-PReID, we propose a novel task translation sub-network to transfer discriminative person body information, extracted by person mask prediction, into VI-PReID. Doing so enables our model to better exploit discriminative and modality-invariant person body information. Thanks to more discriminative modality-shared features, our method outperforms previous state-of-the-arts by a significant margin on several benchmark datasets. Our intriguing findings validate the effectiveness of extracting discriminative person body features for the VI-PReID task.}
}
@article{ZHANG2022108625,
title = {2K-Fold-Net and feature enhanced 4-Fold-Net for medical image segmentation},
journal = {Pattern Recognition},
volume = {127},
pages = {108625},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108625},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001066},
author = {Yunchu Zhang and Jianfei Dong},
keywords = {2K-Fold-Net, EF-Net, U-Net, AFE, Image segmentation},
abstract = {For segmenting medical images, U-Net has become a popular and effective tool. However, it also has some shortcomings in segmenting fuzzy boundaries and eliminating interferences. Improvements of the original U-Net have been proposed by many authors, resulting in many variants such as MultiResUNet, DoubleU-Net and W-Net. Based on the common characteristics of these structures, we propose in this work a generalized structure by multiplying the folds of a fully convolutional network (FCN) for even more times, and thus name it as “2K-Fold-Net”. The more folds in this structure provide more freedoms to create cross links between the neighboring folds. The influence of the fold-pair number K on its performance is also studied. The realizations with K up to 6 are compared to three other variants of cascaded U-Nets using the CVC-ClinicDB dataset. Then the special case “4-Fold-Net” is further empowered with the feature enhancing functionalities recently seen in the attention-aware feature enhancement method. This new net is hence named as “Enhanced-Feature-4-Fold-Net”, abbreviated as “EF3-Net”. Finally, 2K-Fold-Net and EF3-Net have been compared with U-Net, SegNet, DoubleU-Net, MultiResUNet and its variants using four challenging medical image datasets. The results have demonstrated that the proposed nets outperform the other variants of U-Net, even with slightly lower amount of parameters. The code is available on: https://github.com/raik7/EF3-Net.}
}
@article{YU2022108685,
title = {LiDAR-based localization using universal encoding and memory-aware regression},
journal = {Pattern Recognition},
volume = {128},
pages = {108685},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108685},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001662},
author = {Shangshu Yu and Cheng Wang and Chenglu Wen and Ming Cheng and Minghao Liu and Zhihong Zhang and Xin Li},
keywords = {LiDAR localization, Absolute pose regression, Universal encoding, Privacy preserving, Memory-aware regression},
abstract = {Visual localization is critical to many robotics and computer vision applications. Absolute pose regression performs localization by encoding scene features followed by pose regression, which has achieved impressive results in localization. It recovers 6-DoF poses from captured scene data alone. However, current methods suffer from being retrained with specific source data whenever the scene changes, resulting in expensive computational costs, data privacy disclosure, and unreliable localization caused by the inability to memorize all data. In this paper, we propose a novel LiDAR-based absolute pose regression network with universal encoding to avoid redundant retraining and the loss of data privacy. Specifically, we propose using universal feature encoding for different scenes. Only the regressor needs to be retrained to achieve higher efficiency, and the training is performed using the encoded features without source data, which preserves data privacy. Then, we propose a memory regressor for memory-aware regression, where the hidden unit numbers in the regressor determine the memorization capacity. It can be used to derive and improve the upper bound of the capacity to enable more reliable localization. Then, it is possible to modify the regressor structure to adapt different memorization capacity requirements for different scene sizes. Extensive experiments on outdoor and indoor datasets validated the above analyses and demonstrated the effectiveness of the proposed method.}
}
@article{YUAN2022108657,
title = {Adaptive open domain recognition by coarse-to-fine prototype-based network},
journal = {Pattern Recognition},
volume = {128},
pages = {108657},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108657},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001388},
author = {Yuan Yuan and Xinxing He and Zhiyu Jiang},
keywords = {Open domain recognition, Image classification, Adaptive openness, Prototype learning, Unknown class recognition},
abstract = {Open domain recognition has attracted great attention in recent two years, which aims to assign a specific identification for each target sample in the presence of large domain discrepancy both in label space and data distributions. Most existing approaches rely on abundant prior information about the relationship of the label sets between the source and the target domain, which is a great limitation for their applications in practical wild. In this paper, a new Adaptive Open Domain Recognition (AODR) task is introduced, which can generalize to various openness and requires no prior information on the label set. To achieve this adaptive transfer task, a two-stage Progressive Adaptation Network is designed, whose learning process consists of multiple episodes. Each episode is performed to simulate an AODR task. Through training and refining multiple episodes, the basic model has progressively accumulated wealthy experience on predicting unseen categories in the presence of large domain discrepancy, which will well generalize to various openness. More specifically, Fusion Information Guided Feature Prototype Generation module is proposed to synthesize visual feature prototype conditioned on category semantic prototype in training stage. Further, Class-Aware Feature Prototype Alignment module is designed in refining stage to align the global feature prototype for each class between two domains. Experimental results verify that the proposed model not only has superiority on classifying the image instances of known and unknown classes, but also well adapts to various openness.}
}
@article{DAOUI2022108596,
title = {On computational aspects of high-order dual Hahn moments},
journal = {Pattern Recognition},
volume = {127},
pages = {108596},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108596},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000772},
author = {Achraf Daoui and Hicham Karmouni and Mohamed Yamni and Mhamed Sayyouri and Hassan Qjidaa},
keywords = {High-order dual Hahn polynomials, Orthogonal moments, Numerical stability, Signal and image reconstruction, High-order moments},
abstract = {In this paper, we present two new algorithms for the fast and stable computation of high-order discrete orthogonal dual Hahn polynomials (DHPs). These algorithms are essentially based on the proposed computation method of the initial values of DHPs following the order n and the variable s. For both algorithms, a single stable value is computed, fully independent of the gamma function that is the source of the numerical overflow, and then the rest of DHPs values are computed recursively via the proposed recurrence scheme. By analyzing the DHPs matrix, we propose a new method, which allows ensuring the numerical stability of high-order DHPs and dual Hahn moments (DHMs) until the last order. This method is based on the use of appropriate stability conditions. The results of simulations and comparisons carried out show on one hand that the second algorithm with the stability condition allows to compute DHPs up to the order n = 17,603 without propagation of numerical error. On the other hand, the performance of analyzing large-size signals and images by high-order DHMs computed by the proposed method significantly exceeds the existing methods in terms of numerical stability, accuracy of reconstruction and in terms of maximum size of the analyzed signals and images. After the acceptance of this paper, the proposed algorithms for high-order DHPs computation will be made publically available at https://github.com/AchrafDaoui/On-Computational-Aspects-of-High-Order-Dual-Hahn-Moments.}
}
@article{RABIE2022108693,
title = {Expecting individuals’ body reaction to Covid-19 based on statistical Naïve Bayes technique},
journal = {Pattern Recognition},
volume = {128},
pages = {108693},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108693},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001741},
author = {Asmaa H. Rabie and Nehal A. Mansour and Ahmed I. Saleh and Ali E. Takieldeen},
keywords = {Covid-19, Prediction, Naïve Bayes, Prudential Expectation},
abstract = {Covid-19, what a strange, unpredictable mutated virus. It has baffled many scientists, as no firm rule has yet been reached to predict the effect that the virus can inflict on people if they are infected with it. Recently, many researches have been introduced for diagnosing Covid-19; however, none of them pay attention to predict the effect of the virus on the person's body if the infection occurs but before the infection really takes place. Predicting the extent to which people will be affected if they are infected with the virus allows for some drastic precautions to be taken for those who will suffer from serious complications, while allowing some freedom for those who expect not to be affected badly. This paper introduces Covid-19 Prudential Expectation Strategy (CPES) as a new strategy for predicting the behavior of the person's body if he has been infected with Covid-19. The CPES composes of three phases called Outlier Rejection Phase (ORP), Feature Selection Phase (FSP), and Classification Phase (CP). For enhancing the classification accuracy in CP, CPES employs two proposed techniques for outlier rejection in ORP and feature selection in FSP, which are called Hybrid Outlier Rejection (HOR) method and Improved Binary Genetic Algorithm (IBGA) method respectively. In ORP, HOR rejects outliers in the training data using a hybrid method that combines standard division and Binary Gray Wolf Optimization (BGWO) method. On the other hand, in FSP, IBGA as a hybrid method selects the most useful features for the prediction process. IBGA includes Fisher Score (FScore) as a filter method to quickly select the features and BGA as a wrapper method to accurately select the features based on the average accuracy value from several classification models as a fitness function to guarantee the efficiency of the selected subset of features with any classifier. In CP, CPES has the ability to classify people based on their bodies’ reaction to Covid-19 infection, which is built upon a proposed Statistical Naïve Bayes (SNB) classifier after performing the previous two phases. CPES has been compared against recent related strategies in terms of accuracy, error, recall, precision, and run-time using Covid-19 dataset [1]. This dataset contains routine blood tests collected from people before and after their infection with covid-19 through a Web-based form created by us. CPES outperforms the competing methods in experimental results because it provides the best results with values of 0.87, 0.13, 0.84, and 0.79 for accuracy, error, precision, and recall.}
}
@article{RIBA2022108641,
title = {Table detection in business document images by message passing networks},
journal = {Pattern Recognition},
volume = {127},
pages = {108641},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108641},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001224},
author = {Pau Riba and Lutz Goldmann and Oriol Ramos Terrades and Diede Rusticus and Alicia Fornés and Josep Lladós},
keywords = {Business document processing, Anonymized document processing, Table detection, Graph neural networks, Node and edge classification},
abstract = {Tabular structures in business documents offer a complementary dimension to the raw textual data. For instance, there is information about the relationships among pieces of information. Nowadays, digital mailroom applications have become a key service for workflow automation. Therefore, the detection and interpretation of tables is crucial. With the recent advances in information extraction, table detection and recognition has gained interest in document image analysis, in particular, with the absence of rule lines and unknown information about rows and columns. However, business documents usually contain sensitive contents limiting the amount of public benchmarking datasets. In this paper, we propose a graph-based approach for detecting tables in document images which do not require the raw content of the document. Hence, the sensitive content can be previously removed and, instead of using the raw image or textual content, we propose a purely structural approach to keep sensitive data anonymous. Our framework uses graph neural networks (GNNs) to describe the local repetitive structures that constitute a table. In particular, our main application domain are business documents. We have carefully validated our approach in two invoice datasets and a modern document benchmark. Our experiments demonstrate that tables can be detected by purely structural approaches.}
}
@article{2022108733,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {128},
pages = {108733},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(22)00214-X},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200214X}
}
@article{HUANG2022108665,
title = {Quaternion-based weighted nuclear norm minimization for color image restoration},
journal = {Pattern Recognition},
volume = {128},
pages = {108665},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108665},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001467},
author = {Chaoyan Huang and Zhi Li and Yubing Liu and Tingting Wu and Tieyong Zeng},
keywords = {Quaternion representation, Color image restoration, Weighted nuclear norm, Variational method, Low-rank matrix analysis},
abstract = {Color image restoration is one of the basic tasks in pattern recognition. Unlike grayscale image, each color image has three channels in the RGB color space. Due to the inner-relationship within the three channels, color image restoration is usually much more difficult than its grayscale counterpart. Indeed, new problems such as color artifacts could emerge when the grayscale image processing methods are extended to color images directly. Note that one of the most effective gray image restoration methods is the weighted nuclear norm minimization (WNNM) approach. However, when applied to color images, the results of WNNM are usually not as promising as that of grayscale images. In order to solve this problem, in this paper, we propose to restore color images with the quaternion-based WNNM method (QWNNM) since the structure of color channels can be well preserved with quaternion representation. The proposed model can be solved efficiently by the alternating direction method of multipliers (ADMM). The theoretical analysis of the optimal solution is also presented. Numerical experiments are carefully conducted with different kinds of degradation to illustrate the superior performance of our proposed QWNNM over the state-of-the-art methods, including a celebrated deep learning approach, in both visual quality and numerical results.}
}
@article{ZHUGE2022108644,
title = {CubeNet: X-shape connection for camouflaged object detection},
journal = {Pattern Recognition},
volume = {127},
pages = {108644},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108644},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200125X},
author = {Mingchen Zhuge and Xiankai Lu and Yiyou Guo and Zhihua Cai and Shuhan Chen},
keywords = {Camouflaged object detection, Neural network, Edge guidance, Novel feature aggregation},
abstract = {Camouflaged object detection (COD) aims to detect out-of-attention regions in an image. Current binary segmentation solutions fail to tackle COD easily, since COD is more challenging due to object often accompany with weak boundaries, low contrast, or similar patterns to the background. That is, we need a more efficient scheme to address this problem. In this work, we propose a new COD framework called CubeNet by introducing X connection to the standard encoder-decoder architecture. Specifically, CubeNet consists of two square fusion decoder (SFD) and a sub edge decoder (SED). The special designed SFD takes full advantage of low-level and high-level features extracted from encoder-decoder blocks, providing more powerful representations at each stage. To explicitly modeling the weak boundaries of the objects, we introduced a SED between the two SFD. With such kind of holistic designs, these three decoder modules resolve the challenging ambiguity of camouflaged object detection. CubeNet significantly advance the cutting-edge model on three challenging COD datasets (i.e., COD10K, CAMO, and CHAMELEON), and achieves the real-time (50fps) inference.}
}
@article{MUDDAMSETTY2022108604,
title = {Visual explanation of black-box model: Similarity Difference and Uniqueness (SIDU) method},
journal = {Pattern Recognition},
volume = {127},
pages = {108604},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108604},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000851},
author = {Satya M. Muddamsetty and Mohammad N.S. Jahromi and Andreea E. Ciontos and Laura M. Fenoy and Thomas B. Moeslund},
keywords = {Explainable AI (XAI), CNN, Adversarial attack, Eye-tracker},
abstract = {Explainable Artificial Intelligence (XAI) has in recent years become a well-suited framework to generate human understandable explanations of ‘black- box’ models. In this paper, a novel XAI visual explanation algorithm known as the Similarity Difference and Uniqueness (SIDU) method that can effectively localize entire object regions responsible for prediction is presented in full detail. The SIDU algorithm robustness and effectiveness is analyzed through various computational and human subject experiments. In particular, the SIDU algorithm is assessed using three different types of evaluations (Application, Human and Functionally-Grounded) to demonstrate its superior performance. The robustness of SIDU is further studied in the presence of adversarial attack on ’black-box’ models to better understand its performance. Our code is available at: https://github.com/satyamahesh84/SIDU_XAI_CODE.}
}
@article{MALDONADO2022108701,
title = {The Cobb-Douglas Learning Machine},
journal = {Pattern Recognition},
volume = {128},
pages = {108701},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108701},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001820},
author = {Sebastián Maldonado and Julio López and Miguel Carrasco},
keywords = {Cobb-Douglas, Minimax Probability Machine, Minimum Error Minimax Probability Machine, Second-order Cone Programming, Support Vector Machines},
abstract = {In this paper, we propose a novel machine learning approach based on robust optimization. Our proposal defines the task of maximizing the two class accuracies of a binary classification problem as a Cobb-Douglas function. This function is well known in production economics and is used to model the relationship between two or more inputs as well as the quantity produced by those inputs. A robust optimization problem is defined to construct the decision function. The goal of the model is to classify each training pattern correctly, up to a given class accuracy, even for the worst possible data distribution. We demonstrate the theoretical advantages of the Cobb-Douglas function in terms of the properties of the resulting second-order cone programming problem. Important extensions are proposed and discussed, including the use of kernel functions and regularization. Experiments performed on several classification datasets confirm these advantages, leading to the best average performance in comparison to various alternative classifiers.}
}
@article{AHN2022108649,
title = {Efficient deep neural network for photo-realistic image super-resolution},
journal = {Pattern Recognition},
volume = {127},
pages = {108649},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108649},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001303},
author = {Namhyuk Ahn and Byungkon Kang and Kyung-Ah Sohn},
keywords = {Super-resolution, Photo-realistic, Convolutional neural network, Efficient model, Adversarial learning, Multi-scale approach},
abstract = {Recent progress in deep learning-based models has improved photo-realistic (or perceptual) single-image super-resolution significantly. However, despite their powerful performance, many methods are difficult to apply to real-world applications because of the heavy computational requirements. To facilitate the use of a deep model under such demands, we focus on keeping the network efficient while maintaining its performance. In detail, we design an architecture that implements a cascading mechanism on a residual network to boost the performance with limited resources via multi-level feature fusion. In addition, our proposed model adopts group convolution and recursive schemes in order to achieve extreme efficiency. We further improve the perceptual quality of the output by employing the adversarial learning paradigm and a multi-scale discriminator approach. The performance of our method is investigated through extensive internal experiments and benchmarks using various datasets. Our results show that our models outperform the recent methods with similar complexity, for both traditional pixel-based and perception-based tasks.}
}
@article{BASAK2022108673,
title = {MFSNet: A multi focus segmentation network for skin lesion segmentation},
journal = {Pattern Recognition},
volume = {128},
pages = {108673},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108673},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001546},
author = {Hritam Basak and Rohit Kundu and Ram Sarkar},
keywords = {Lesion Segmentation, Deep Learning, Parallel Partial Decoder, Attention Modules, Skin Melanoma},
abstract = {Segmentation is essential for medical image analysis to identify and localize diseases, monitor morphological changes, and extract discriminative features for further diagnosis. Skin cancer is one of the most common types of cancer globally, and its early diagnosis is pivotal for the complete elimination of malignant tumors from the body. This research develops an Artificial Intelligence (AI) framework for supervised skin lesion segmentation employing the deep learning approach. The proposed framework, called MFSNet (Multi-Focus Segmentation Network), uses differently scaled feature maps for computing the final segmentation mask using raw input RGB images of skin lesions. In doing so, initially, the images are preprocessed to remove unwanted artifacts and noises. The MFSNet employs the Res2Net backbone, a recently proposed convolutional neural network (CNN), for obtaining deep features used in a Parallel Partial Decoder (PPD) module to get a global map of the segmentation mask. In different stages of the network, convolution features and multi-scale maps are used in two boundary attention (BA) modules and two reverse attention (RA) modules to generate the final segmentation output. MFSNet, when evaluated on three publicly available datasets: PH2, ISIC 2017, and HAM10000, outperforms state-of-the-art methods, justifying the reliability of the framework. The relevant codes for the proposed approach are accessible at https://github.com/Rohit-Kundu/MFSNet.}
}
@article{KURBAN2022108621,
title = {Human and action recognition using adaptive energy images},
journal = {Pattern Recognition},
volume = {127},
pages = {108621},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108621},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001029},
author = {Onur Can Kurban and Nurullah Calik and Tülay Yildirim},
keywords = {Motion recognition, Human recognition, Correlation coefficients, Deep learning, Behavioral biometrics},
abstract = {In this paper, we propose a new temporal template approach for action recognition and person identification based on motion sequence information in masked depth video streams obtained from RGB-D data. This new representation creates a membership function that models the change in motion based on the correlation between frames that occur during motion flow. The energy images created with this function emphasize the intervals of motion with more change, while the intervals with less change are suppressed. To understand the distinctive features, the obtained energy images by using the proposed function are given as input to the convolutional neural networks and different handcrafted classifiers. The proposed method was observed on the BodyLogin, NATOPS, and SBU Kinect datasets and compared with the existing temporal templates and recent methods. The results indicate that the proposed method provides both higher performance and better motion representation.}
}
@article{NORONHA2022108612,
title = {Impact of metrics on biclustering solution and quality: A review},
journal = {Pattern Recognition},
volume = {127},
pages = {108612},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108612},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000930},
author = {Marta D.M. Noronha and Rui Henriques and Sara C. Madeira and Luis E. Zárate},
keywords = {Merit functions, Biclustering evaluation, Biclustering validation, Bicluster discovery, Systematic review},
abstract = {To understand how subspace clustering algorithms discover distinct bicluster types and how their effectiveness has been validated, we offer a systematic literature review on available merit functions and how they affect the biclustering task. The covered principles are structured within a methodology to show how evaluation and validation measures/metrics determine the bicluster coherence, ensuring the algorithm effectiveness, and the limitations reported in some selected works. The review did not find any metrics that can be used in a generic way to guarantee the effectiveness of a biclustering algorithm when compared to all others. Therefore, the choice of evaluation metrics must meet to specific objectives of the application. So in this work, we present the measures and metrics in 7 major classes, including metrics based on residues, score thresholding, plaid, and order-preserving constraints, space transforms, correlations, theoretical and probabilistic frames, and set operations.}
}
@article{YAN2022108629,
title = {Discriminative information restoration and extraction for weakly supervised low-resolution fine-grained image recognition},
journal = {Pattern Recognition},
volume = {127},
pages = {108629},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108629},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001108},
author = {Tiantian Yan and Jian Shi and Haojie Li and Zhongxuan Luo and Zhihui Wang},
keywords = {Low-resolution, Fine-grained image recognition, Minimum spanning tree, Semantic relation distillation},
abstract = {The existing methods of fine-grained image recognition mainly devote to learning subtle yet discriminative features from the high-resolution input. However, their performance deteriorates significantly when they are used for low quality images because a lot of discriminative details of images are missing. We propose a discriminative information restoration and extraction network, termed as DRE-Net, to address the problem of low-resolution fine-grained image recognition, which has widespread application potential, such as shelf auditing and surveillance scenarios. DRE-Net is the first framework for weakly supervised low-resolution fine-grained image recognition and consists of two sub-networks: (1) fine-grained discriminative information restoration sub-network (FDR) and (2) recognition sub-network with the semantic relation distillation loss (SRD-loss). The first module utilizes the structural characteristic of minimum spanning tree (MST) to establish context information for each pixel by employing the spatial structures between each pixel and other pixels, which can help FDR focus on and restore the critical texture details. The second module employs the SRD-loss to calibrate recognition sub-network by transferring the correct relationships between every two pixels on the feature map. Meanwhile the SRD-loss can further prompt the FDR to recover reliable and accurate fine-grained details and guide the recognition sub-network to perceive the discriminative features from the correct relationships. Extensive experiments on three benchmark datasets and one retail product dataset demonstrate the effectiveness of our proposed framework.}
}
@article{CAI2022108608,
title = {Arbitrarily shaped scene text detection with dynamic convolution},
journal = {Pattern Recognition},
volume = {127},
pages = {108608},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108608},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000899},
author = {Ying Cai and Yuliang Liu and Chunhua Shen and Lianwen Jin and Yidong Li and Daji Ergu},
keywords = {Scene text detection, Image segmentation, Arbitrary shape, Dynamic convolution},
abstract = {Arbitrarily shaped scene text detection has witnessed great development in recent years, and text detection using segmentation has been proven to an effective approach. However, problems caused by the diverse attributes of text instances, such as shapes, scales, and presentation styles (dense or sparse), persist. In this paper, we propose a novel text detector, termed DText, which can effectively formulate an arbitrarily shaped scene text detection task based on dynamic convolution. Our method can dynamically generate independent text-instance-aware convolutional parameters for each text instance from multi-features thus overcoming some intractable limitations of arbitrary text detection, such as the splitting of similar adjacent text, which poses challenges to fixed instance-shared convolutional parameters-based methods. Unlike standard segmentation methods relying on regions-of-interest bounding boxes, DText focuses on enhancing the flexibility of the network to retain details of instances from diverse resolutions while effectively improving prediction accuracy. Moreover, we propose encoding the shape and position information according to the characteristics of the text instance, termed text-shape sensitive position embedding. Thus, it can provide explicit shape and position information to the generator of the dynamic convolution parameters. Experiments on five benchmarks (Total-Text, SCUT-CTW1500, MSRA-TD500, ICDAR2015, and MLT) showed that our method achieves superior detection performance.}
}
@article{2022108681,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {127},
pages = {108681},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(22)00162-5},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001625}
}
@article{JEON2022108592,
title = {Bayesian mixture of gaussian processes for data association problem},
journal = {Pattern Recognition},
volume = {127},
pages = {108592},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108592},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000735},
author = {Younghwan Jeon and Ganguk Hwang},
keywords = {Gaussian processes, Bayesian models, Variational inference, Expectation maximization},
abstract = {We address the data association problem and propose a Bayesian approach based on a mixture of Gaussian Processes (GPs) having two key components, the assignment probabilities and the GPs. In the proposed approach, the two key components are simultaneously updated according to observations through an efficient Expectation-Maximization (EM) algorithm that we develop. The proposed approach is thus more adaptive to the observations than the existing approaches for data association. To validate the performance of the proposed approach, we provide experimental results with real data sets as well as two synthetic data sets. We also provide a theoretical analysis to show the effectiveness of the Bayesian update.}
}
@article{CHEN2022108689,
title = {Nonconvex clustering via ℓ0 fusion penalized regression},
journal = {Pattern Recognition},
volume = {128},
pages = {108689},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108689},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001704},
author = {Huangyue Chen and Lingchen Kong and Yan Li},
keywords = {Penalized clustering,  fusion penalty, Nonconvex discontinuous optimization, Alternating direction method of multipliers},
abstract = {Cluster analysis has attracted widespread attention in the past several decades. Generally speaking, clustering is considered as an important unsupervised learning method because its goal is to discover unknown subgroups in data without category label information. In this paper, we propose the ℓ0 fusion penalized clustering model (ℓ0-PClust), which is a novel clustering framework founded on the penalized regression method. Theoretically, we first analyze the existence of the optimal solutions of our model and deduce an upper bound of the tuning parameter. Then we define the Karush-Kuhn-Tucker point and P-stationary point of the ℓ0-PClust model, and establish the relationship between them and local optimal solutions. Moreover, based on the P-stationary point of the ℓ0-PClust model, we prove that the distances among different cluster centers are greater than a positive threshold. Computationally, we solve the ℓ0-PClust model via the famous alternating direction method of multipliers, whose limit point is a P-stationary point and local optimal solution of the model. Finally, we conduct extensive experiments on both synthetic and real data sets. Experimental results show outstanding performance of our method in comparison with several state-of-the-art clustering methods.}
}
@article{GAO2022108616,
title = {Hierarchical feature disentangling network for universal domain adaptation},
journal = {Pattern Recognition},
volume = {127},
pages = {108616},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108616},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000978},
author = {Yuan Gao and Peipeng Chen and Yue Gao and Jinpeng Wang and YoungSun Pan and Andy J. Ma},
keywords = {Universal domain adaptation, Feature disentanglement, Domain adversarial training, Sample reweighting},
abstract = {Universal Domain Adaptation (UniDA) aims to address a more practical problem compared with traditional Close-Set Domain Adaptation (CSDA). Besides the domain gap in traditional CSDA, the common and private label sets across domains are unknown in UniDA leading to an additional category gap. Without considering the category gap for domain adversarial training to extract domain-relevant features, existing methods may suffer from the feature misalignment problem and result in negative transfer. This paper proposes a Hierarchical Feature Disentangling Network (HFDN) to disentangle domain-relevant features into domain-specific and category-shift features for latent variables caused by domain gap and category gap, respectively. Domain-specific features are trained to distinguish the source domain from the target one by discovering domain-specific attributes (e.g. illumination, style), and adversarially aligned to bridge the domain gap for knowledge transfer. Category-shift features are extracted to distinguish domains by identifying private classes across domains, so that they can be leveraged to assign larger weights for samples from the common label set. Experiments show that the proposed HFDN surpasses state-of-the-art CSDA, partial DA, open-set DA and UniDA models.}
}
@article{WAN2022108603,
title = {R2CI: Information theoretic-guided feature selection with multiple correlations},
journal = {Pattern Recognition},
volume = {127},
pages = {108603},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108603},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200084X},
author = {Jihong Wan and Hongmei Chen and Tianrui Li and Wei Huang and Min Li and Chuan Luo},
keywords = {Feature selection, Information theory, Relevance, Redundancy, Complementarity, Interaction},
abstract = {Information theoretic-guided feature selection approaches (ITFSs), which exploit the uncertainty of information to measure the correlation of features, aim to select the most informative features. However, most previous approaches suffer from two drawbacks. 1) Complementarity and interaction are not valued, leading to features with potential discriminatory information for learning tasks such as classification not being excavated and affecting the effectiveness of learning. 2) The various correlations that exist between features for the class have not been fully considered, and their differentiation and relationships have not been well reflected. To address the former issue, guided by information theory, the complementarity and interaction between features are studied. For the latter, firstly, some ITFSs are reviewed and analyzed in terms of feature correlation. The analysis reveals that considering feature multi-correlation is absent in the selection process. Motivated by this problem, a feature selection algorithm with class-based relevance, redundancy, complementarity, and interaction (R2CI) is designed for the first time. Moreover, the distinctions and connections among different correlations are also explored. The results of comparisons and hypothesis test against competitive algorithms show that R2CI has significant advantages in most cases.}
}
@article{WU2022108610,
title = {An attention-based framework for multi-view clustering on Grassmann manifold},
journal = {Pattern Recognition},
volume = {128},
pages = {108610},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108610},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000917},
author = {Danyang Wu and Xia Dong and Feiping Nie and Rong Wang and Xuelong Li},
keywords = {Multi-view clustering, Grassmann manifold, Principle angles, Attentive weighted-learning scheme},
abstract = {The key problem of multi-view clustering is to handle the inconsistency among multiple views. This article proposes an attention-based framework for multi-view clustering on Grassmann manifold (AMCGM). To be specific, the proposed AMCGM framework aims to learn a representative element on Grassmann manifold with the following four highlights: 1) AMCGM framework performs an attention-based weighted-learning scheme to capture the difference of views; 2) The clustering results can be directly generated by the structured graph learned via AMCGM, avoiding the randomness caused by traditional label-generation procedures, such as K-means clustering; 3) AMCGM has high extensibility since it can generate many multi-view clustering models on Grassmann manifold; 4) On Grassmann manifold, the relationship between the projection metric (PM)-based multi-view clustering model and squared projection metric (SPM)-based model is studied. Based on AMCGM framework, we propose some generated models and provide some useful conclusions. Moreover, to solve the optimization problems involved in the proposed AMCGM framework and generated models, we propose an efficiently iterative algorithm and provide rigorous convergence analysis. Extensive experimental results demonstrate the superb performance of our framework.}
}
@article{DAI2022108628,
title = {Personalized knowledge-aware recommendation with collaborative and attentive graph convolutional networks},
journal = {Pattern Recognition},
volume = {128},
pages = {108628},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108628},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001091},
author = {Quanyu Dai and Xiao-Ming Wu and Lu Fan and Qimai Li and Han Liu and Xiaotong Zhang and Dan Wang and Guli Lin and Keping Yang},
keywords = {Recommender system, Graph convolutional network, Attention mechanism, Knowledge graph},
abstract = {Knowledge graphs (KGs) are increasingly used to solve the data sparsity and cold start problems of collaborative filtering. Recently, graph neural networks (GNNs) have been applied to build KG-based recommender systems and achieved competitive performance. However, existing GNN-based methods are either limited in their ability to capture fine-grained semantics in a KG, or insufficient in effectively modeling user-item interactions. To address these issues, we propose a novel framework with collaborative and attentive graph convolutional networks for personalized knowledge-aware recommendation. Particularly, we model the user-item graph and the KG separately and simultaneously with an efficient graph convolutional network and a personalized knowledge graph attention network, where the former aims to extract informative collaborative signals, while the latter is designed to capture fine-grained semantics. Collectively, they are able to learn meaningful node representations for predicting user-item interactions. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed method compared with state-of-the-arts.}
}
@article{GUPTA2022108496,
title = {Nested conformal prediction and quantile out-of-bag ensemble methods},
journal = {Pattern Recognition},
volume = {127},
pages = {108496},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108496},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006725},
author = {Chirag Gupta and Arun K. Kuchibhotla and Aaditya Ramdas},
keywords = {Conformal prediction, Quantile regression, Cross-conformal, Out-of-bag methods, Ensemble methods, Random forests},
abstract = {Conformal prediction is a popular tool for providing valid prediction sets for classification and regression problems, without relying on any distributional assumptions on the data. While the traditional description of conformal prediction starts with a nonconformity score, we provide an alternate (but equivalent) view that starts with a sequence of nested sets and calibrates them to find a valid prediction set. The nested framework subsumes all nonconformity scores, including recent proposals based on quantile regression and density estimation. While these ideas were originally derived based on sample splitting, our framework seamlessly extends them to other aggregation schemes like cross-conformal, jackknife+ and out-of-bag methods. We use the framework to derive a new algorithm (QOOB, pronounced cube) that combines four ideas: quantile regression, cross-conformalization, ensemble methods and out-of-bag predictions. We develop a computationally efficient implementation of cross-conformal, that is also used by QOOB. In a detailed numerical investigation, QOOB performs either the best or close to the best on all simulated and real datasets.}
}
@article{WANG2022108636,
title = {EANet: Iterative edge attention network for medical image segmentation},
journal = {Pattern Recognition},
volume = {127},
pages = {108636},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108636},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001170},
author = {Kun Wang and Xiaohong Zhang and Xiangbo Zhang and Yuting Lu and Sheng Huang and Dan Yang},
keywords = {Medical image segmentation, Dynamic scale-aware context, Edge attention preservation, Multi-level pairwise regression, Computer-aided diagnosis (CAD)},
abstract = {Accurate and automatic segmentation of medical images can greatly assist the clinical diagnosis and analysis. However, it remains a challenging task due to (1) the diversity of scale in the medical image targets and (2) the complex context environments of medical images, including ambiguity of structural boundaries, complexity of shapes, and the heterogeneity of textures. To comprehensively tackle these challenges, we propose a novel and effective iterative edge attention network (EANet) for medical image segmentation with steps as follows. First, we propose a dynamic scale-aware context (DSC) module, which dynamically adjusts the receptive fields to extract multi-scale contextual information efficiently. Second, an edge-attention preservation (EAP) module is employed to effectively remove noise and help the edge stream focus on processing only the boundary-related information. Finally, a multi-level pairwise regression (MPR) module is designed to combine the complementary edge and region information for refining the ambiguous structure. This iterative optimization helps to learn better representations and more accurate saliency maps. Extensive experimental results demonstrate that the proposed network achieves superior segmentation performance to state-of-the-art methods in four different challenging medical segmentation tasks, including lung nodule segmentation, COVID-19 infection segmentation, lung segmentation, and thyroid nodule segmentation. The source code of our method is available at https://github.com/DLWK/EANet}
}
@article{WANG2022108668,
title = {Erlang planning network: An iterative model-based reinforcement learning with multi-perspective},
journal = {Pattern Recognition},
volume = {128},
pages = {108668},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108668},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001492},
author = {Jiao Wang and Lemin Zhang and Zhiqiang He and Can Zhu and Zihui Zhao},
keywords = {Model-based reinforcement learning, Multi-perspective, Bi-level, Planning, Trajectory imagination},
abstract = {For model-based reinforcement learning (MBRL), one of the key challenges is modeling error, which cripples the effectiveness of model planning and causes poor robustness during training. In this paper, we propose a bi-level Erlang Planning Network (EPN) architecture, which is composed of an upper-level agent and several multi-scale parallel sub-agents, trained in an iterative way. The proposed method focuses upon the expansion of representation by environment: a multi-perspective over the world model, which presents a varied way to represent an agent’s knowledge about the world that alleviates the problem of falling into local optimal points and enhances robustness during the progress of model planning. Moreover, our experiments evaluate EPN on a range of continuous-control tasks in MuJoCo, the evaluation results show that the proposed framework finds exemplar solutions faster and consistently reaches the state-of-the-art performance.}
}
@article{DUAN2022108676,
title = {MS2GAH: Multi-label semantic supervised graph attention hashing for robust cross-modal retrieval},
journal = {Pattern Recognition},
volume = {128},
pages = {108676},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108676},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001571},
author = {Youxiang Duan and Ning Chen and Peiying Zhang and Neeraj Kumar and Lunjie Chang and Wu Wen},
keywords = {Cross-modal retrieval, Deep hashing, Graph attention network},
abstract = {Due to the strong nonlinear representation capabilities of deep neural networks and the low storage and high efficiency characteristics of hash learning, deep cross-modal hashing has been propelled to the forefront of academics. How to preferably bridge semantic relevance to further bridge the semantic modality gap is the vital bottleneck to improve model performance. Confronting samples with rich semantics, how to comprehensively explore the hidden correlations and establish more precise modality relationships is the primary issue to be solved. In this work, we propose a novel deep hashing method called Multi-Label Semantic Supervised Graph Attention Hashing (MS2GAH), which is an end-to-end framework that integrates graph attention networks (GATs). It constructs graph features through the adjacency of nodes and assigns different weights to adjacent edges to enhance the robustness of the model. Simultaneously, multi-label annotations are utilized to bridge the semantic relevance between modalities in a more fine-grained manner. To make preferable use of rich semantic information, an end-to-end label encoder is designed to mine high-level semantics from multi-label annotations to guide the feature extraction process of specific-modality networks, thereby further narrowing the modality gap. Finally, extensive experiments have been conducted on four datasets, and the results show that MS2GAH is superior to other baselines and one step forward.}
}
@article{XU2022108624,
title = {SA-DPNet: Structure-aware dual pyramid network for salient object detection},
journal = {Pattern Recognition},
volume = {127},
pages = {108624},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108624},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001054},
author = {Xuemiao Xu and Jiaxing Chen and Huaidong Zhang and Guoqiang Han},
keywords = {Saliency detection, Structure coherence, Deep neural network},
abstract = {Salient object detection aims at highlighting the most visually distinctive objects in the scene. Previous deep learning based works mainly focus on designing different integration strategies of multi-level features to improve the quality of prediction. However, due to the negligence of spatial structure coherence in predicted saliency maps, they fail to produce satisfactory results in complex scenarios. In this work, we present a structure-aware dual pyramid network (SA-DPNet) for salient object detection. By explicitly formulating spatial location information and spatial covariance features into the self-attention mechanism, a structure-aware spatial non-local block is proposed in SA-DPNet to learn the spatial-sensitive global context. With the proposed edge loss and adversarial loss, the edge structure context and patch-based global structure context are introduced to refine the structural coherence of the predicted results. Comprehensive experimental results on six RGB saliency benchmark datasets and three RGB-D saliency benchmark datasets demonstrate the superiority of proposed SA-DPNet over other state-of-the-art methods, both quantitatively and visually.}
}
@article{NAN2022108648,
title = {Automatic fine-grained glomerular lesion recognition in kidney pathology},
journal = {Pattern Recognition},
volume = {127},
pages = {108648},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108648},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001297},
author = {Yang Nan and Fengyi Li and Peng Tang and Guyue Zhang and Caihong Zeng and Guotong Xie and Zhihong Liu and Guang Yang},
keywords = {Deep convolutional neural network, Glomerulus segmentation, Fine-grained lesion classification, Uncertainty assessment, Kidney pathology},
abstract = {Recognition of glomeruli lesions is the key for diagnosis and treatment planning in kidney pathology; however, the coexisting glomerular structures such as mesangial regions exacerbate the difficulties of this task. In this paper, we introduce a scheme to recognize fine-grained glomeruli lesions from whole slide images. First, a focal instance structural similarity loss is proposed to drive the model to locate all types of glomeruli precisely. Then an Uncertainty Aided Apportionment Network is designed to carry out the fine-grained visual classification without bounding-box annotations. This double branch-shaped structure extracts common features of the child class from the parent class and produces the uncertainty factor for reconstituting the training dataset. Results of slide-wise evaluation illustrate the effectiveness of the entire scheme, with an 8–22% improvement of the mean Average Precision compared with remarkable detection methods. The comprehensive results clearly demonstrate the effectiveness of the proposed method.}
}
@article{SANTRA2022108627,
title = {Graph-based modelling of superpixels for automatic identification of empty shelves in supermarkets},
journal = {Pattern Recognition},
volume = {127},
pages = {108627},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108627},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200108X},
author = {Bikash Santra and Udita Ghosh and Dipti Prasad Mukherjee},
keywords = {Gap detection, Retail store, Graph convolutional network, Siamese network, Structural support vector machine},
abstract = {Automatic detection of empty spaces (gaps) between the displayed products as seen in the images of shelves of a supermarket is an interesting image segmentation problem. This paper presents the first known attempt to solve this commercially relevant challenge. The shelf image is first over-segmented into a number of superpixels to construct a graph of superpixels (SG). Subsequently, a graph convolutional network and a Siamese network are built to process the SG. Finally, a structural support vector machine based inference model is formulated based on SG for segmenting the gap and non-gap regions. In order to validate our method, we manually annotate the images of shelves of three benchmark datasets of retail products. We have achieved ∼70 to ∼85% segmentation accuracy (in terms of mean intersection-over-union) on the annotated datasets. A part of the annotated data is released at https://github.com/gapDetection/gapDetectionDatasets.}
}
@article{HE2022108587,
title = {Temporal-adaptive sparse feature aggregation for video object detection},
journal = {Pattern Recognition},
volume = {127},
pages = {108587},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108587},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000681},
author = {Fei He and Qiaozhe Li and Xin Zhao and Kaiqi Huang},
keywords = {Video object detection, Temporal-adaptive sparse sampling, Pixel-adaptive aggregation, Object-relational aggregation},
abstract = {Video object detection is a challenging task due to the appearance deterioration in video frames. To enhance feature representation of the deteriorated frames, previous methods usually aggregate features from fixed-density and fixed-length adjacent frames. However, due to the redundancy of videos and irregular object movements over time, temporal information may not be efficiently exploited using the traditional inflexible strategy. Alternatively, we present a temporal-adaptive sparse feature aggregation framework, an accurate and efficient method for video object detection. Instead of adopting a fixed-density and fixed-length window fusion strategy, a temporal-adaptive sparse sampling strategy is proposed using a stride predictor to encode informative frames more efficiently. A collaborative feature aggregation framework, which consists of a pixel-adaptive aggregation module and an object-relational aggregation module, is proposed for feature enhancement. The pixel-adaptive aggregation module enhances pixel-level features on the current frame using corresponding pixel-level features from other frames. Similarly, the object-relational aggregation module further enhances feature representation at proposal level. A graph is constructed to model the relations between different proposals so that the relation features and proposal features are adaptively fused for feature enhancement. Experiments demonstrate that our proposed framework significantly surpasses traditional dense aggregation methods, and comprehensive ablation studies verify the effectiveness of each proposed module in our framework.}
}
@article{LI2022108684,
title = {Spatial information enhancement network for 3D object detection from point cloud},
journal = {Pattern Recognition},
volume = {128},
pages = {108684},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108684},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001650},
author = {Ziyu Li and Yuncong Yao and Zhibin Quan and Jin Xie and Wankou Yang},
keywords = {3D object detection, Autonomous vehicles, Point cloud, LiDAR sensor, 3D shape completion},
abstract = {LiDAR-based 3D object detection pushes forward an immense influence on autonomous vehicles. Due to the limitation of the intrinsic properties of LiDAR, fewer points are collected at the objects farther away from the sensor. This imbalanced density of point clouds degrades the detection accuracy but is generally neglected by previous works. To address the challenge, we propose a novel two-stage 3D object detection framework, named SIENet. Specifically, we design the Spatial Information Enhancement (SIE) module to predict the spatial shapes of the foreground points within proposals, and extract the structure information to learn the representative features for further box refinement. The predicted spatial shapes are complete and dense point sets, thus the extracted structure information contains more semantic representation. Besides, we design the Hybrid-Paradigm Region Proposal Network (HP-RPN) which includes multiple branches to learn discriminate features and generate accurate proposals for the SIE module. Extensive experiments on the KITTI 3D object detection benchmark show that our elaborately designed SIENet outperforms the state-of-the-art methods by a large margin. Codes will be publicly available at https://github.com/Liz66666/SIENet.}
}
@article{DENTAMARO2022108656,
title = {AUCO ResNet: an end-to-end network for Covid-19 pre-screening from cough and breath},
journal = {Pattern Recognition},
volume = {127},
pages = {108656},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108656},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001376},
author = {Vincenzo Dentamaro and Paolo Giglio and Donato Impedovo and Luigi Moretti and Giuseppe Pirlo},
keywords = {Audio classification, Spectrograms, Attention mechanism, Covid, Pre-screening, Convolutional neural network},
abstract = {This study presents the Auditory Cortex ResNet (AUCO ResNet), it is a biologically inspired deep neural network especially designed for sound classification and more specifically for Covid-19 recognition from audio tracks of coughs and breaths. Differently from other approaches, it can be trained end-to-end thus optimizing (with gradient descent) all the modules of the learning algorithm: mel-like filter design, feature extraction, feature selection, dimensionality reduction and prediction. This neural network includes three attention mechanisms namely the squeeze and excitation mechanism, the convolutional block attention module, and the novel sinusoidal learnable attention. The attention mechanism is able to merge relevant information from activation maps at various levels of the network. The net takes as input raw audio files and it is able to fine tune also the features extraction phase. In fact, a Mel-like filter is designed during the training, thus adapting filter banks on important frequencies. AUCO ResNet has proved to provide state of art results on many datasets. Firstly, it has been tested on many datasets containing Covid-19 cough and breath. This choice is related to the fact that that cough and breath are language independent, allowing for cross dataset tests with generalization aims. These tests demonstrate that the approach can be adopted as a low cost, fast and remote Covid-19 pre-screening tool. The net has also been tested on the famous UrbanSound 8K dataset, achieving state of the art accuracy without any data preprocessing or data augmentation technique.}
}
@article{WANG2022108595,
title = {Domain generalization and adaptation based on second-order style information},
journal = {Pattern Recognition},
volume = {127},
pages = {108595},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108595},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000760},
author = {Hao Wang and Xiaojun Bi},
keywords = {Domain generalization, Unsupervised domain adaptation, Two-level style normalization and restitution, Second-order statistics, Dynamic affine parameter},
abstract = {Domain generalization (DG) and unsupervised domain adaptation (UDA) aim to solve the domain-shift problem that arises when the trained model is tested in the domain with different style distribution from the training data. Style Normalization and Restitution(SNR) has solved this problem to a certain extent and achieved the best performance. However, SNR ignores the discriminative information encoded in the appearance style information, which limits the performance of the model. In this paper, we propose Two-level Style Normalization and Restitution(Tl-SNR) to solve this problem. First, we use group whitening to introduce the appearance style information encoded in the second-order statistic into the SNR, which prepares for restituting the task-relevant discriminative information in the appearance information later. Secondly, we defined dynamic affine parameters, which improves the affine parameters in group whitening. It makes the model adjust adaptively according to the characteristics of the sample, so as to better exploit the capabilities of the model. Finally, we designed a Two-level Style Normalization and Restitution module based on the improved group whitening for domain generalization and unsupervised domain adaptation. Extensive experiments show that our method is effective. And our method outperforms state-of-the-art DG and UDA methods on four benchmarks.}
}
@article{SRIVASTVA2022108640,
title = {Statistical independence of ECG for biometric authentication},
journal = {Pattern Recognition},
volume = {127},
pages = {108640},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108640},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001212},
author = {Ranjeet Srivastva and Yogendra Narain Singh and Ashutosh Singh},
keywords = {Biometric authentication, Electrocardiogram, Statistical independence, Probability of resemblance},
abstract = {The biometric authentication system using electrocardiogram (ECG) may protect individuals’ privacy and prevent identity frauds. Researchers have demonstrated that ECG is suitable for biometrics use due to its pervasiveness, immutability, measurability, acceptance, and individuality. However, ECG’s statistical independence for biometric authentication has yet to be substantiated. Thereby, this paper proposes a novel model to evaluate the statistical independence of ECG among individuals using heartbeat morphological features. The signal is qualitatively improved and heartbeat features are extracted using signal processing techniques. Three classes of features such as interval, amplitude, and angle are extracted from each heartbeat. The hypothesis estimating the probability of resemblance of interval, amplitude, and angle classes of features is derived. The accumulated effect of these classes of features measure the statistical independence of ECG. Further, the proposed model of statistical independence of ECG biometrics is validated by comparing the statistical performance with the empirical performance of the ECG verification system. The empirical performance is estimated using three different ECG biometric methods, i.e., traditional intraclass-interclass features, artificial neural network, and convolutional neural network. The false resemblance probabilities of heartbeats among individuals computed for four interval class features, five amplitude class features, and five angle class features are found to be 3.4×10−6, 1.0×10−7, and 3.9×10−8, respectively. The cumulative probability of resemblance computed using fourteen heartbeat features of interval, amplitude, and angle classes is found as 1.3×10−20.}
}
@article{HUI2022108664,
title = {Gradient-based refined class activation map for weakly supervised object localization},
journal = {Pattern Recognition},
volume = {128},
pages = {108664},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108664},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001455},
author = {Wenjun Hui and Chuangchuang Tan and Guanghua Gu and Yao Zhao},
keywords = {Weakly supervised object localization, Gradients of loss function, Class-specific mask, Bounding box revision, Category consistency},
abstract = {Weakly supervised object localization locates objects based on the localization map generated from the classification network. However, most existing methods utilize the information of the target class to locate objects based on the feature map of a single image, which ignores both the relationships of inter-class and intra-class. In this work, we propose a Gradient-based Refined Class Activation Map (GRCAM) approach to achieve more accurate localization. Two kinds of gradients are applied to reveal the relationships of inter-class and intra-class during the testing stage. First, we exploit the gradients of the classification loss function concerning the feature map to enhance class-specific information. The gradients of classification loss reveal the connection among the predicted probabilities of all classes. Second, we design a regression function that refers to the loss between the pseudo-bounding box coordinates containing category consistency and the predicted coordinates generated from the localization map. The predicted coordinates are revised by the gradients of the regression function. The gradients of the regression function reveal the consistency within a class. Despite the apparent simplicity, we demonstrate the advantages of GRCAM on ILSVRC and CUB-200-2011 in extensive experiments. Especially, on ILSVRC dataset, the proposed GRCAM achieves a new state-of-the-art Top-1 localization error of 42.94%.}
}
@article{FANG2022108672,
title = {Enhanced task attention with adversarial learning for dynamic multi-task CNN},
journal = {Pattern Recognition},
volume = {128},
pages = {108672},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108672},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001534},
author = {Yuchun Fang and Shiwei Xiao and Menglu Zhou and Sirui Cai and Zhaoxiang Zhang},
keywords = {Deep learning, Adversarial learning, Multi-task learning},
abstract = {Multi-task deep learning is promising to solve multi-label multi-instance visual recognition tasks. However, flexible information sharing in the task group might bring performance bottlenecks to an individual task. To tackle this problem, we propose a novel learning framework of multi-task Convolutional Neural Network (CNN) to enhance task attention through conditionally tuning the Task Transfer Connections (TTC) with adversarial learning. For the dynamic multi-task CNN, we set up a shared subnet to extract shared features across multiple tasks and a task discriminator shared by all layers to distinguish features of all subnets. The adversarial training is introduced between the shared subnet and the task discriminator to guide each task subnet to focus on its specific task. To apply adversarial learning to the complex labeling system of multiple tasks, we design an even-label strategy for the multi-task model with a shared subnet to make adversarial learning feasible for the complex labeling system of multiple tasks. As a result, the proposed model can constrain the shared subnet’s learning unbiased to any single task and achieve task attention for all task subnets. Experimental results of the ablation study and the TTC analysis validate the effectiveness of the proposed approach.}
}
@article{WU2022108620,
title = {LiTMNet: A deep CNN for efficient HDR image reconstruction from a single LDR image},
journal = {Pattern Recognition},
volume = {127},
pages = {108620},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108620},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001017},
author = {Guotao Wu and Ran Song and Mingxin Zhang and Xiaolei Li and Paul L. Rosin},
keywords = {HDR image reconstruction, Lightweight CNN, Inverse tone mapping},
abstract = {Existing methods can generate a high dynamic range (HDR) image from a single low dynamic range (LDR) image using convolutional neural networks (CNNs). However, they are too cumbersome to run on mobile devices with limited computational resources. In this work, we design a lightweight CNN, namely LiTMNet which takes a single LDR image as input and recovers the lost information in its saturated regions to reconstruct an HDR image. To avoid trading off the reconstruction quality for efficiency, LiTMNet does not only adapt a lightweight encoder for efficient feature extraction, but also contains newly designed upsampling blocks in the decoder to alleviate artifacts and further accelerate the reconstruction. The final HDR image is produced by nonlinearly blending the network prediction and the original LDR image. Qualitative and quantitative comparisons demonstrate that LiTMNet produces HDR images of high quality comparable with the current state of the art and is 38× faster as tested on a mobile device. Please refer to the supplementary video for additional visual results.}
}
@article{ZHANG2022108696,
title = {Causal GraphSAGE: A robust graph method for classification based on causal sampling},
journal = {Pattern Recognition},
volume = {128},
pages = {108696},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108696},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001777},
author = {Tao Zhang and Hao-Ran Shan and Max A. Little},
keywords = {Causal GraphSAGE, GraphSAGE, Causal sampling, Robustness, Causal inference},
abstract = {GraphSAGE is a widely-used graph neural network for classification, which generates node embeddings in two steps: sampling and aggregation. In this paper, we introduce causal inference into the GraphSAGE sampling stage, and propose Causal GraphSAGE (C-GraphSAGE) to improve the robustness of the classifier. In C-GraphSAGE, we use causal bootstrapping to obtain a weighting between the target node's neighbors and their label. Then, these weights are used to resample the node's neighbors to enforce the robustness of the sampling stage. Finally, an aggregation function is trained to integrate the features of the selected neighbors to obtain the embedding of the target node. Experimental results on the Cora, Pubmed, and Citeseer citation datasets show that the classification performance of C-GraphSAGE is equivalent to that of GraphSAGE, GCN, GAT, and RL-GraphSAGE in the case of no perturbation, and outperforms these as the perturbation ratio increases.}
}
@article{ALPAR2022108675,
title = {Nakagami-Fuzzy imaging framework for precise lesion segmentation in MRI},
journal = {Pattern Recognition},
volume = {128},
pages = {108675},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108675},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200156X},
author = {Orcan Alpar and Rafael Dolezal and Pavel Ryska and Ondrej Krejcar},
keywords = {Nakagami imaging, Fuzzy c-means, Lesion segmentation, MRI, BraTS},
abstract = {Nakagami distribution and related imaging methods are very efficient in diagnostic ultrasonography for visualization and characterization of tissues for years. Abnormalities in tissues are distinguished from surrounding cells by application of the distribution ruled by the Nakagami m-parameter. The potential of discrimination in ultrasonography enables intelligent segmentation of lesions by other diagnostic tools and the imaging technique is very promising in other areas of medicine, like magnetic resonance imaging (MRI) for brain lesion identification, as presented in this paper. Therefore, we propose a novel Nakagami-Fuzzy imaging framework for intelligent and fully automated suspicious region segmentation from axial FLAIR MRI images exhibiting brain tumor characteristics to satisfy ground truth images with different precision levels. The images from MRI data set are processed by applying Nakagami distribution from pre-Rayleigh to post-Rayleigh for adjusting m-parameter. Amorphous and non-homogenous suspicious regions revealed by Nakagami imaging are segmented using customized Fuzzy 2-means to compare with two types of binary ground truths. The framework we propose is an outstanding example of fuzzy-based expert systems providing an average of 92.61% dice score for the main clinical experiment we conducted using the images and two types of ground truths provided by University of Hospital, Hradec Kralove. We also tested our framework by the BraTS 2012 and BraTS 2020 datasets and achieved an average of 91.88% and 89.25% dice scores respectively, which are competitive among the relevant researches.}
}
@article{BEDRATYUK2022108607,
title = {Non-separable rotation moment invariants},
journal = {Pattern Recognition},
volume = {127},
pages = {108607},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108607},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000887},
author = {Leonid Bedratyuk and Jan Flusser and Tomáš Suk and Jitka Kostková and Jaroslav Kautsky},
keywords = {Image recognition, Rotation invariants, Non-separable moments, Appell polynomials, Bi-orthogonality, Recurrent relation},
abstract = {In this paper, we introduce new rotation moment invariants, which are composed of non-separable Appell moments. We prove that Appell polynomials behave under rotation as monomials, which enables easy construction of the invariants. We show by extensive tests that non-separable moments may outperform the separable ones in terms of recognition power and robustness thanks to a better distribution of their zero curves over the image space.}
}
@article{YANG2022108652,
title = {Searching part-specific neural fabrics for human pose estimation},
journal = {Pattern Recognition},
volume = {128},
pages = {108652},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108652},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001339},
author = {Sen Yang and Wankou Yang and Zhen Cui},
keywords = {Human pose estimation, Neural architecture search, Cell-based neural fabrics, Micro and macro search space, Prior knowledge, Part-specific},
abstract = {Neural architecture search (NAS) has emerged in many domains to jointly learn the architectures and weights of neural networks. The core spirit behind NAS is to automatically search neural architectures for target tasks with better performance-efficiency trade-offs. However, existing approaches emphasize on only searching a single architecture with less human intervention to replace a human-designed neural network, yet making the search process almost independent of the domain knowledge. In this paper, we aim to apply NAS for human pose estimation and we ask: when NAS meets this localization task, can the articulated human body structure help to search better task-specific architectures? To this end, we first design a new neural architecture search space, Cell-based Neural Fabric (CNF), to learn micro as well as macro neural architecture using a differentiable search strategy. Then, by viewing locating human parts as multiple disentangled prediction sub-tasks, we exploit the compositionality of human body structure as guidance to search multiple part-specific CNFs specialized for different human parts. After the search, all these part-specific neural fabrics have been tailored with distinct micro and macro architecture parameters. The results show that such knowledge-guided NAS-based model outperforms a hand-crafted part-based baseline model, and the resulting multiple part-specific architectures gain significant performance improvement against a single NAS-based architecture for the whole body. The experiments on MPII and COCO datasets show that our models11Code is available at https://github.com/yangsenius/PoseNFS. achieve comparable performance against the state-of-the-art methods while being relatively lightweight.}
}
@article{ABATE2022108591,
title = {Head pose estimation: An extensive survey on recent techniques and applications},
journal = {Pattern Recognition},
volume = {127},
pages = {108591},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108591},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000723},
author = {Andrea F. Abate and Carmen Bisogni and Aniello Castiglione and Michele Nappi},
keywords = {Biometrics, Head pose estimation, Face recognition, Frontalization},
abstract = {Biometric based systems are involved in many areas, from surveillance to user authentication, from autonomous systems to human-robot interactions. Head pose estimation (HPE) is the task to support biometric systems in which any of the biometric traits of the head is involved, as face, ear or iris. This particular biometric branch finds its application in driver attention detection, surveillance for recognition, face frontalization, best frame selection and so on. The goal of HPE is to determine the head pose orientation (yaw, pitch, roll). The implemented methods use different techniques depending on the kind of input. In this survey we present an overview of involved datasets, recent techniques and applications. We evaluate and compare the different approaches with respect to their advantages and practical usage. In addition, we propose a technical comparison between training and training-free techniques for the most popular HPE methods.}
}
@article{ZHENG2022108615,
title = {Soft pseudo-Label shrinkage for unsupervised domain adaptive person re-identification},
journal = {Pattern Recognition},
volume = {127},
pages = {108615},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108615},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000966},
author = {Dingyuan Zheng and Jimin Xiao and Ke Chen and Xiaowei Huang and Lin Chen and Yao Zhao},
keywords = {Person re-identification, Unsupervised domain adaptation, Clustering algorithms, Label noise, Soft pseudo-labels},
abstract = {One effective way to tackle unsupervised domain adaptation (UDA) on person re-identification (Re-ID) is to use clustering-based self-training approach, where a model is trained with hard pseudo-labels obtained from a clustering method. Using a hard pseudo-label, a sample is assigned to the cluster with the highest probability, which is sensitive to the incorrect clustering result due to imperfect clustering algorithms. Soft pseudo-labels can mitigate this issue by representing the sample with the full range of class probabilities from all clusters. Specifically, soft pseudo-labels comprise probabilities of full range classes, because they consider both the hard samples and easy samples. This will distract the model from learning more discriminative features in the hard examples. To solve this issue, we propose a coarse-to-fine refinement mechanism to produce robust refined soft pseudo-labels by progressively focusing more on the hard samples while less on the easy samples. The proposed refined soft pseudo-labels can be readily integrated into cross-entropy loss as a strong supervision to guide the model to learn more discriminative features. Extensive experiments demonstrate that our proposed method outperforms the state-of-the-art unsupervised domain adaptation approaches on person Re-ID with a considerable margin. Code will be available at: http://github.com/Dingyuan-Zheng/ctf-UDA.}
}
@article{RAMAN2022108660,
title = {Synthetic document generator for annotation-free layout recognition},
journal = {Pattern Recognition},
volume = {128},
pages = {108660},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108660},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001418},
author = {Natraj Raman and Sameena Shah and Manuela Veloso},
keywords = {Synthetic image generation, Bayesian network, Layout analysis},
abstract = {Analyzing the layout of a document to identify headers, sections, tables, figures etc. is critical to understanding its content. Deep learning based approaches for detecting the layout structure of document images have been promising. However, these methods require a large number of annotated examples during training, which are both expensive and time consuming to obtain. We describe here a synthetic document generator that automatically produces realistic documents with labels for spatial positions, extents and categories of the layout elements. The proposed generative process treats every physical component of a document as a random variable and models their intrinsic dependencies using a Bayesian Network graph. Our hierarchical formulation using stochastic templates allow parameter sharing between documents for retaining broad themes and yet the distributional characteristics produces visually unique samples, thereby capturing complex and diverse layouts. We empirically illustrate that a deep layout detection model trained purely on the synthetic documents can match the performance of a model that uses real documents.}
}