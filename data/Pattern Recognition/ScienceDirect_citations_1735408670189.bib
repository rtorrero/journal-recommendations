@article{WANG2023109460,
title = {Skeleton estimation of directed acyclic graphs using partial least squares from correlated data},
journal = {Pattern Recognition},
volume = {139},
pages = {109460},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109460},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001607},
author = {Xiaokang Wang and Shan Lu and Rui Zhou and Huiwen Wang},
keywords = {Directed acyclic graph, partial least squares, hierarchical clustering, sparse learning},
abstract = {Directed acyclic graphs (DAGs) are directed graphical models that are well known for discovering causal relationships between variables in a high-dimensional setting. When the DAG is not identifiable due to the lack of interventional data, the skeleton can be estimated using observational data, which is formed by removing the direction of the edges in a DAG. In real data analyses, variables are often highly correlated due to some form of clustered sampling, and ignoring this correlation will inflate the standard errors of the parameter estimates in the regression-based DAG structure learning framework. In this work, we propose a two-stage DAG skeleton estimation approach for highly correlated data. First, we propose a novel neighborhood selection method based on sparse partial least squares (PLS) regression, and a cluster-weighted adaptive penalty is imposed on the PLS weight vectors to exploit the local information. In the second stage, the DAG skeleton is estimated by evaluating a set of conditional independence hypotheses. Simulation studies are presented to demonstrate the effectiveness of the proposed method. The algorithm is also tested on publicly available datasets, and we show that our algorithm obtains higher sensitivity with comparable false discovery rates for high-dimensional data under different network structures.}
}
@article{YANG2023109526,
title = {Triple-attention interaction network for breast tumor classification based on multi-modality images},
journal = {Pattern Recognition},
volume = {139},
pages = {109526},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109526},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002261},
author = {Xiao Yang and Xiaoming Xi and Kesong Wang and Liangyun Sun and Lingzhao Meng and Xiushan Nie and Lishan Qiao and Yilong Yin},
keywords = {Breast tumor classification, Multi-modality fusion, Triple inter-modality interaction, Intra-modality interaction},
abstract = {Breast cancer can be diagnosed using medical imaging. Classification performance of medical imaging can be improved by multi-modality image fusion. However, existing fusion algorithm fail to consider the importance of modality interactions and cannot fully utilize multi-modality information. Attention mechanisms can effectively explore and combine multi-modality information. Thus, we propose a novel triple-attention interaction network for breast tumor classification based on diffusion-weighted imaging (DWI) and apparent dispersion coefficient (ADC) images. A triple inter-modality interaction mechanism is proposed to fully fuse the multi-modality information. Three modal interactions were performed through the developed inter-modality relation module, channel interaction module, and multi-level attention fusion module to explore the correlation, complementary, and discriminative information, respectively. Additionally, we introduce a novel dual parallel-attention module for the incorporation of spatial and channel attention to improve the discriminative ability of single-modality features. Using these mechanisms, the proposed algorithm can mine and explore useful multi-modality information fully, to improve classification performance. Experimental results demonstrate that our algorithm outperforms other multi-modality fusion algorithm, and extensive ablation studies were conducted to verify the advantages of our algorithm. The area under the receiver operating characteristic curve, accuracy, specificity, and sensitivity were 90.5%, 89.0%, 85.6%, and 92.4%, respectively.}
}
@article{NIKPOUR2023109428,
title = {Spatio-temporal hard attention learning for skeleton-based activity recognition},
journal = {Pattern Recognition},
volume = {139},
pages = {109428},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109428},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001292},
author = {Bahareh Nikpour and Narges Armanfard},
keywords = {Temporal attention, Spatial attention, Spatio-temporal attention, Activity recognition, Skeleton data, Deep reinforcement learning},
abstract = {The use of skeleton data for activity recognition has become prevalent due to its advantages over RGB data. A skeleton video includes frames showing two- or three-dimensional coordinates of human body joints. For recognizing an activity, not all the video frames are informative, and only a few key frames can well represent an activity. Moreover, not all joints participate in every activity; i.e., the key joints may vary across frames and activities. In this paper, we propose a novel framework for finding temporal and spatial attentions in a cooperative manner for activity recognition. The proposed method, which is called STH-DRL, consists of a temporal agent and a spatial agent. The temporal agent is responsible for finding the key frames, i.e., temporal hard attention finding, and the spatial agent attempts to find the key joints, i.e., spatial hard attention finding. We formulate the search problems as Markov decision processes and train both agents through interacting with each other using deep reinforcement learning. Experimental results on three widely used activity recognition benchmark datasets demonstrate the effectiveness of our proposed method.}
}
@article{FAN2023109518,
title = {Markov clustering regularized multi-hop graph neural network},
journal = {Pattern Recognition},
volume = {139},
pages = {109518},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109518},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002182},
author = {Xiaolong Fan and Maoguo Gong and Yue Wu},
keywords = {Graph data mining, Graph neural network, Graph-level representation learning, Graph pattern recognition},
abstract = {Graph Neural Networks (GNNs) have shown great potential for graph data analysis. In this paper, we focus on multi-hop graph neural networks and aim to extend existing models to a high-order multi-hop form for graph-level representation learning. However, such a directly extending method suffers from two limitations, i.e., computational inefficiency and limited representation ability of the multi-hop neighbor. For the former limitation, we utilize an iteration approach to approximate the power of a complex adjacency matrix to achieve linear computational complexity. For the latter limitation, we introduce the Regularized Markov Clustering (R-MCL) to regularize the flow matrix, i.e., the adjacency matrix, in each iteration step. With these two strategies, we construct Markov Clustering Regularized Multi-hop Graph Neural Network (MCMGN) for graph-level representation learning tasks. Specifically, MCMGN consists of a multi-hop message passing phase and a readout phase, where the multi-hop message passing phase aims to learn multi-hop node embedding, and then the readout phase aggregates multi-hop node representations to generate graph embedding for graph-level representation learning tasks. Extensive experiments on eight graph benchmark datasets strongly demonstrate the effectiveness of Markov Clustering Regularized Multi-hop Graph Neural Network, leading to superior performance on graph classification.}
}
@article{WU2023109449,
title = {ECM-EFS: An ensemble feature selection based on enhanced co-association matrix},
journal = {Pattern Recognition},
volume = {139},
pages = {109449},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109449},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001498},
author = {Ting Wu and Yihang Hao and Bo Yang and Lizhi Peng},
keywords = {Ensemble feature selection, Machine learning, Feature kernel, Relative-co-association matrix (RCM)},
abstract = {Currently, feature selection faces a huge challenge that no single feature selection method can effectively deal with various data sets for all real cases. Ensemble learning is a potential promising solution to address this problem. We propose an ensemble feature selection method based on enhanced co-association matrix (ECM-EFS). Positive-co-association matrix (PCM), negative-co-association matrix (NCM), and relative-co-association matrix (RCM) are first introduced to discover the relationship among features by ensembling the results in multiple feature selection methods. To further produce a more stable feature selection result, “Feature Kernel” is also introduced and used as a starting point for feature selection. Comparative experiments with four state-of-the-art methods have confirmed that the ECM-EFS can provide more robust results. Moreover, compared with traditional ensemble feature selection methods, our method can compensate information loss and reduce computational cost significantly.}
}
@article{LIU2023109514,
title = {Bi-RRNet: Bi-level recurrent refinement network for camouflaged object detection},
journal = {Pattern Recognition},
volume = {139},
pages = {109514},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109514},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002145},
author = {Yan Liu and Kaihua Zhang and Yaqian Zhao and Hu Chen and Qingshan Liu},
keywords = {Camouflaged object detection, Convolutional neural networks, Recurrent refinement network, Dense prediction},
abstract = {In this paper, we present a lightweight Bi-level Recurrent Refinement Network (Bi-RRNet) for Camouflaged Object Detection (COD) that consists of a Lower-level RRNet (L-RRN) and an Up-level RRNet (U-RRN) to progressively refine the multi-level context features for precise dense prediction. In particular, the L-RRN recursively refines the deeper layer high-level semantic features with the high-resolution low-level features from the earlier layers in a top-down manner, and the U-RRN progressively polishes the refined features from the L-RRN in a recurrent manner, producing the high-resolution semantic features that are essential to accurate COD. Moreover, we develop a Multi-scale Scene Perception Module (MSPM) that, in order to deal with target appearance variation, first compresses the global scene context information at each layer into a learnable weight vector and then modulates the multi-scale context features produced by a filter bank with various local receptive fields using the learned weights. Meanwhile, we design a Region-Consistency Enhancement Module (RCEM) that makes use of high-level semantic features to direct filtering out the cluttered information in the lower-layer features. This module can highlight the regions of camouflaged objects, maximizing the inter-class contrast between the objects and their surroundings. Extensive experiments on four challenging benchmark datasets, including CHAMELEON, CAMO, COD10K, and NC4K, show that our Bi-RRNet outperforms a variety of state-of-the-art methods in terms of accuracy and model parameters. Our Bi-RRNet, in particular, is lightweight, with 14.95M parameters that are only half the size of the state-of-the-art BSA-Net.}
}
@article{HOU2023109477,
title = {Deep generative image priors for semantic face manipulation},
journal = {Pattern Recognition},
volume = {139},
pages = {109477},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109477},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001772},
author = {Xianxu Hou and Linlin Shen and Zhong Ming and Guoping Qiu},
keywords = {GANs, Face attribute prediction, Semantic face manipulation},
abstract = {Previous works on generative adversarial networks (GANs) mainly focus on how to synthesize high-fidelity images. In this paper, we present a framework to leverage the knowledge learned by GANs for semantic face manipulation. In particular, we propose to control the semantics of synthesized faces by adapting the latent codes with an attribute prediction model. Moreover, in order to achieve a more accurate estimation of different facial attributes, we propose to pretrain the attribute prediction model by inverting the synthesized face images back to the GAN latent space. As a result, our method explicitly considers the semantics encoded in the latent space of a pretrained GAN and is able to faithfully edit various attributes like eyeglasses, smiling, bald, age, mustache and gender for high-resolution face images. Extensive experiments show that our method has superior performance compared to state of the art for both face attribute prediction and semantic face manipulation.}
}
@article{ZHAO2023109445,
title = {A gradient optimization and manifold preserving based binary neural network for point cloud},
journal = {Pattern Recognition},
volume = {139},
pages = {109445},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109445},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001450},
author = {Zhi Zhao and Ke Xu and Yanxin Ma and Jianwei Wan},
keywords = {Binarization, Gradient optimization, Manifold, Point cloud},
abstract = {With significant progress of deep learning on 3D point cloud, the demand for deployment of point cloud neural network on the edge devices is growing. Binary neural network, a type of quantization compression method, with extreme low bit and fast inference speed, attracts more attention. It is more challenging, but has greater potentiality. Most of the researches on binary networks focus on images rather than point cloud. Considering the particularity of point cloud neural network, this paper presents a novel binarization framework, which includes two main contributions. Firstly, a gradient optimization method is proposed to overcome the shortcomings of Straight Through Estimator (STE) commonly used in the back propagation of binary network training. Secondly, based on the analysis of manifold distortion caused by the binary convolution and pooling operations, we propose an optimized scaling recovery method to restore manifold for the convoluted feature, and also, a pooling correction method to improve the pooled feature's fidelity. Manifold distortion leads to the severe feature homogeneity problem, which brings trouble in generating features with sufficient discrimination for classification and segmentation. The manifold preserving optimizations are designed to introduce minimum extra parameters to balance the accuracy with the computation and storage consumption. Experiments show that the proposed method outperforms state-of-the-art in accuracy with ignored overhead, and also has good scalability.}
}
@article{ZHENG2023109469,
title = {Deep embedded clustering with distribution consistency preservation for attributed networks},
journal = {Pattern Recognition},
volume = {139},
pages = {109469},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109469},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001693},
author = {Yimei Zheng and Caiyan Jia and Jian Yu and Xuanya Li},
keywords = {Deep embedded clustering, Autoencoder, Graph autoencoder, Node representation learning, Cluster distribution consistency},
abstract = {Many complex systems in the real world can be characterized as attributed networks. To mine the potential information in these networks, deep embedded clustering, which obtains node representations and clusters simultaneously, has been given much attention in recent years. Under the assumption of consistency for data in different views, the cluster structure of network topology and that of node attributes should be consistent for an attributed network. However, many existing methods ignore this property, even though they separately encode node representations from network topology and node attributes and cluster nodes on representation vectors learned from one of the views. Therefore, in this study, we propose an end-to-end deep embedded clustering model for attributed networks. It utilizes graph autoencoder and node attribute autoencoder to learn node representations and cluster assignments. In addition, a distribution consistency constraint is introduced to maintain the latent consistency of cluster distributions in two views. Extensive experiments on several datasets demonstrate that the proposed model achieves significantly better or competitive performance compared with the state-of-the-art methods. The source code can be found at https://github.com/Zhengymm/DCP.}
}
@article{DING2023109436,
title = {Multi-agent dueling Q-learning with mean field and value decomposition},
journal = {Pattern Recognition},
volume = {139},
pages = {109436},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109436},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001371},
author = {Shifei Ding and Wei Du and Ling Ding and Lili Guo and Jian Zhang and Bo An},
keywords = {Multi-agent, value decomposition, mixed cooperative-competitive task, mean filed},
abstract = {A great deal of multi agent reinforcement learning(MARL) work has investigated how multiple agents effectively accomplish cooperative tasks utilizing value function decomposition methods. However, existing value decomposition methods can only handle cooperative tasks with shared reward, due to these methods factorize the value function from a global perspective. To tackle the competitive tasks and mixed cooperative-competitive tasks with differing individual reward setting, we design the Multi-agent Dueling Q-learning (MDQ) method based on mean-filed theory and individual value decomposition. Specifically, we integrate the mean-field theory with the value decomposition to factorize the value function at the individual level, which can deal with mixed cooperative-competitive tasks. Besides, we take a dueling network architecture to distinguish which states are valuable, eliminating the need to learn the impact of each action on each state, therefore enabling efficient learning and leading to better policy evaluation. The proposed method MDQ is applicable not only to cooperative tasks with shared rewards setting, but also to mixed cooperative-competitive tasks with individualized reward settings. In this end, it is flexible and generically applicable enough to most multi-agent tasks. Empirical experiments on various mixed cooperative-competitive tasks demonstrate that MDQ significantly outperforms existing multi agent reinforcement learning methods.}
}
@article{DORNAIKA2023109481,
title = {Object-centric Contour-aware Data Augmentation Using Superpixels of Varying Granularity},
journal = {Pattern Recognition},
volume = {139},
pages = {109481},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109481},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001814},
author = {F. Dornaika and D. Sun and K. Hammoudi and J. Charafeddine and A. Cabani and C. Zhang},
keywords = {Data augmentation, Cutmix, Object-centric Contour-aware, Discriminative regions, Attention, Superpixels},
abstract = {Regional dropout strategies have demonstrated to be very effective in improving both the performance and the generalization capability of deep learning models. However, when such strategies are performed in a totally random manner, the background noise and label mismatch problems arise. To tackle such problems, existing approaches typically focus on regions with the highest distinctiveness. Yet, there are two main drawbacks of existing approaches: (I) Many existing region-based augmentation methods can only use rectangular regions, resulting in the loss of object contour information; (II) Deterministic selection of the most discriminative regions leads to poor diversification in data augmentation. In fact, a trade-off is needed between diversification and concentration, which can decrease the undesirable noise. In this paper, we propose a novel object-centric contour-aware CutMix data augmentation strategy with arbitrary- shape and size superpixel supports, which is hereafter referred to as OcCaMix for short. It not only captures the most discriminative regions, but also effectively preserves the contour details of the objects. Moreover, it enables the search of natural object parts of different sizes. Extensive experiments on a large number of benchmark datasets show that OcCaMix significantly outperforms state-of-the-art CutMix based data augmentation methods in classification tasks. The source codes and trained models are available at https://github.com/DanielaPlusPlus/OcCaMix.}
}
@article{ASIF2023109484,
title = {DeepActsNet: A deep ensemble framework combining features from face, hands, and body for action recognition},
journal = {Pattern Recognition},
volume = {139},
pages = {109484},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109484},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300184X},
author = {Umar Asif and Deval Mehta and Stefan {Von Cavallar} and Jianbin Tang and Stefan Harrer},
keywords = {Activity recognition, Convolutional neural networks, Deep learning},
abstract = {Human action recognition from videos has gained substantial focus due to its wide applications in the field of video understanding. Most of the existing approaches extract human skeleton data from videos to encode actions because of the invariance nature of the skeleton information with respect to lightning conditions and background changes. Despite their success in achieving high recognition accuracy, methods based on limited body joints fail to capture the nuances of subtle body parts which are highly relevant for discriminating similar actions. In this paper, we overcome this limitation by presenting a holistic framework for combining spatial and motion features from the body, face, and hands to develop a novel data representation termed “Deep Actions Stamps (DeepActs)” for video-based action recognition. Compared to the skeleton sequences based on limited body joints, DeepActs encode more effective spatio-temporal features that provide robustness against pose estimation noises and improve action recognition accuracy. We also present “DeepActsNet”, a deep learning based ensemble model which learns convolutional and structural features from Deep Action Stamps for highly accurate action recognition. Experiments on three challenging action recognition datasets (NTU60, NTU120, and SYSU) show that the proposed model produces significant improvements in the action recognition accuracy with less computational cost compared to the state-of-the-art methods.}
}
@article{2023109573,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {139},
pages = {109573},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(23)00273-X},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300273X}
}
@article{KONG2023109473,
title = {Flexible model weighting for one-dependence estimators based on point-wise independence analysis},
journal = {Pattern Recognition},
volume = {139},
pages = {109473},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109473},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001735},
author = {He Kong and Limin Wang},
keywords = {Point-wise independence analysis, Independence assumption, Point-wise log likelihood, Weighted one-dependence estimators},
abstract = {Recent studies have shown that Bayesian network classifiers (BNCs) are powerful tools for knowledge representation and classification, and averaged one-dependence estimators (AODE) is one of the most popular and effective BNCs since it can achieve the tradeoff between bias and variance due to its independence assumptions and ensemble learning strategy. However, unverified independence assumptions may result in biased estimates of probability distribution and then degradation in classification performance. In this paper, we prove theoretically the uncertainty of probability-theoretic independence and propose to measure the independence between attribute values implicated in specific instance. The estimates of conditional probability can be finely tuned based on point-wise independence analysis. Point-wise log likelihood function is then applied as weighting metric for committee members of AODE to improve the estimate of joint probability. Extensive experiments on 36 benchmark datasets show that, compared to other state-of-the-art classifiers, weighted one-dependence estimators using point-wise independence analysis can achieve competitive classification performance in terms of zero-one loss, RMSE, bias-variance decomposition and conditional log likelihood.}
}
@article{QIU2023109497,
title = {Weakly-supervised pre-training for 3D human pose estimation via perspective knowledge},
journal = {Pattern Recognition},
volume = {139},
pages = {109497},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109497},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001978},
author = {Zhongwei Qiu and Kai Qiu and Jianlong Fu and Dongmei Fu},
keywords = {Human pose estimation, Pre-training, Relative depth, Weakly-supervised},
abstract = {Modern deep learning-based 3D pose estimation approaches require plenty of 3D pose annotations. However, existing 3D datasets lack diversity, which limits the performance of current methods and their generalization ability. Although existing methods utilize 2D pose annotations to help 3D pose estimation, they mainly focus on extracting 2D structural constraints from 2D poses, ignoring the 3D information hidden in the images. In this paper, we propose a novel method to extract weak 3D information directly from 2D images without 3D pose supervision. Firstly, we utilize 2D pose annotations and perspective prior knowledge to generate the relative depth of human joints. Then, we collect a 2D pose dataset (MCPC) and generate relative depth labels. Based on MCPC, we propose a weakly-supervised pre-training (WSP) strategy to distinguish the depth relationship between two points in an image. WSP enables the learning of the relative depth of two keypoints on lots of in-the-wild images, which is more capable of predicting depth and generalization ability for 3D human pose estimation. After fine-tuning the pose model on 3D pose datasets, WSP achieves state-of-the-art results on two widely-used benchmarks.}
}
@article{AN2023109510,
title = {Patch loss: A generic multi-scale perceptual loss for single image super-resolution},
journal = {Pattern Recognition},
volume = {139},
pages = {109510},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109510},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002108},
author = {Tai An and Binjie Mao and Bin Xue and Chunlei Huo and Shiming Xiang and Chunhong Pan},
keywords = {Single-image super-resolution, Multi-scale loss functions, Image visual perception, Perceptual metrics},
abstract = {In single image super-resolution (SISR), although PSNR is a key metric for signal fidelity, images with high PSNR do not necessarily render high visual quality. As a result, current perception-driven SISR methods employ perceptual metrics close to the human eye to measure the quality of the generated images. Unfortunately, the perceptual loss and adversarial loss, widely used by the perception-driven SISR methods, still underperform on these non-differentiable perceptual metrics. To this end, we propose a generic multi-scale perceptual loss, i.e., the patch loss, which can be easily plugged into off-the-shelf SISR methods to improve a broad range of perceptual metrics. Specifically, the proposed patch loss minimizes the multi-scale similarity of image patches and enhances the restoration of regions with complex textures and sharp edges via parameter-free adaptive patch-wise attention. Our proposed patch loss introduces more realistic details compared to the perceptual loss and fewer artifacts compared to the adversarial loss.}
}
@article{WANG2023109465,
title = {Corrigendum to ‘Toward a Blind Image Quality Evaluator in the Wild by Learning beyond Human Opinion Scores’ Pattern Recognition. Volume 137 (2023) 109296},
journal = {Pattern Recognition},
volume = {139},
pages = {109465},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109465},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001656},
author = {Zhihua Wang and Zhi-Ri Tang and Jianguo Zhang and Yuming Fang}
}
@article{MOORTHY2023109457,
title = {Adaptive spatial-temporal surrounding-aware correlation filter tracking via ensemble learning},
journal = {Pattern Recognition},
volume = {139},
pages = {109457},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109457},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001577},
author = {Sathishkumar Moorthy and Young Hoon Joo},
keywords = {Surrounding information, Object tracking, Feature fusion, Correlation filters framework, Selective spatial regularization},
abstract = {With the advancement of computer vision, object trackers based on discriminative correlation filters (DCF) have demonstrated superior performance and accuracy compared to traditional trackers. However, most existing DCF-based trackers are easily affected by various factors, such as cluttered background, illumination variations, occlusions, rotations etc. Therefore, in order to accurately track the target, further investigation into the characteristics of the correlation filter is required. In this study, we propose an adaptive spatial-temporal surrounding-aware correlation filter tracker via the ensemble learning (ASTSAELT) technique. Specifically, the adaptive spatial-temporal regularized correlation filter to remove the boundary effects and temporal degradation is presented. And then, a method of extracting surrounding samples according to the size and shape of the tracking object, designed to preserve the integrity of the object, is proposed. Moreover, our tracker utilizes a multi-expert tracking framework to improve its performance by integrating both handcrafted features and deep convolutional layer features. And then, the update strategy is proposed to measure the reliability of the current tracking result and mitigate model corruption. Finally, numerous experiments on visual tracking benchmarks including OTB2013, OTB2015, TempleColor128, UAV123, UAVDT and DTB70 are implemented to verify the developed method achieves superior performance compared with several state-of-the-art methods.}
}
@article{WANG2023109441,
title = {Dynamic dual graph networks for textbook question answering},
journal = {Pattern Recognition},
volume = {139},
pages = {109441},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109441},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000961},
author = {Yaxian Wang and Jun Liu and Jie Ma and Hongwei Zeng and Lingling Zhang and Junjun Li},
keywords = {Textbook question answering, Multi-modal machine comprehension, Diagram understanding},
abstract = {Textbook Question Answering (TQA) is a complex task oriented to multi-modal context, which requires reasoning on a diagram and a long essay to get the correct answer. There are mainly two related issues for the task. First, diagrams are mostly abstract expressions of real world and some constituents with similar appearance may have different semantics, which makes it difficult to understand them effectively. Secondly, a long essay contains abundant and useful information for question answering, which shows that it is vital to extract the relevant information from the abundant text and then perform reasoning on it. To address the two issues, we propose a new model, Dynamic Dual Graph Networks (DDGNet), which performs question-guided multi-step reasoning on the dynamic directed Diagram Graph Network (DGN) for the diagram and Textual Graph Network (TGN) for the most related paragraph extracted from a long essay. Specifically, DGN combines text features with positional features of text boxes in the diagram as the node feature to avoid the ambiguity of visual features for the abstract constituents and help express explicit semantics. TGN uses the representation of each sentence in the most related paragraph as the node feature to learn the contextualized interaction of the useful information in the graph reasoning process. For the reasoning strategy, we propose a question-guided multi-step graph reasoning method to update both DGN and TGN dynamically under the question guidance in every step. Experimental results show that our proposed model outperforms the baselines on the TQA dataset. Moreover, extensive ablation studies are also conducted to analyze the effectiveness of our proposed model.}
}
@article{DISSANAYAKE2023109440,
title = {Multi-stage stacked temporal convolution neural networks (MS-S-TCNs) for biosignal segmentation and anomaly localization},
journal = {Pattern Recognition},
volume = {139},
pages = {109440},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109440},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001413},
author = {Theekshana Dissanayake and Tharindu Fernando and Simon Denman and Sridha Sridharan and Clinton Fookes},
keywords = {Deep learning, Electrocardiogram, Heart sounds, Lung sounds, Segmentation, Model interpretation},
abstract = {In the computer vision domain, temporal convolution networks (TCN) have gained traction due to their lightweight, robust architectures for sequence-to-sequence prediction tasks. With that insight, in this study, we propose a novel deep learning architecture for biosignal segmentation and anomaly localization based on TCNs, named the multi-stage stacked TCN, which employs multiple TCN modules with varying dilation factors. More precisely, for each stage, our architecture uses TCN modules with multiple dilation factors, and we use convolution-based fusion to combine predictions returned from each stage. Furthermore, aiming smoothed predictions, we introduce a novel loss function based on the first-order derivative. To demonstrate the robustness of our architecture, we evaluate our model on five different tasks related to three 1D biosignal modalities (heart sounds, lung sounds and electrocardiogram). Our proposed framework achieves state-of-the-art performance for all tasks, significantly outperforming the respective state-of-the-art models having F1 score gains up to ≈9%. Furthermore, the framework demonstrates competitive performance gains compared to traditional multi-stage TCN models with similar configurations yielding F1 score gains up to ≈5%. Our model is also interpretable. Using neural conductance, we demonstrate the effectiveness of having TCNs with varying dilation factors. Our visualizations show that the model benefits from feature maps captured at multiple dilation factors, and the information is effectively propagated through the network such that the final stage produces the most accurate result.}
}
@article{MORALES2023109367,
title = {BabyNet: Reconstructing 3D faces of babies from uncalibrated photographs},
journal = {Pattern Recognition},
volume = {139},
pages = {109367},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109367},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323000687},
author = {Araceli Morales and Antonia Alomar and Antonio R. Porras and Marius George Linguraru and Gemma Piella and Federico M. Sukno},
keywords = {3D face reconstruction, Graph neural network, Baby model},
abstract = {We present a 3D face reconstruction system that aims at recovering the 3D facial geometry of babies from uncalibrated photographs, BabyNet. Since the 3D facial geometry of babies differs substantially from that of adults, baby-specific facial reconstruction systems are needed. BabyNet consists of two stages: 1) a 3D graph convolutional autoencoder learns a latent space of the baby 3D facial shape; and 2) a 2D encoder that maps photographs to the 3D latent space based on representative features extracted using transfer learning. In this way, using the pre-trained 3D decoder, we can recover a 3D face from 2D images. We evaluate BabyNet and show that 1) methods based on adult datasets cannot model the 3D facial geometry of babies, which proves the need for a baby-specific method, and 2) BabyNet outperforms classical model-fitting methods even when a baby-specific 3D morphable model, such as BabyFM, is used.}
}
@article{SORIA2023109461,
title = {Dense extreme inception network for edge detection},
journal = {Pattern Recognition},
volume = {139},
pages = {109461},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109461},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001619},
author = {Xavier Soria and Angel Sappa and Patricio Humanante and Arash Akbarinia},
keywords = {Edge detection, Deep learning, CNN, Contour detection, Boundary detection, Segmentation},
abstract = {Edge detection is the basis of many computer vision applications. State of the art predominantly relies on deep learning with two decisive factors: dataset content and network architecture. Most of the publicly available datasets are not curated for edge detection tasks. Here, we address this limitation. First, we argue that edges, contours and boundaries, despite their overlaps, are three distinct visual features requiring separate benchmark datasets. To this end, we present a new dataset of edges. Second, we propose a novel architecture, termed Dense Extreme Inception Network for Edge Detection (DexiNed), that can be trained from scratch without any pre-trained weights. DexiNed outperforms other algorithms in the presented dataset. It also generalizes well to other datasets without any fine-tuning. The higher quality of DexiNed is also perceptually evident thanks to the sharper and finer edges it outputs.}
}
@article{WEI2023109503,
title = {A Composite Network Model for Face Super-Resolution with Multi-Order Head Attention Facial Priors},
journal = {Pattern Recognition},
volume = {139},
pages = {109503},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109503},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002030},
author = {Feng Wei and Song Wang and Jucheng Yang and Xiao Sun and Yuan Wang and Yarui Chen},
keywords = {Face super-resolution, FSR, Multi-order head attention, Facial components, Prior information, Transformer},
abstract = {Face super-resolution (FSR) aims to reconstruct high-resolution face images from low-resolution (LR) ones. Despite the progress made by deep convolutional neural networks (DCNNs) on FSR, convolutions struggle to relate spatially distant concepts and what is more, all image pixels and prior information (e.g., landmarks and facial component heatmaps) are treated equally regardless of importance, causing inaccuracy and decreasing the quality of face image recovery. To address these issues, in this paper we propose a composite network model for FSR with multi-order head attention facial priors. The proposed model contains a face hallucination transformer (FHT)-based network and a multi-order head attention (MOHA)-based DCNN. The FHT-based network can capture long-range dependencies and gradually increase resolution to achieve efficient and effective inference, while the MOHA-based DCNN exploits detailed and two-dimensional information of LR face images. Moreover, the novel generic submodule of the MOHA-based DCNN, namely Multi-Order Head Attention Network, can accurately model the relationship of facial components between spatial and channel dimensions. The proposed composite network model seamlessly integrates the advantages of DCNNs and transformers to super-resolve LR face images. When compared with state-of-the-art FSR methods on public benchmark datasets, the proposed model shows competitive recovery performance.}
}
@article{YU2023109478,
title = {Fast support vector machine training via three-term conjugate-like SMO algorithm},
journal = {Pattern Recognition},
volume = {139},
pages = {109478},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109478},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001784},
author = {Lang Yu and Shengjie Li and Siyi Liu},
keywords = {Support vector machine, Sequential minimal optimization, Three-term conjugate direction},
abstract = {Support vector machine (SVM) is an important class of methods in pattern recognition, and the sequential minimal optimization (SMO) algorithm is one of the most popular methods for training SVM at present. Based on the conjugate sequential minimal optimization algorithm (CSMO), we propose a novel three-term conjugate-like sequential minimal optimization algorithm (TCSMO) for classification and regression tasks. Compared with the CSMO, although the three-term conjugate-like SMO slightly increases the amount of arithmetic operations in each iteration, it significantly reduces the number of iterations required to converge to the specified accuracy and shortens the training time of the SVM. Additionally, we give a convergence proof of the three-term conjugate-like SMO algorithm and four new conjugate parameters. Numerical experiments show that the three-term conjugate-like SMO algorithm performs better numerically in both classification and regression tasks.}
}
@article{HU2023109404,
title = {An Effective and Adaptable K-means Algorithm for Big Data Cluster Analysis},
journal = {Pattern Recognition},
volume = {139},
pages = {109404},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109404},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300105X},
author = {Haize Hu and Jianxun Liu and Xiangping Zhang and Mengge Fang},
keywords = {-means algorithm, Local optimization, Lévy flight, Global search, Clustering centroids},
abstract = {Tradition K-means clustering algorithm is easy to fall into local optimum, poor clustering effect on large capacity data and uneven distribution of clustering centroids. To solve these problems, a novel k-means clustering algorithm based on Lévy flight trajectory (Lk-means) is proposed in the paper. In the iterative process of LK-means algorithm, Lévy flight is used to search new positions to avoid premature convergence in clustering. It is also applied to increase the diversity of the cluster, strengthen the global search ability of K-means algorithm, and avoid falling into the local optimal value too early. Nevertheless, the complexity of hybrid algorithm is not increased in the process of Lévy flight optimization. To verify the data clustering effect of LK-means algorithm, experiments are conducted to compare it with the k-means algorithm, XK-means algorithm, DDKmeans algorithm and Canopyk-means algorithm on 10 open source data sets. The results show that LK-means algorithm has better search results and more evenly distributed cluster centroids, which greatly improves the global search ability, big data processing ability and uneven distribution centroids of cluster of K-means algorithm.}
}
@article{ZHANG2023109462,
title = {Deep collaborative graph hashing for discriminative image retrieval},
journal = {Pattern Recognition},
volume = {139},
pages = {109462},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109462},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001620},
author = {Zheng Zhang and Jianning Wang and Lei Zhu and Yadan Luo and Guangming Lu},
keywords = {Deep hashing, Image retrieval, Collaborative learning, Graph convolutional hashing, Semantic encoding},
abstract = {The most striking success of deep hashing for large-scale image retrieval benefits from its powerful discriminative representation of deep learning and the attractive computational efficiency of compact hash code learning. Most existing deep semantic-preserving hashing regard the available semantic labels as the ground truth for classification or transform them into prevalent pairwise similarities. However, such strategies fail to capture the interactive correlations between the visual semantics embedded in images and the given category-level labels. Moreover, they utilize the fixed piecewise or pairwise semantics as the optimization objectives, which suffers from the limited flexibility on semantic representation and adaptive knowledge communication in hash code learning. In this paper, we propose a novel Deep Collaborative Graph Hashing (DCGH), which collectively considers multi-level semantic embeddings, latent common space construction, and intrinsic structure mining in discriminative hash codes learning, for large-scale image retrieval. To the best of our knowledge, this is the first collaborative graph hashing for image retrieval. Specifically, instead of using the conventional single-flow visual network architecture, we design a dual-stream feature encoding network to jointly explore the multi-level semantic information across visual and semantic features. Moreover, a well-established shared latent space is constructed based on space reconstruction to explore the concurrent information and bridge the semantic gap between visual and semantic space. Furthermore, a graph convolutional network is introduced to preserve the latent structural relations in the optimal pairwise similarity-preserving hash codes. The whole learning framework is optimized in an end-to-end fashion. Extensive experiments on different datasets demonstrate that our DCGH can achieve superb image retrieval performance against state-of-the-art supervised hashing methods. The source codes of the proposed DCGH are available at https://github.com/JalinWang/DCGH .}
}
@article{CHANG2023109515,
title = {Elaborate multi-task subspace learning with discrete group constraint},
journal = {Pattern Recognition},
volume = {139},
pages = {109515},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109515},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002157},
author = {Wei Chang and Feiping Nie and Rong Wang and Xuelong Li},
keywords = {Multi-task learning, Negative transfer, Subspace learning, Re-weighted method},
abstract = {In multi-task learning (MTL), multiple related tasks can be learned simultaneously under the shared information to improve the generalization performance. However, most of MTL methods assume that all the learning tasks are related indeed and appropriate for joint learning. In some real situations, this assumption may not hold and further lead to the problem of negative transfer. Therefore, in this paper, we not only focus on researching the problem of robustly learning the common feature structure shared by tasks, but also discriminate with which tasks one task should share. By combining with the idea of subspace learning, we propose an elaborate multi-task subspace learning model (EMTSL) with discrete group structure constraint, which can cluster the learned tasks into a set of groups. By introducing the Schatten p-norm instead of trace norm, our model EMTSL can better approximate the low-rank constraint and also avoid the trivial solution. Furthermore, we design an efficient algorithm based on the re-weighted method to solve the proposed model. In addition, the convergence analysis of our algorithm is given in this paper. Experimental results on both synthetic and real-world datasets demonstrate the superiority of our method.}
}
@article{REZAEI2023109454,
title = {K-sets and k-swaps algorithms for clustering sets},
journal = {Pattern Recognition},
volume = {139},
pages = {109454},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109454},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001541},
author = {Mohammad Rezaei and Pasi Fränti},
keywords = {Clustering sets, Similarity of sets, -means, -medoids, Random swap, K-swaps, Customer segmentation, Clustering healthcare records},
abstract = {We present two new clustering algorithms called k-sets and k-swaps for data where each object is a set. First, we define the mean of the sets in a cluster, and the distance between a set and the mean. We then derive the k-sets algorithm from the principles of classical k-means so that it repeats the assignment and update steps until convergence. To the best of our knowledge, the proposed algorithm is the first k-means based algorithm for this kind of data. We adopt the idea also into random swap algorithm, which is a wrapper around the k-means that avoids local minima. This variant is called k-swaps. We show by experiments that this algorithm provides more accurate clustering results than k-medoids and other competitive methods.}
}
@article{ZHANG2023109507,
title = {Doubly contrastive representation learning for federated image recognition},
journal = {Pattern Recognition},
volume = {139},
pages = {109507},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109507},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002078},
author = {Yupei Zhang and Yunan Xu and Shuangshuang Wei and Yifei Wang and Yuxin Li and Xuequn Shang},
keywords = {Image recognition, Doubly contrastive learning, Federated machine learning, Representation learning, Non-IID data classification},
abstract = {This paper focuses on the problem of personalized federated learning (FL) with the schema of contrastive learning (CL), which is to implement collaborative pattern classification by many clients. The traditional FL frameworks mostly facilitate the global model for the server and the local models for the clients to be similar, often ignoring the data heterogeneity of the clients. Aiming at achieving better performance in clients, this study introduces a personalized federated contrastive learning model, dubbed PerFCL, by proposing a new approach to doubly contrastive representation learning (DCL). Concretely, PerFCL borrows the DCL scheme, where one CL loss compares the shared parts of local models with the global model and the other CL loss compares the personalized parts of local models with the global model. To encourage the difference between the two parts, we created a double optimization problem composed of maximizing the comparison agreement for the former and minimizing the comparison agreement for the latter. We evaluated the proposed model on three publicly available data sets for federated image classification. Experiment results show that PerFCL benefits from the proposed DCL strategy and performs better than the state-of-the-art federated-learning models.}
}
@article{YAN2023109446,
title = {A bi-level metric learning framework via self-paced learning weighting},
journal = {Pattern Recognition},
volume = {139},
pages = {109446},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109446},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001462},
author = {Jing Yan and Wei Wei and Xinyao Guo and Chuangyin Dang and Jiye Liang},
keywords = {Metric learning, Self-paced learning, Adaptive neighborhood, Weighting tuples},
abstract = {Distance metric learning (DML) has achieved great success in many real-world applications. However, most existing DML models characterize the quality of tuples on the tuple level while ignoring the anchor level. Therefore, the models are less accurate to portray the quality of tuples and tend to be over-fitting when anchors are noisy samples. In this paper, we devise a bi-level metric learning framework (BMLF), which characterizes the quality of tuples more finely on both levels, enhancing the generalization performance of the DML model. Furthermore, we present an implementation of BMLF based on a self-paced learning regular term and design the corresponding optimization algorithm. By weighing tuples on the anchor level and training the model using tuples with higher weights preferentially, the side effect of low-quality noisy samples will be alleviated. We empirically demonstrate that the effectiveness and robustness of the proposed method outperform the state-of-the-art methods on several benchmark datasets.}
}
@article{WEI2023109437,
title = {Exploring the diversity and invariance in yourself for visual pre-training task},
journal = {Pattern Recognition},
volume = {139},
pages = {109437},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109437},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001383},
author = {Longhui Wei and Lingxi Xie and Wengang Zhou and Houqiang Li and Qi Tian},
keywords = {Visual pre-training, Self-supervised learning, Multi-grained visual information},
abstract = {Recently, self-supervised learning methods have achieved remarkable success in the visual pre-training task. By simply pulling the different augmented views of each image together or other novel mechanisms, they can learn much unsupervised knowledge and significantly improve the transfer performance of pre-training models. However, these works still cannot avoid the representation collapse problem, i.e., they only focus on limited regions or the extracted features on totally different regions inside each image are nearly the same. Generally, this problem makes the pre-training models cannot sufficiently describe the multi-grained information inside images, which further limits the upper bound of their transfer performance. To alleviate this issue, this paper introduces a simple but effective mechanism, called Exploring the Diversity and Invariance in Yourself (E-DIY). By simply pushing the most different regions inside each augmented view away, E-DIY can preserve the diversity of extracted region-level features. By pulling the most similar regions from different augmented views of the same image together, E-DIY can ensure the robustness of extracted region-level features. Benefiting from the above diversity and invariance exploring mechanism, E-DIY better extracts the multi-grained visual information inside each image compared with previous self-supervised learning approaches. Extensive experiments on various downstream tasks have demonstrated the superiority of our method, e.g., there is 1.9% improvement (compared with the recent state-of-the-art method, BYOL) on AP50 metric of detection task on VOC.}
}
@article{GHALYAN2023109482,
title = {Capacitive empirical risk function-based bag-of-words and pattern classification processes},
journal = {Pattern Recognition},
volume = {139},
pages = {109482},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109482},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001826},
author = {Ibrahim F. Ghalyan},
keywords = {Bag-of-words, Classification, Fuzzy measure, Generalization, Statistical modeling},
abstract = {This paper proposes capacitive bag-of-words (Cap BoW) modeling and capacitive pattern classification processes for improved generalization ability. The Cap BoW and capacitive pattern classification are realized by introducing the notion of the capacitive empirical risk function (CERF). The CERF is used as a cost function to build a capacitive pattern classification process. The resulted capacitive pattern classification is used in the classification stage of the bag-of-words process, realizing thereby the Cap BoW process. The use of the CERF in building the capacitive pattern classification and Cap BoW processes is demonstrated to achieve simultaneous reduction to the empirical risk function (ERF) and confidence interval, reducing potential overfitting and enhancing the generalization ability of considered models. To have a thorough evaluation, two groups of experiments are carried out. The first group addresses the capacitive pattern classification process for 4 datasets. The second group addresses a more holistic impact of the CERF by using Cap Bow model to build capacitive dual ergodicity limits-based bag-of-words (Cap DEL-BoW) process, which is applied to 5 image datasets. In both groups of experiments, remarkable enhancement is demonstrated with the use of CERF-based pattern classification and Cap BoW processes. Comparison with corresponding conventional non-capacitive models demonstrates the tangible enhancement with the use of CERF-based models.}
}
@article{YIN2023109450,
title = {Discriminative subspace learning via optimization on Riemannian manifold},
journal = {Pattern Recognition},
volume = {139},
pages = {109450},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109450},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001504},
author = {Wanguang Yin and Zhengming Ma and Quanying Liu},
keywords = {Discriminative subspace learning, Riemannian manifold optimization, Dimensionality reduction, Classification},
abstract = {Discriminative subspace learning is an important problem in machine learning, which aims to find the maximum separable decision subspace. Traditional Euclidean-based methods usually use Fisher discriminant criterion for finding an optimal linear mapping from a high-dimensional data space to a lower-dimensional subspace, which hardly guarantee a quadratic rate of global convergence and suffers from the singularity problem. Here, we propose the manifold optimization-based discriminant analysis (MODA) which is constructed by using the latent subspace alignment and the geometry of objective function with orthogonality constraint. MODA is solved by using Riemannian version of trust-region algorithm. Experimental results on various image datasets and electroencephalogram (EEG) datasets show that MODA achieves the best separability and is significantly superior to the competing algorithms. Especially for the time series of EEG signals, the accuracy of MODA is 20–30% higher than existing algorithms. The code for MODA is available at https://github.com/ncclabsustech/MODA-algorithm.}
}
@article{XU2023109474,
title = {Fourier-based augmentation with applications to domain generalization},
journal = {Pattern Recognition},
volume = {139},
pages = {109474},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109474},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001747},
author = {Qinwei Xu and Ruipeng Zhang and Ziqing Fan and Yanfeng Wang and Yi-Yan Wu and Ya Zhang},
keywords = {Domain shift, Domain generalization, Fourier-based augmentation, Consistency training},
abstract = {When deployed on a new domain different from the training set, deep learning often suffers from severe performance degradation. To combat domain shift, domain adaptation and domain generalization are proposed, where the former aims at transferring knowledge from related source domains to a known target domain, while the latter is more challenging by requiring the model to generalize to unknown target domains. This paper focuses on domain generalization and introduce a novel Fourier-based perspective for it. The main idea comes from the fact that Fourier amplitude component contains low-level statistics while phase component preserves high-level semantics. We thus propose a novel Fourier-based data augmentation strategy called AmpMix by linearly interpolating the amplitudes of two images while keeping their phases unchanged, to highlight the generalizable semantics contained in phase. To make full use of Fourier-augmented samples, we further incorporate consistency training between different augmentation views and devise a Fourier-based framework for three different domain generalization settings. Extensive experiments demonstrate the effectiveness of our Fourier-based method.}
}
@article{GUPTA2023109453,
title = {A survey of human-computer interaction (HCI) & natural habits-based behavioural biometric modalities for user recognition schemes},
journal = {Pattern Recognition},
volume = {139},
pages = {109453},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109453},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300153X},
author = {Sandeep Gupta and Carsten Maple and Bruno Crispo and Kiran Raja and Artsiom Yautsiukhin and Fabio Martinelli},
keywords = {Internet of Things (IoT), User recognition, Behavioural biometrics, Secutity, Privacy, Usability},
abstract = {The proliferation of Internet of Things (IoT) systems is having a profound impact across all aspects of life. Recognising and identifying particular users is central to delivering the personalised experience that citizens want to experience, and that organisations wish to deliver. This article presents a survey of human-computer interaction-based (HCI-based) and natural habits-based behavioural biometrics that can be acquired unobtrusively through smart devices or IoT sensors for user recognition purposes. Robust and usable user recognition is also a security requirement for emerging IoT ecosystems to protect them from adversaries. Typically, it can be specified as a fundamental building block for most types of human-to-things accountability principles and access-control methods. However, end-users are facing numerous security and usability challenges in using currently available knowledge- and token-based recognition (i.e., authentication and identification) schemes. To address the limitations of conventional recognition schemes, biometrics, naturally come as a first choice to supporting sophisticated user recognition solutions. We perform a comprehensive review of touch-stroke, swipe, touch signature, hand-movements, voice, gait and footstep behavioural biometrics modalities. This survey analyzes the recent state-of-the-art research of these behavioural biometrics with a goal to identify their attributes and features for generating unique identification signatures. Finally, we present security, privacy, and usability evaluations that can strengthen the designing of robust and usable user recognition schemes for IoT applications.}
}
@article{WEI2023109466,
title = {Adversarial pan-sharpening attacks for object detection in remote sensing},
journal = {Pattern Recognition},
volume = {139},
pages = {109466},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109466},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001668},
author = {Xingxing Wei and Maoxun Yuan},
keywords = {Adversarial pan-sharpening, Remote sensing, Object detection},
abstract = {Pan-sharpening, as one of the most commonly used techniques in remote sensing systems, aims to fuse texture-rich PAN images and multi-spectral MS images to obtain texture-rich MS images. With the development of deep learning, CNN based Pan-sharpening methods have received more and more attention in recent years. Since Pan-sharpening technique can integrate the complementary information of Pan and MS images, researchers usually apply object detectors on these pan-sharpened images to achieve reliable detection results. However, recent studies have shown that Deep Learning-based object detection methods are vulnerable to adversarial examples, i.e., adding imperceptible noise to clean images can fool well-trained deep neural networks. It is interesting to combine the pan-sharpening technique with adversarial examples to attack object detectors in remote sensing. In this paper, we propose a framework to generate adversarial pan-sharpened images. Specifically, we propose a two-stream network to generate the pan-sharpened images, and then utilize the shape loss and label loss to perform the attack task. To guarantee the quality of pan-sharpened images, a perceptual loss is utilized to balance spectral preservation and attacking performance. Experimental results demonstrate that the proposed method can generate effective adversarial pan-sharpened images that maintain a high success rate for white-box attacks and achieve transferability for black-box attacks.}
}
@article{ZHANG2023109486,
title = {Dynamic graph convolutional networks by semi-supervised contrastive learning},
journal = {Pattern Recognition},
volume = {139},
pages = {109486},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109486},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001863},
author = {Guolin Zhang and Zehui Hu and Guoqiu Wen and Junbo Ma and Xiaofeng Zhu},
keywords = {Topology, Dynamic feature graph, Semi-supervised contrastive learning},
abstract = {The traditional graph convolutional network(GCN) and its variants usually only propagate node information through the topology given by the dataset. However, the given topology can only represent a certain relationship and ignore some correlative feature information between nodes, which may make the graph convolutional networks unable to fully utilize the data information. To address the above issue, a novel model named Dynamic Graph Convolutional Networks by Semi-Supervised Contrastive Learning (DGSCL) is proposed in this paper. First, a feature graph is dynamically constructed from the input node features to exploit the potential correlative feature information between nodes. Then, to ensure a high-quality feature graph, a semi-supervised contrastive learning method is designed to learn discriminative node embeddings, which can iteratively refine the constructed feature graph with the learned node embeddings. Finally, we fuse the node embeddings obtained from the given topology and the dynamic feature graph by two co-attention modules to produce more informative embeddings for the classification task. Through a series of experiments, we demonstrate the competitive performance of our model on seven node classification benchmarks.}
}
@article{NING2023109433,
title = {Corrigendum to’ HCFNN: High-order coverage function neural network for image classification’ Pattern Recognition. Volume 131(2022) 108873},
journal = {Pattern Recognition},
volume = {139},
pages = {109433},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109433},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001346},
author = {Xin Ning and Weijuan Tian and Zaiyang Yu and Weijun Li and Xiao Bai and Yuebao Wang}
}
@article{XIAO2023109458,
title = {Where you edit is what you get: Text-guided image editing with region-based attention},
journal = {Pattern Recognition},
volume = {139},
pages = {109458},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109458},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001589},
author = {Changming Xiao and Qi Yang and Xiaoqiang Xu and Jianwei Zhang and Feng Zhou and Changshui Zhang},
keywords = {Generative adversarial networks, Text-guided image editing, Spatial disentanglement},
abstract = {Leveraging the abundant knowledge learned from pre-trained multi-modal models like CLIP has recently proved to be effective for text-guided image editing. Though convincing results have been made when combining the image generator StyleGAN with CLIP, most methods need to train separate models for different prompts, and irrelevant regions are often changed after editing due to the lack of spatial disentanglement. We propose a novel framework that can edit different images according to different prompts in one model. Besides, an innovative region-based spatial attention mechanism is adopted to explicitly guarantee the locality of editing. Experiments mainly in the face domain verify the feasibility of our framework and show that when multi-text editing and local editing are accomplishable, our method can complete practical applications like sequential editing and regional style transfer.}
}
@article{ZHA2023109459,
title = {Conditional invertible image re-scaling},
journal = {Pattern Recognition},
volume = {139},
pages = {109459},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109459},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001590},
author = {Yufei Zha and Fan Li and Peng Zhang and Wei Huang},
keywords = {Invertible neural network, Image re-scaling, Information theory},
abstract = {One of the key issue for image re-scaling is modeling the down-sampling loss. The loss can be approached by a prior distribution through an invertible network in recent works. However, this assumption of independence between images and down-sampling losses is not always satisfied in practice. To address the above problem, we design a module to learn a latent variable representing the down-scaling loss conditional on low-resolution(LR) images according to the information entropy theory. Unlike previous methods, the down-scaling loss is recovered through the proposed module whose inputs are both LR images and predefined distributions. The difference between the high-resolution(HR) image and the low-resolution(LR) image is represented with a latent variable through a lossless invertible neural network. In addition, a conditional entropy loss is proposed to train the invertible neural network to suppress the conditional entropy between LR images and latent variables. It helps to decrease the effect of the latent variable to generate the HR image from LR images during the up-scaling procedure. We evaluate the proposed method on widely used data sets, and the experimental results demonstrate that our proposed method performs favorably against SOTA methods in terms of PSNR and SSIM metrics.}
}
@article{ZHAO2023109406,
title = {Density peaks clustering algorithm based on fuzzy and weighted shared neighbor for uneven density datasets},
journal = {Pattern Recognition},
volume = {139},
pages = {109406},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109406},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001073},
author = {Jia Zhao and Gang Wang and Jeng-Shyang Pan and Tanghuai Fan and Ivan Lee},
keywords = {Uneven density data, Density peaks clustering, Fuzzy neighborhood, K-nearest neighbor, Weighted shared neighbor},
abstract = {Uneven density data refers to data with a certain difference in sample density between clusters. The local density of density peaks clustering algorithm (DPC) does not consider the effect of sample density difference between clusters of uneven density data, which may lead to wrong selection of cluster centers; the algorithm allocation strategy makes it easy to incorrectly allocate samples originally belonging to sparse clusters to dense clusters, which reduces clustering efficiency. In this study, we proposed the density peaks clustering algorithm based on fuzzy and weighted shared neighbor for uneven density datasets (DPC-FWSN). First, a nearest neighbor fuzzy kernel function is obtained by combining K-nearest neighbor and fuzzy neighborhood. Then, local density is redefined by the nearest neighbor fuzzy kernel function. The local density can better characterize the distribution characteristics of the sample by balancing the contribution of sample density in dense and sparse areas, in order to avoid the situation that the sparse cluster does not have a cluster center. Finally, the allocation strategy for weighted shared neighbor similarity is proposed to optimize the sample allocation at the boundary of the sparse cluster. Experiments are performed on IDPC-FA, FKNN-DPC, FNDPC, DPCSA and DPC for uneven density datasets, complex morphologies datasets and real datasets. The clustering results demonstrate that DPC-FWSN effectively handles datasets with uneven density distribution.}
}
@article{GEDAMU2023109455,
title = {Relation-mining self-attention network for skeleton-based human action recognition},
journal = {Pattern Recognition},
volume = {139},
pages = {109455},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109455},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001553},
author = {Kumie Gedamu and Yanli Ji and LingLing Gao and Yang Yang and Heng Tao Shen},
keywords = {Action recognition, Relation-mining self-attention, Pairwise self-attention, Unary self-attention, Position attention},
abstract = {Modeling spatiotemporal global dependencies and dynamics of body joints are crucial to recognizing actions from 3D skeleton sequences. We propose a Relation-mining Self-Attention Network (RSA-Net) for skeleton-based human action recognition. The proposed RSA-Net is motivated by two important observations: (1) body joint relationships can be modeled independently as pairwise and unary to reduce the difficulty of action feature learning. (2) Computing action semantics and position information independently removes noisy correlations over heterogeneous embedding. The proposed RSA-Net contains pairwise self-attention, unary self-attention, and position embedding attention modules. The pairwise self-attention captures the relationship between every two body joints. The unary self-attention learns a general correlation features among one key joint over all other query joints. The position embedding attention module computes the correlation between action semantics and position information independently with separate projection matrices. Extensive evaluations are performed in the NTU-60, NTU-120, and UESTC datasets with CS, CV, CSet, and A-view evaluation benchmarks. The proposed RSA-Net outperforms existing transformer-based approaches and comparable results with state-of-the-art graph ConvNet methods. The source code is available in Github11https://github.com/GedamuA/RSA-Net..}
}
@article{WANG2023109426,
title = {Complementary adversarial mechanisms for weakly-supervised temporal action localization},
journal = {Pattern Recognition},
volume = {139},
pages = {109426},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109426},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001279},
author = {Chuanxu Wang and Jing Wang and Peng Liu},
keywords = {Temporal action localization, Boundary regression, Complementary adversarial mechanism, Weakly supervision learning, Action recognition},
abstract = {Weakly supervised Temporal Action Localization (WTAL) aims to locate the start and end boundaries of action instances and recognize their corresponding categories. Classical methods mainly rely on random erasure mechanisms, attention mechanisms, or imposing loss constraints. Despite their great progress, there are still two challenges of incomplete positioning and context confusion. Therefore, we propose a framework with complementary adversarial mechanisms to address these issues. In the adversarial learning stage, for an input snippet, we roughly determine its proper duration, by matching it with the specified multi-scaled anchors based on CAS score loss; then, it undergoes a frame-level iterative regression to precisely figure out its boundary, which can reject none closely related frames merged in, and ensures no overlapping between different action proposals. Subsequently, the GCN module explicitly enhances the feature representation for this fine localized snippet, aiming to strengthen the exclusiveness between different action snippets. Afterward, our complementary learning module calculates the similarity between the original input video Vg and the video Vr reconstructed with the above localization refined snippets, aiming to ensure no closely relevant frames missing, this checking mechanism works as feedback to guide the regression module for more accurate localization regression. Finally, each refined snippet undergoes multi-instance learning to obtain its classification score, and the top-k strategy is used to aggregate temporally adjacent snippets based on their content similarity, which can avoid fragmentation of an action proposal. This method is tested on datasets of THUMOS14 and ActivityNet1.2, and their average accuracy is 64.68% and 32.94% respectively, its comparisons with other articles prove its effectiveness.}
}
@article{BORAL2023109447,
title = {MEQA: Manifold embedding quality assessment via anisotropic scaling and Kolmogorov-Smirnov test},
journal = {Pattern Recognition},
volume = {139},
pages = {109447},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109447},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001474},
author = {Subhadip Boral and Mainak Sarkar and Ashish Ghosh},
keywords = {Manifold learning, Anisotropic scaling, Gradient descent, Global scaling, Singular value decomposition, Kolmogorov-Smirnov test},
abstract = {Manifold learning methods unfold the manifold structures and embed them in a lower-dimensional space. The quality of such an embedding should be measured both qualitatively and quantitatively. The proposed manifold embedding quality assessment (MEQA) method does so by taking into account of local and global structure preservation as both are important traits of an embedding. To measure the local structure preservation MEQA uses two transformations. Initially anisotropic scaling, rotation and translation are incorporated to measure the closeness between the original and the embedded data points. In the next stage, rigid transformation is incorporated to quantify the previous transformation which involved anisotropic scaling. For quantifying the global structure preservation, the Kolmogorov-Smirnov test is applied in a distributed manner over each dimension and averaged over them. To establish the superiority of MEQA we conducted several studies over standard synthetic and real-life datasets across separate existing feature extraction techniques.}
}
@article{BIGOLINLANFREDI2023109430,
title = {Quantifying the preferential direction of the model gradient in adversarial training with projected gradient descent},
journal = {Pattern Recognition},
volume = {139},
pages = {109430},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109430},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001310},
author = {Ricardo {Bigolin Lanfredi} and Joyce D. Schroeder and Tolga Tasdizen},
keywords = {Robustness, Robust models, Gradient direction, Gradient alignment, Deep learning, PGD, Adversarial training, GAN},
abstract = {Adversarial training, especially projected gradient descent (PGD), has proven to be a successful approach for improving robustness against adversarial attacks. After adversarial training, gradients of models with respect to their inputs have a preferential direction. However, the direction of alignment is not mathematically well established, making it difficult to evaluate quantitatively. We propose a novel definition of this direction as the direction of the vector pointing toward the closest point of the support of the closest inaccurate class in decision space. To evaluate the alignment with this direction after adversarial training, we apply a metric that uses generative adversarial networks to produce the smallest residual needed to change the class present in the image. We show that PGD-trained models have a higher alignment than the baseline according to our definition, that our metric presents higher alignment values than a competing metric formulation, and that enforcing this alignment increases the robustness of models.}
}
@article{WANG2023109438,
title = {Attention reweighted sparse subspace clustering},
journal = {Pattern Recognition},
volume = {139},
pages = {109438},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109438},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001395},
author = {Libin Wang and Yulong Wang and Hao Deng and Hong Chen},
keywords = {Subspace clustering, Sparse, Robust representation},
abstract = {Subspace clustering has attracted much attention in many applications of computer vision and pattern recognition. Spectral clustering based methods, such as sparse subspace clustering (SSC) and low-rank representation (LRR), have become popular due to their theoretical guarantees and impressive performance. However, many state-of-the-art subspace clustering methods specify the mean square error (MSE) criterion as the loss function, which is sensitive to outliers and complex noises in reality. These methods have poor performance when the data are corrupted by complex noise. In this paper, we propose a robust sparse subspace clustering method, termed Attention Reweighted SSC (ARSSC), by paying less attention to the corrupted entries (adaptively assigning small weights to the corrupted entries in each data point). To reduce the extra bias in estimation introduced by ℓ1 regularization, we also utilize non-convex penalties to overcome the overpenalized problem. In addition, we provide theoretical guarantees for ARSSC and theoretically show that our method gives a subspace-preserving affinity matrix under appropriate conditions. To solve the ARSSC optimization problem, we devise an optimization algorithm using an Alternating Direction Method of Multipliers (ADMM) method. Experiments on real-world databases validate the effectiveness of the proposed method.}
}
@article{GAO2023109512,
title = {Not All Samples Are Born Equal: Towards Effective Clean-Label Backdoor Attacks},
journal = {Pattern Recognition},
volume = {139},
pages = {109512},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109512},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002121},
author = {Yinghua Gao and Yiming Li and Linghui Zhu and Dongxian Wu and Yong Jiang and Shu-Tao Xia},
keywords = {Backdoor attack, Clean-label attack, Sample selection, Trustworthy ML, AI Security, Deep learning},
abstract = {Recent studies demonstrated that deep neural networks (DNNs) are vulnerable to backdoor attacks. The attacked model behaves normally on benign samples, while its predictions are misled whenever adversary-specified trigger patterns appear. Currently, clean-label backdoor attacks are usually regarded as the most stealthy methods in which adversaries can only poison samples from the target class without modifying their labels. However, these attacks can hardly succeed. In this paper, we reveal that the difficulty of clean-label attacks mainly lies in the antagonistic effects of ‘robust features’ related to the target class contained in poisoned samples. Specifically, robust features tend to be easily learned by victim models and thus undermine the learning of trigger patterns. Based on these understandings, we propose a simple yet effective plug-in method to enhance clean-label backdoor attacks by poisoning ‘hard’ instead of random samples. We adopt three classical difficulty metrics as examples to implement our method. We demonstrate that our method can consistently improve vanilla attacks, based on extensive experiments on benchmark datasets.}
}
@article{HAO2023109504,
title = {Multi-dimensional Graph Neural Network for Sequential Recommendation},
journal = {Pattern Recognition},
volume = {139},
pages = {109504},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109504},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002042},
author = {Yongjing Hao and Jun Ma and Pengpeng Zhao and Guanfeng Liu and Xuefeng Xian and Lei Zhao and Victor S. Sheng},
keywords = {Sequential Recommendation, Graph Neural Networks, Self-attention Networks, Graph Embedding},
abstract = {Graph neural networks (GNNs) technology has been widely used in recommendation systems because most information in recommendation systems has a graph structure in nature, and GNNs have advantages in graph representation learning. In sequential recommendation, the relationships between interacting items can be constructed as an isomorphic graph, and (GNNs) can capture high-order information between graph nodes. Many models have used graph-based methods for sequential recommendation, and achieved great success. However, the existing research only considers the number of interactions between items when constructing the item graph. As such, revisions are needed to capture the multi-dimensional transformation relationships between items. Hence, we emphasize the importance of multi-dimensional information, and we propose a Category and Time information integrated Graph Neural Network (CTGNN), which combines the item category and interaction time information with a multi-layer graph convolution network to form multi-dimensional fine-grained item representations. In addition, we design a temporal self-attention network to model the dynamic user preference and make the next-item recommendation. Finally, we conduct extensive experiments on three real-world datasets, and the results demonstrate the excellent performance of the proposed model.}
}
@article{SHU2023109475,
title = {Localized curvature-based combinatorial subgraph sampling for large-scale graphs},
journal = {Pattern Recognition},
volume = {139},
pages = {109475},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109475},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001759},
author = {Dong Wook Shu and Youjin Kim and Junseok Kwon},
keywords = {Subgraph sampling method, Discrete Ricci curvature},
abstract = {This paper introduces a subgraph sampling method based on curvature to train large-scale graphs via mini-batch training. Owing to the difficulty in sampling globally optimal subgraphs from large graphs, we sample the subgraphs to minimize the distributional metric with combinatorial sampling. In particular, we define a combinatorial metric that distributionally measures the similarity between an original graph and all possible node and edge combinations of the subgraphs. Further, we prove that the subgraphs sampled using the probability model proportional to the discrete Ricci curvature (i.e., Ollivier-Ricci curvatures) of the edges can minimize the proposed metric. Moreover, as accurate calculation of the curvature on a large graph is challenging, we propose to use a localized curvature considering only 3-cycles on the graph, suggesting that this is a sufficiently approximated curvature on a sparse graph. We also show that the probability models of conventional sampling methods are related to coarsely approximated curvatures with no cycles, implying that the curvature is closely related to subgraph sampling. The experimental results confirm the feasibility of integrating the proposed curvature-based sampling method into existing graph neural networks to improve performance.}
}
@article{SUNKARA2023109451,
title = {YOGA: Deep object detection in the wild with lightweight feature learning and multiscale attention},
journal = {Pattern Recognition},
volume = {139},
pages = {109451},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109451},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001516},
author = {Raja Sunkara and Tie Luo},
abstract = {We introduce YOGA, a deep learning based yet lightweight object detection model that can operate on low-end edge devices while still achieving competitive accuracy. The YOGA architecture consists of a two-phase feature learning pipeline with a cheap linear transformation, which learns feature maps using only half of the convolution filters required by conventional convolutional neural networks. In addition, it performs multi-scale feature fusion in its neck using an attention mechanism instead of the naive concatenation used by conventional detectors. YOGA is a flexible model that can be easily scaled up or down by several orders of magnitude to fit a broad range of hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets with over 10 state-of-the-art object detectors. The results show that YOGA strikes the best trade-off between model size and accuracy (up to 22% increase of AP and 23–34% reduction of parameters and FLOPs), making it an ideal choice for deployment in the wild on low-end edge devices. This is further affirmed by our hardware implementation and evaluation on NVIDIA Jetson Nano.}
}
@article{HOFMEYR2023109471,
title = {Incremental estimation of low-density separating hyperplanes for clustering large data sets},
journal = {Pattern Recognition},
volume = {139},
pages = {109471},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109471},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001711},
author = {David P. Hofmeyr},
keywords = {Clustering, Low-density separation, Big data, Stochastic gradient descent, Smoothing kernel, High dimensionality},
abstract = {An efficient unsupervised method for obtaining low-density hyperplane separators is proposed. The method is based on a modified stochastic gradient descent applied on a convolution of the empirical distribution function with a smoothing kernel. Low-density hyperplanes are motivated by the fact that they avoid intersecting high density regions, and so tend to pass between high density clusters, thus separating them from one another, while keeping the individual clusters intact. Multiple hyperplanes can be combined in a hierarchical model to obtain a complete clustering solution. A simple post-processing of solutions induced by large collections of hyperplanes yields an efficient and accurate clustering method, capable of automatically selecting the number of clusters. Experiments show that the proposed method is highly competitive in terms of both speed and accuracy when compared with relevant benchmarks. Code is available in the form of an R package at https://github.com/DavidHofmeyr/iMDH.}
}
@article{HUA2023109511,
title = {Underwater object detection algorithm based on feature enhancement and progressive dynamic aggregation strategy},
journal = {Pattern Recognition},
volume = {139},
pages = {109511},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109511},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300211X},
author = {Xia Hua and Xiaopeng Cui and Xinghua Xu and Shaohua Qiu and Yingjie Liang and Xianqiang Bao and Zhong Li},
keywords = {Underwater image, Dynamic feature fusion, Small object detection, Rapid spatial pyramid pooling, Feature enhancement},
abstract = {To solve the problems that the conventional object detector is hard to extract features and miss detection of small objects when detecting underwater objects due to the noise of underwater environment and the scale change of objects, this paper designs a novel feature enhancement & progressive dynamic aggregation strategy, and proposes a new underwater object detector based on YOLOv5s. Firstly, a feature enhancement gating module is designed to selectively suppress or enhance multi-level features and reduce the interference of underwater complex environment noise on feature fusion. Then, the adjacent feature fusion mechanism and dynamic fusion module are designed to dynamically learn fusion weights and perform multi-level feature fusion progressively, so as to suppress the conflict information in multi-scale feature fusion and prevent small objects from being submerged by the conflict information. At last, a spatial pyramid pool structure (FMSPP) based on the same size quickly mixed pool layer is proposed, which can make the network obtain stronger description ability of texture and contour features, reduce the parameters, and further improve the generalization ability and classification accuracy. The ablation experiments and multi-method comparison experiments on URPC and DUT-USEG data sets prove the effectiveness of the proposed strategy. Compared with the current mainstream detectors, our detector achieves obvious advantages in detection performance and efficiency.}
}
@article{LI2023109421,
title = {Non-contact PPG signal and heart rate estimation with multi-hierarchical convolutional network},
journal = {Pattern Recognition},
volume = {139},
pages = {109421},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109421},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300122X},
author = {Bin Li and Panpan Zhang and Jinye Peng and Hong Fu},
keywords = {Remote heart rate measurement, Multi-hierarchical feature fusion, Spatio-temporal convolution},
abstract = {Heartbeat rhythm and heart rate (HR) are important physiological parameters of the human body. This study presents an efficient multi-hierarchical spatio-temporal convolutional network that can quickly estimate remote physiological (rPPG) signal and HR from face video clips. First, the facial color distribution characteristics are extracted using a low-level face feature generation (LFFG) module. Then, the three-dimensional (3D) spatio-temporal stack convolution module (STSC) and multi-hierarchical feature fusion module (MHFF) are used to strengthen the spatio-temporal correlation of multi-channel features. In the MHFF, sparse optical flow is used to capture the tiny motion information of faces between frames and generate a self-adaptive region of interest (ROI) skin mask. Finally, the signal prediction module (SP) is used to extract the estimated rPPG signal. The heart rate estimation results show that the proposed network overperforms the state-of-the-art methods on three datasets, 1) UBFC-RPPG, 2) COHFACE, 3) our dataset, with the mean absolute error (MAE) of 2.15, 5.57, 1.75 beats per minute (bpm) respectively.}
}
@article{SU2023109443,
title = {Hybrid token transformer for deep face recognition},
journal = {Pattern Recognition},
volume = {139},
pages = {109443},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109443},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001437},
author = {Weicong Su and Yali Wang and Kunchang Li and Peng Gao and Yu Qiao},
keywords = {Face recognition, Hybrid tokens, Relation learning},
abstract = {Although Convolutional Neural Networks have achieved remarkable successes in face recognition, they still suffer a critical limitation on capturing long range relations among facial regions. The recent vision transformers can naturally alleviate this problem, by learning global token dependencies. However, They are insufficient to discover high-level facial semantics since tokens in these transformers are based on small and fixed regions. To tackle such difficulty, we propose a novel Hybrid tOken Transformer (HOTformer) module to identify key facial semantics for effective recognition with cooperation of atomic and holistic tokens. Specifically, atomic tokens are generated from small fixed-size regions that can learn fine-grained core representation. Alternatively, holistic tokens are constructed from big adaptively-learned regions that can capture coarse-grained contextual representation. Furthermore, our HOTformer is a plug-and-play module. By hierarchically inserting it into convolutional networks, we can build a concise HOTformer-Net that achieves a preferable computation like CNN while boosting accuracy like transformer.}
}
@article{CHIEN2023109463,
title = {Bayesian asymmetric quantized neural networks},
journal = {Pattern Recognition},
volume = {139},
pages = {109463},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109463},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001632},
author = {Jen-Tzung Chien and Su-Ting Chang},
keywords = {Quantized neural network, Model compression, Binary neural network, Bayesian asymmetric quantization},
abstract = {This paper develops a robust model compression for neural networks via parameter quantization. Traditionally, quantized neural networks (QNN) were constructed by binary or ternary weights where the weights were deterministic. This paper generalizes QNN in two directions. First, M-ary QNN is developed to adjust the balance between memory storage and model capacity. The representation values and the quantization partitions in M-ary quantization are mutually estimated to enhance the resolution of gradients in neural network training. A flexible quantization with asymmetric partitions is formulated. Second, the variational inference is incorporated to implement the Bayesian asymmetric QNN. The uncertainty of weights is faithfully represented to enhance the robustness of the trained model in presence of heterogeneous data. Importantly, the multiple spike-and-slab prior is proposed to represent the quantization levels in Bayesian asymmetric learning. M-ary quantization is then optimized by maximizing the evidence lower bound of classification network. An adaptive parameter space is built to implement Bayesian quantization and neural representation. The experiments on various image recognition tasks show that M-ary QNN achieves similar performance as the full-precision neural network (FPNN), but the memory cost and the test time are significantly reduced relative to FPNN. The merit of Bayesian M-ary QNN using multiple spike-and-slab prior is investigated.}
}
@article{SUN2023109524,
title = {MSCA-Net: Multi-scale contextual attention network for skin lesion segmentation},
journal = {Pattern Recognition},
volume = {139},
pages = {109524},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109524},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002248},
author = {Yongheng Sun and Duwei Dai and Qianni Zhang and Yaqi Wang and Songhua Xu and Chunfeng Lian},
keywords = {Skin lesion segmentation, Multi-scale bridge module, Global-local channel spatial attention module, Scale-aware deep supervision module},
abstract = {Lesion segmentation algorithms automatically outline lesion areas in medical images, facilitating more effective identification and assessment of the clinically relevant features, and improving the efficacy and diagnosis accuracy. However, most fully convolutional network based segmentation methods suffer from spatial and contextual information loss when decreasing image resolution. To overcome this shortcoming, this paper proposes a skin lesion segmentation model, namely, the Multi-Scale Contextual Attention Network (MSCA-Net), which can exploit the multi-scale contextual information in images. Inspired by the skip connection of U-Net, we design a multi-scale bridge (MSB) module which interacts with multi-scale features to effectively fuse the multi-scale contextual information of the encoder and decoder path features. We further propose a global-local channel spatial attention module (GL-CSAM), aiming at capturing global contextual information. In addition, to take full advantage of the multi-scale features of the decoder, we propose a scale-aware deep supervision (SADS) module to achieve hierarchical iterative deep supervision. Comprehensive experimental results on the public dataset of ISIC 2017, ISIC 2018, and PH2 show that our proposed method outperforms other state-of-the-art methods, demonstrating the efficacy of our method in skin lesion segmentation. Our code is available at https://github.com/YonghengSun1997/MSCA-Net.}
}
@article{XIANG2023109487,
title = {Self-supervised learning of scene flow with occlusion handling through feature masking},
journal = {Pattern Recognition},
volume = {139},
pages = {109487},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109487},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001875},
author = {Xuezhi Xiang and Rokia Abdein and Ning Lv and Jie Yang},
keywords = {Optical flow, Depth estimation, Camera pose, Rigidity segmentation, Occlusion handling, Deformable decoder},
abstract = {In this work, we improve the optical flow and depth based on the important observation that they should share the geometric structure of the reference image. We initially propose a feature masking method to reduce the occlusion impact on the optical flow and depth by producing preliminary motion features that share the structure of the reference image. In addition, we propose a deformable decoder that learns the geometrical structure of the reference image in the form of a group of offsets and uses them to adapt the motion features of flow and depth maps, thereby preventing incorrect propagation in occluded regions and providing more structural details in the other regions. Furthermore, we recursively update the optical flow with self-supervised cues learned from the rigid flow and optical flow. Our method achieves a new state-of-the-art result for the optical flow on the KITTI 2015 benchmark with F1 = 11.17%.}
}
@article{LU2023109434,
title = {Intensity mixture and band-adaptive detail fusion for pansharpening},
journal = {Pattern Recognition},
volume = {139},
pages = {109434},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109434},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001358},
author = {Hangyuan Lu and Yong Yang and Shuying Huang and Xiaolong Chen and Hongfu Su and Wei Tu},
keywords = {Pansharpening, Intensity mixture, Band-adaptive detail fusion, Point spread function},
abstract = {Pansharpening aims to sharpen a low-resolution multispectral (MS) image through a high-resolution single-channel panchromatic (PAN) image to obtain a high-resolution multi-spectral (HRMS) image. However, low correlation between the PAN and MS images, as well as the inaccurate detail injection for each band of MS image are the key problems causing spectral and spatial distortions in pansharpening. To address these issues, a new pansharpening method based on the intensity mixture and band-adaptive detail fusion is proposed. To obtain a mixed-intensity image (T) that has a high correlation with the MS image and maintain the gradient information of the PAN image, the intensity mixture model is constructed by establishing the intensity and gradient constraints between T and the source images. As it is hard to obtain a proper degradation filter in the model, a filter estimation algorithm is designed by the distribution alignment. To inject the details that match the point spread function of the sensor, a band-adaptive detail fusion algorithm is presented to fuse the details extracted from T with those from the MS image for each band. Furthermore, as there are far fewer details in the MS image than in T, a detail enhancement algorithm is proposed to enhance the details proportionally. The final HRMS image is obtained by injecting the fused details into the upsampled MS image. Extensive experiments show that the proposed method can efficiently achieve the best results in fusion quality compared to state-of-the-art methods. The code is availabe at https://github.com/yotick/IMBD.}
}
@article{DENG2023109470,
title = {Strongly augmented contrastive clustering},
journal = {Pattern Recognition},
volume = {139},
pages = {109470},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109470},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300170X},
author = {Xiaozhi Deng and Dong Huang and Ding-Hua Chen and Chang-Dong Wang and Jian-Huang Lai},
keywords = {Data clustering, Deep clustering, Image clustering, Contrastive learning, Deep neural network},
abstract = {Deep clustering has attracted increasing attention in recent years due to its capability of joint representation learning and clustering via deep neural networks. In its latest developments, the contrastive learning has emerged as an effective technique to substantially enhance the deep clustering performance. However, the existing contrastive learning based deep clustering algorithms mostly focus on some carefully-designed augmentations (often with limited transformations to preserve the structure), referred to as weak augmentations, but cannot go beyond the weak augmentations to explore the more opportunities in stronger augmentations (with more aggressive transformations or even severe distortions). In this paper, we present an end-to-end deep clustering approach termed Strongly Augmented Contrastive Clustering (SACC), which extends the conventional two-augmentation-view paradigm to multiple views and jointly leverages strong and weak augmentations for strengthened deep clustering. Particularly, we utilize a backbone network with triply-shared weights, where a strongly augmented view and two weakly augmented views are incorporated. Based on the representations produced by the backbone, the weak-weak view pair and the strong-weak view pairs are simultaneously exploited for the instance-level contrastive learning (via an instance projector) and the cluster-level contrastive learning (via a cluster projector), which, together with the backbone, can be jointly optimized in a purely unsupervised manner. Experimental results on five challenging image datasets have shown the superiority of our SACC approach over the state-of-the-art. The code is available at https://github.com/dengxiaozhi/SACC.}
}
@article{WEI2023109483,
title = {Deep debiased contrastive hashing},
journal = {Pattern Recognition},
volume = {139},
pages = {109483},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109483},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001838},
author = {Rukai Wei and Yu Liu and Jingkuan Song and Yanzhao Xie and Ke Zhou},
keywords = {Learning to hash, Contrastive learning, Instance discrimination, EM Algorithm, Neighborhood discovery},
abstract = {Hashing has achieved great success in multimedia retrieval due to its high computing efficiency and low storage cost. Recently, contrastive-learning-based hashing methods have achieved decent retrieval performance in label-free scenarios by learning distortion-invariant representations with Siamese networks. Their learning principle, i.e., instance discrimination, maximizes the correlation between self-augmented views and treats all others as negative samples. However, it may learn with false negative samples that are naturally similar, resulting in biased hash learning. To bridge this flaw, we reveal the between-instance similarity of naturally similar samples by exploring the latent structure of the training data. As a result, we propose the Deep Debiased Contrastive Hashing (DDCH) algorithm, using the neighborhood discovery module to explore the intrinsic similarity relationship that can help contrastive hashing reduce false negatives for superior discriminatory ability. Furthermore, we elucidate the rationale for incorporating the module into the contrastive hashing framework and explain our hashing process from an Expectation-Maximization (EM) perspective. Extensive experimental results on three benchmark image datasets demonstrate that DDCH significantly outperforms the state-of-the-art unsupervised hashing methods for image retrieval.}
}
@article{HAN2023109517,
title = {The impact of isolation kernel on agglomerative hierarchical clustering algorithms},
journal = {Pattern Recognition},
volume = {139},
pages = {109517},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109517},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002170},
author = {Xin Han and Ye Zhu and Kai Ming Ting and Gang Li},
keywords = {Agglomerative hierarchical clustering, Varied densities, Dendrogram purity, Isolation kernel, Gaussian kernel},
abstract = {Agglomerative hierarchical clustering (AHC) is one of the popular clustering approaches. AHC generates a dendrogram that provides richer information and insights from a dataset than partitioning clustering. However, a major problem with existing distance-based AHC methods is: it fails to effectively identify adjacent clusters with varied densities, regardless of the cluster extraction methods applied to the resultant dendrogram. This paper aims to reveal the root cause of this issue and provides a solution by using a data-dependent kernel. We analyse the condition under which existing AHC methods fail to effectively extract clusters, and give the reason why the data-dependent kernel is an effective remedy. This leads to a new approach to kernerlise existing hierarchical clustering algorithms including the traditional AHC algorithms, HDBSCAN, GDL, PHA and HC-OT. Our extensive empirical evaluation shows that the recently introduced Isolation Kernel produces a higher quality or purer dendrogram than distance, Gaussian Kernel and adaptive Gaussian Kernel in all the above mentioned AHC algorithms.}
}
@article{SHAO2023109509,
title = {FGPNet: A weakly supervised fine-grained 3D point clouds classification network},
journal = {Pattern Recognition},
volume = {139},
pages = {109509},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109509},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002091},
author = {Huihui Shao and Jing Bai and Rusong Wu and Jinzhe Jiang and Hongbo Liang},
keywords = {3D point clouds, Fine-grained classification, Context-aware feature extraction, SimAM-Capsule aggregation, Local geometric details, Spatial relationships},
abstract = {3D point clouds classification has been a hot research topic and received great progress in recent years. However, due to the similar data distributions and subtle differences among various sub-categories in a meta-category, the 3D point clouds classification at a fine-grained level is still very challenging, especially without the annotations of part locations or attributes. In this paper, we propose a novel weakly supervised network for fine-grained 3D point clouds classification, namely FGPNet. Different from the previous supervised fine-grained classification methods that use class labels and other manual annotation information, FGPNet develops a unified framework to address both local geometric details and global spatial structures only using the class labels as input. Specifically, FGPNet firstly employs a context-aware discriminative feature extraction (CDFE) module, which extract contextual contrasted information across differential receptive fields hierarchically, and further capture discriminative local details from point clouds. Subsequently, an SimAM-Capsule Aggregation (SCA) module is introduced to highlight the significant local features and capture their spatial relationships. Quantitative and qualitative experimental results on fine-grained dataset including three categories Airplane, Chair and Car demonstrate that FGPNet outperforms the state-of-the-art methods on fine-grained 3D point clouds classification tasks.}
}
@article{HE2023109456,
title = {Fuzzy granular recurrence plot and quantification analysis: A novel method for classification},
journal = {Pattern Recognition},
volume = {139},
pages = {109456},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109456},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001565},
author = {Qian He and Fusheng Yu and Jiaqi Chang and Chenxi Ouyang},
keywords = {Fuzzy granular recurrence plot, Recurrence plot, Fuzzy information granule, Noise, Classification},
abstract = {Recently, recurrence plot (RP) and its quantification techniques have become an important research tool in nonlinear analysis. In the existing researches, an RP is directly established on a time series ignoring the influence of noise on data, which will affect our judgement on the dynamic properties of a system. To tackle the problem there, this paper proposes a novel recurrence plot, namely fuzzy granular recurrence plot (FGRP). An FGRP of a time series is built not directly on the time series itself but on its corresponding granular time series which is composed of fuzzy information granules. With specific capability, fuzzy information granules are used as building blocks of an FGRP to achieve high-level, compact and understandable signal models. In order to apply the FGRP method to time series classification tasks, an FGRP based classification model is designed in this paper. Subsequent experiments show that the FGRP of a time series can reduce the effect of noise, and the FGRP based classification model can improve the classification performance.}
}
@article{CHENG2023109403,
title = {Bottom-up 2D pose estimation via dual anatomical centers for small-scale persons},
journal = {Pattern Recognition},
volume = {139},
pages = {109403},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109403},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001048},
author = {Yu Cheng and Yihao Ai and Bo Wang and Xinchao Wang and Robby T. Tan},
keywords = {Multi-person pose estimation, Human pose estimation, Anatomical centers},
abstract = {In multi-person 2D pose estimation, the bottom-up methods simultaneously predict poses for all persons, and unlike the top-down methods, do not rely on human detection. However, the SOTA bottom-up methods’ accuracy is still inferior compared to the existing top-down methods. This is due to the predicted human poses being regressed based on the inconsistent human bounding box center and the lack of human-scale normalization, leading to the predicted human poses being inaccurate and small-scale persons being missed. To push the envelope of the bottom-up pose estimation, we firstly propose multi-scale training to enhance the network to handle scale variation with single-scale testing, particularly for small-scale persons. Secondly, we introduce dual anatomical centers (i.e., head and body), where we can predict the human poses more accurately and reliably, especially for small-scale persons. Moreover, existing bottom-up methods use multi-scale testing to boost the accuracy of pose estimation at the price of multiple additional forward passes, which weakens the efficiency of bottom-up methods, the core strength compared to top-down methods. By contrast, our multi-scale training enables the model to predict high-quality poses in a single forward pass (i.e., single-scale testing). Our method achieves 38.4% improvement on bounding box precision and 39.1% improvement on bounding box recall over the state of the art (SOTA) on the challenging small-scale persons subset of COCO. For the human pose AP evaluation, we achieve a new SOTA (71.0 AP) on the COCO test-dev set with the single-scale testing. We also achieve the top performance (40.3 AP) on the OCHuman dataset in cross-dataset evaluation.}
}
@article{XI2023109476,
title = {TreeNet: Structure preserving multi-class 3D point cloud completion},
journal = {Pattern Recognition},
volume = {139},
pages = {109476},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109476},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001760},
author = {Long Xi and Wen Tang and TaoRuan Wan},
keywords = {3D Point cloud completion, Multi-class training, Hierarchical tree, Computer vision, Artificial intelligence, Deep learning},
abstract = {Generating the missing data of 3D object point clouds from partial observations is a challenging task. Existing state-of-the-art learning-based 3D point cloud completion methods tend to use a limited number of categories/classes of training data and regenerate the entire point cloud based on the training datasets. As a result, output 3D point clouds generated by such methods may lose details (i.e. sharp edges and topology changes) due to the lack of multi-class training. These methods also lose the structural and spatial details of partial inputs due to the models do not separate the reconstructed partial input from missing points in the output. In this paper, we propose a novel deep learning network - TreeNet for 3D point cloud completion. TreeNet has two networks in hierarchical tree-based structures: TreeNet-multiclass focuses on multi-class training with a specific class of the completion task on each sub-tree to improve the quality of point cloud output; TreeNet-binary focuses on generating points in missing areas and fully preserving the original partial input. TreeNet-multiclass and TreeNet-binary are both network decoders and can be trained independently. TreeNet decoder is the combination of TreeNet-multiclass and TreeNet-binary and is trained with an encoder from existing methods (i.e. PointNet encoder). We compare the proposed TreeNet with five state-of-the-art learning-based methods on fifty classes of the public Shapenet dataset and unknown classes, which shows that TreeNet provides a significant improvement in the overall quality and exhibits strong generalization to unknown classes that are not trained.}
}
@article{LUO2023109448,
title = {Dual-channel graph contrastive learning for self-supervised graph-level representation learning},
journal = {Pattern Recognition},
volume = {139},
pages = {109448},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109448},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001486},
author = {Zhenfei Luo and Yixiang Dong and Qinghua Zheng and Huan Liu and Minnan Luo},
keywords = {Contrastive learning, Graph representation learning, Graph neural networks, Graph classification},
abstract = {Self-supervised graph-level representation learning aims to learn discriminative representations for subgraphs or entire graphs without human-curated labels. Recently, graph contrastive learning (GCL) methods have revolutionized this field and achieved state-of-the-art results in various downstream tasks. Nonetheless, current GCL models are mostly based on simple node-level information aggregation operations and fail to reveal various substructures from input graphs. Moreover, to perform graph-graph contrastive training, they often involve well-designed graph augmentation, which is expensive and requires extensive expert efforts. Here, we propose a novel GCL framework, namely DualGCL, for self-supervised graph-level representation learning. For fine-grained local information incorporation, we first present an adaptive hierarchical aggregation process with a differentiable Transformer-based aggregator. Then, to efficiently learn graph-level discriminative representations, we introduce a dual-channel contrastive learning process in a multi-granularity and augmentation-free contrasting mode. When tested empirically on six popular graph classification benchmarks, our DualGCL achieves better or comparable performance than various strong baselines.}
}
@article{WANG2023109513,
title = {Dual similarity pre-training and domain difference encouragement learning for vehicle re-identification in the wild},
journal = {Pattern Recognition},
volume = {139},
pages = {109513},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109513},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002133},
author = {Qi Wang and Yuling Zhong and Weidong Min and Haoyu Zhao and Di Gai and Qing Han},
keywords = {Vehicle re-identification, Unsupervised domain adaptation, Dual constraint label smoothing regularization loss, Domain difference penalty term, Pseudo label refinement},
abstract = {Existing unsupervised domain adaptation (UDA) tasks require extensive annotated wild data in the source domain to be generalized to the target domain. Additionally, the large gap between the source and target domains hinder the clustering performance. Our work concentrates on few-shot UDA task to train a robust Re-ID model from practical vehicle re-identification (Re-ID). That is to say, this task learns discriminative representations from a few labeled source data to unlabeled target data. In this paper, a novel progressive few-shot UDA learning framework for vehicle Re-ID is proposed, which consists of two branches. In source branch, the dual prior model is used to gain the color and IDs in unlabeled source data. A dual constraint label smoothing regularization (DCLSR) loss is designed to supervise extensive unlabeled source data during pre-training phase, which considers color and ID constraints to mine the similarity between unlabeled source data and a few labeled ones. The target branch develops a progressive domain difference encouragement learning method to optimize the cross-domain capability of Re-ID model. The domain difference penalty term (DDPT) is encoded by the feature variations before and after style transfer, which improves clustering results and refines the pseudo label. Comprehensive experimental results verify that the proposed approach outperforms other ones in the practical UDA task.}
}
@article{LIU2023109468,
title = {Cycle optimization metric learning for few-shot classification},
journal = {Pattern Recognition},
volume = {139},
pages = {109468},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109468},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001681},
author = {Qifan Liu and Wenming Cao and Zhihai He},
keywords = {Few-shot learning, Cycle optimization, Image classification},
abstract = {Metric learning methods are widely used in few-shot learning due to their simplicity and effectiveness. Most existing methods directly predict query labels by comparing the similarity between support and query samples. In this paper, we design a cycle optimization metric network for few-shot classification task that optimizes model performance based on loop-prediction of the labels of query samples and support samples. Specifically, we construct a forward network and reverse network based on a geometric algebra Graph Neural Network (GA-GNN). These two networks form the loop prediction from support samples to query samples and then back to support samples, guided by a cycle-consistency loss. We also introduce an optimization module that is able to correct the predicted results of query samples to further improve the network performance. Our extensive experimental results demonstrate that the proposed cycle optimization metric network outperforms existing state-of-the-art few-shot learning methods on classification tasks.}
}
@article{LI2023109452,
title = {Semantic-based conditional generative adversarial hashing with pairwise labels},
journal = {Pattern Recognition},
volume = {139},
pages = {109452},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109452},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001528},
author = {Qi Li and Weining Wang and Yuanyan Tang and Chengzhong Xu and Zhenan Sun},
keywords = {Generative adversarial networks, Semantic-based conditional information, Hashing with pairwise labels},
abstract = {Hashing has been widely exploited in recent years due to the rapid growth of image and video data on the web. Benefiting from recent advances in deep learning, deep hashing methods have achieved promising results with supervised information. However, it is usually expensive to collect the supervised information. In order to utilize both labeled and unlabeled data samples, many semi-supervised hashing methods based on Generative Adversarial Networks (GANs) have been proposed. Most of them still need the conditional information, which is usually generated by the pre-trained neural networks or leveraging random binary vectors. One natural question about these methods is that how can we generate a better conditional information given the semantic similarity information? In this paper, we propose a general two-stage conditional GANs hashing framework based on the pairwise label information. Both the labeled and unlabeled data samples are exploited to learn hash codes under our framework. In the first stage, the conditional information is generated via a general Bayesian approach, which has a much lower dimensional representation and maintains the semantic information of original data samples. In the second stage, a semi-supervised approach is presented to learn hash codes based on the conditional information. Both pairwise based cross entropy loss and adversarial loss are introduced to make full use of labeled and unlabeled data samples. Extensive experiments have shown that the propose algorithm outperforms current state-of-the-art methods on three benchmark image datasets, which demonstrates the effectiveness of our method.}
}
@article{LIU2023109496,
title = {Joint spatial and scale attention network for multi-view facial expression recognition},
journal = {Pattern Recognition},
volume = {139},
pages = {109496},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109496},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001966},
author = {Yuanyuan Liu and Jiyao Peng and Wei Dai and Jiabei Zeng and Shiguang Shan},
keywords = {Facial expression recognition, Joint spatial and scale attention mechanism, Expression-related regions, Pose variation, Dynamically multi-task learning},
abstract = {Multi-view facial expression recognition (FER) is a challenging task because the appearance of an expression varies greatly due to poses. To alleviate the influences of poses, recently developed methods perform pose normalization, learn pose-invariant features, or learn pose-specific FER classifiers. However, these methods usually rely on a prerequisite pose estimator or expressive region detector that is independent of the subsequent expression analysis. Different from existing methods, we propose a joint spatial and scale attention network (SSA-Net) to localize proper regions for simultaneous head pose estimation (HPE) and FER. Specifically, SSA-Net discovers the regions most relevant to the facial expression at hierarchical scales by a spatial attention mechanism, and the most informative scales are selected in a scale attention learning manner to learn the joint pose-invariant and expression-discriminative representations. Then, we employ a dynamically constrained multi-task learning mechanism with a delicately designed constrain regulation to properly and adaptively train the network to optimize the representations, thus achieving accurate multi-view FER. The effectiveness of the proposed SSA-Net is validated on three multi-view datasets (BU-3DFE, Multi-PIE, and KDEF) and three in-the-wild FER datasets (AffectNet, SFEW, and FER2013). Extensive experiments demonstrate that the proposed framework outperforms existing state-of-the-art methods under both within-dataset and cross-dataset settings, with relative accuracy gains of 2.36%, 1.33%, 3.11%, 2.84%, 15.7%, and 7.57%, respectively.}
}
@article{KRAWCZYK2023109444,
title = {Segmentation of 3D Point Cloud Data Representing Full Human Body Geometry: A Review},
journal = {Pattern Recognition},
volume = {139},
pages = {109444},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109444},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001449},
author = {Damian Krawczyk and Robert Sitnik},
keywords = {human body silhouette segmentation, automated point cloud segmentation, surface measurement, 3D data acquisition},
abstract = {This article aims to present a review of the segmentation techniques of the 3D data representing human body in the form of point clouds. The techniques discussed are divided into three subgroups: 2D contour approaches, topological techniques, and machine learning heuristics. These subgroups are then reviewed regarding the following aspects: computation time, accuracy, reliability, dependency on human pose, and segment count. The authors emphasize an analysis of these algorithms with respect to their exploitation in the segmentation of 3D data varying in time, as well as further improvement of the applications in anthropometry. The conclusion reached is that machine learning approach tends to be the most suitable solution for future 4D applications. Another foreseeable direction of development in the field of segmentation algorithms is the classification of the points on the borders between segments and maintaining fluent and consistent edges of the segments between the subsequent frames.}
}
@article{LU2023109480,
title = {A survey on machine learning from few samples},
journal = {Pattern Recognition},
volume = {139},
pages = {109480},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109480},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001802},
author = {Jiang Lu and Pinghua Gong and Jieping Ye and Jianwei Zhang and Changshui Zhang},
keywords = {Few sample learning, Learn to learn, Survey, Few-shot learning, Meta learning},
abstract = {The capability of learning and generalizing from very few samples successfully is a noticeable demarcation separating artificial intelligence and human intelligence. Despite the long history dated back to the early 2000s and the widespread attention in recent years with booming deep learning, few surveys for few sample learning (FSL) are available. We extensively study almost all papers of FSL spanning from the 2000s to now and provide a timely and comprehensive survey for FSL. In this survey, we review the evolution history and current progress on FSL, categorize FSL approaches into the generative model based and discriminative model based kinds in principle, and emphasize particularly on the meta learning based FSL approaches. We also summarize several recently emerging extensional topics of FSL and review their latest advances. Furthermore, we highlight the important FSL applications covering many research hotspots in computer vision, natural language processing, audio and speech, reinforcement learning and robotic, data analysis, etc. Finally, we conclude the survey with a discussion on promising trends in the hope of providing guidance and insights to follow-up researches.}
}
@article{DONG2023109488,
title = {Class-incremental object detection},
journal = {Pattern Recognition},
volume = {139},
pages = {109488},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109488},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001887},
author = {Na Dong and Yongqiang Zhang and Mingli Ding and Yancheng Bai},
keywords = {Class-incremental learning, Object detection, Information asymmetry, Non-affection distillation, Deep learning},
abstract = {Deep learning architectures have shown remarkable results in the object detection task. However, they experience a critical performance drop when they are required to learn new classes incrementally without forgetting old ones. This catastrophic forgetting phenomenon impedes the deployment of artificial intelligence in real word scenarios where systems need to learn new and different representations over time. Recently, many incremental learning methods have been proposed to avoid the catastrophic forgetting problem. However, current state-of-the-art class-incremental learning strategies aim at preserving the knowledge of old classes while learning new ones sequentially, which would encounter other problems as follows: (1) In the process of preserving information of old classes, only a small portion of data in the previous tasks are kept and replayed during training, which inevitably incurs bias that is favorable for the new classes but malicious to the old classes. (2) With the knowledge of previous classes distilled into the new model, a sub-optimal solution for the new task is obtained since the preserving process of previous classes sabotages the training of new classes. To address these issues, termed as Information Asymmetry (IA), we propose a double-head framework which preserves the knowledge of old classes and learns the knowledge of new classes separately. Specifically, we transfer the knowledge of the previous model to the current learned one for overcoming the catastrophic forgetting problem. Furthermore, considering that IA would introduce impacts on the training of the new model, we propose a Non-Affection mask to distill the knowledge of the interested regions at the feature level. Comprehensive experimental results demonstrate that our proposed method significantly outperforms other state-of-the-art class-incremental object detection methods on PASCAL VOC and MS COCO datasets.}
}
@article{CHEN2023109467,
title = {An Interpretable Channelwise Attention Mechanism based on Asymmetric and Skewed Gaussian Distribution},
journal = {Pattern Recognition},
volume = {139},
pages = {109467},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109467},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300167X},
author = {Cheng Chen and Bo Li},
keywords = {Channelwise attention, Interpretable modeling, Skewness distribution, Asymmetric distribution},
abstract = {Channelwise attention mechanisms have recently been demonstrated to boost the performance of deep convolutional neural networks (CNNs). The hypothesis on the negative correlation between channelwise responses and their importance levels has been verified. Therefore, according to the shifted means and unbalanced responses that are observed in attention distribution, two empirical hypotheses are proposed in this paper. Then, as an interpretable attention module, bilateral asymmetric skewed Gaussian attention (bi-SGA) is utilized to combine skewed and asymmetric properties into the Gaussian context transformer (GCT), which is the state-of-the-art. Finally, extensive experiments on the different benchmarks validate the rationality of the hypotheses. The proposed bi-SGA improves the overall performance of GCT attention mechanisms with only two extra parameters to be learned. Moreover, our attention mechanism also provides a perspective on analyzing the channelwise importance levels in deep neural networks in an interpretable and logical manner.}
}
@article{WANG2023109472,
title = {Sparse feature selection via fast embedding spectral analysis},
journal = {Pattern Recognition},
volume = {139},
pages = {109472},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109472},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001723},
author = {Jingyu Wang and Hongmei Wang and Feiping Nie and Xuelong Li},
keywords = {Unsupervised learning, Feature selection, Spectral analysis, Sparse subspace, -Norm},
abstract = {Feature selection has been a research hotspot in many fields. Models based on graph learning are currently the most popular approaches. However, the sparsity of most models is not strong, and graph learning for pair-sample evaluation takes a lot of time. ℓ2,1-norm regularization is the sparsity strategy adopted in most sparse models at present since the convex function is easy to solve. Nevertheless, the sparsity of ℓ2,1-norm is insufficient, and there exist parameter adjustment problems. ℓ2,0-norm is a better choice, which can strengthen the sparse constraints of the subspace. In this paper, the Sparse feature selection via Fast Embedding Spectral Analysis (SFESA) is proposed. Firstly, an adaptive anchor nearest neighbor graph is constructed to avoid the high time cost of learning pairwise nearest neighbor graphs to a certain extent. The low-dimensional embedding of data manifold structure is maintained by performing spectral analysis for the constructed graph. Secondly, the projected data is approximated to the low-dimensional embedding structure via a regularization term. Finally, ℓ2,0-norm is employed to constrain the projection matrix to enhance the subspace sparsity. Furthermore, a fast iterative algorithm is presented to solve this non-convex optimization problem. Extensive experiments on multiple public datasets show that SFESA can obtain excellent performance in less time.}
}
@article{BAKKALI2023109419,
title = {VLCDoC: Vision-Language contrastive pre-training model for cross-Modal document classification},
journal = {Pattern Recognition},
volume = {139},
pages = {109419},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109419},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001206},
author = {Souhail Bakkali and Zuheng Ming and Mickael Coustaty and Marçal Rusiñol and Oriol Ramos Terrades},
keywords = {Multimodal document representation learning, Document classification, Contrastive learning, Self-Attention, Transformers},
abstract = {Multimodal learning from document data has achieved great success lately as it allows to pre-train semantically meaningful features as a prior into a learnable downstream task. In this paper, we approach the document classification problem by learning cross-modal representations through language and vision cues, considering intra- and inter-modality relationships. Instead of merging features from different modalities into a joint representation space, the proposed method exploits high-level interactions and learns relevant semantic information from effective attention flows within and across modalities. The proposed learning objective is devised between intra- and inter-modality alignment tasks, where the similarity distribution per task is computed by contracting positive sample pairs while simultaneously contrasting negative ones in the joint representation space. Extensive experiments on public benchmark datasets demonstrate the effectiveness and the generality of our model both on low-scale and large-scale datasets.}
}
@article{ZHANG2023109439,
title = {Kernel-based feature aggregation framework in point cloud networks},
journal = {Pattern Recognition},
volume = {139},
pages = {109439},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109439},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001401},
author = {Jianjia Zhang and Zhenxi Zhang and Lei Wang and Luping Zhou and Xiaocai Zhang and Mengting Liu and Weiwen Wu},
keywords = {Point cloud, Kernel, Feature aggregation, Deep learning, Pooling},
abstract = {Various effective deep networks have been developed for analysis of 3D point clouds. One key step in these networks is to aggregate the features of orderless points into a compact representation for the cloud. As a typical order-invariant aggregation method, max-pooling has been widely applied. However, while enjoying simplicity and high efficiency, max-pooling does not fully exploit the feature information since it not only ignores the non-maximum elements in each feature dimension but also neglects the interactions between different dimensions. These drawbacks of max-pooling motivate us to explore advanced feature aggregation methods for 3D point cloud analysis. The desired advanced method should be capable of modeling richer information between the point features than max-pooling, and, at the same time, it can readily replace max-pooling without the need to modify other parts of the existing network architecture. To this end, this paper proposes a novel kernel-based feature aggregation framework for 3D point cloud analysis for the first time. The proposed method effectively considers all the elements in each dimension and models the nonlinear interactions between feature dimensions as complementary information to max-pooling. In addition, it is a plug-in module that can be integrated to many common networks as a replacement of max-pooling. Comprehensive experiments are conducted to demonstrate the consistently superior performance and high generality of the proposed method over max-pooling. Specifically, the proposed kernel-based feature aggregation framework consistently improves the max-pooling with three representative backbones of PointNet, DGCNN and PCT across four 3D point cloud based analysis tasks, including supervised 3D object classification, 3D part segmentation, indoor semantic segmentation and one additional unsupervised place retrieval task. Especially, it shows remarkable performance improvement over max-pooling in the unsupervised retrieval task, demonstrating its advantage in forming 3D point cloud representation.}
}
@article{SUN2023109464,
title = {Hyperspectral subpixel target detection based on interaction subspace model},
journal = {Pattern Recognition},
volume = {139},
pages = {109464},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109464},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323001644},
author = {Shengyin Sun and Jun Liu and Siyu Sun},
keywords = {Subpixel target detection, Hyperspectral images, Subspace model, Generalized likelihood ratio test (GLRT), Spectral variability},
abstract = {In this paper, we examine the problem of detecting subpixel targets in hyperspectral images. The so-called subpixel target refers to a target that only occupies a part of a pixel due to the low spatial resolution of hyperspectral sensors. Considering that the subpixel target spectrum is not always reliable (e.g., due to spectral variability), an interaction subspace model is designed to deal with this problem. In this subspace model, the second-order interaction terms are introduced to better describe the spectral variability, thereby improving the robustness. Specifically, the subspace model uses a hyperplane in a high-dimensional space to model spectral variability, while traditional models (e.g., the additive model and the replacement model) use a line in the high-dimensional space to model spectral variability. Obviously, the stronger description ability of the hyperplane makes the subspace model more tolerant to the mismatch of the target spectrum. Based on this interaction subspace model, we derive adaptive detectors according to the one-step generalized likelihood ratio test and its two-step variant. Experiments conducted on hyperspectral data demonstrate that the proposed two-step detector exhibits the strongest robustness in cases where the target spectrum is not very reliable.}
}