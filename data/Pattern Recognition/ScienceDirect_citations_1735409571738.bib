@article{JI2022108414,
title = {Fast Camouflaged Object Detection via Edge-based Reversible Re-calibration Network},
journal = {Pattern Recognition},
volume = {123},
pages = {108414},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108414},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005902},
author = {Ge-Peng Ji and Lei Zhu and Mingchen Zhuge and Keren Fu},
keywords = {Camouflaged Object Detection, Reversible Re-calibration Unit, Selective Edge Aggregation, NGES Priors},
abstract = {Camouflaged Object Detection (COD) aims to detect objects with similar patterns (e.g., texture, intensity, colour, etc) to their surroundings, and recently has attracted growing research interest. As camouflaged objects often present very ambiguous boundaries, how to determine object locations as well as their weak boundaries is challenging and also the key to this task. Inspired by the biological visual perception process when a human observer discovers camouflaged objects, this paper proposes a novel edge-based reversible re-calibration network called ERRNet. Our model is characterized by two innovative designs, namely Selective Edge Aggregation (SEA) and Reversible Re-calibration Unit (RRU), which aim to model the visual perception behaviour and achieve effective edge prior and cross-comparison between potential camouflaged regions and background. More importantly, RRU incorporates diverse priors with more comprehensive information comparing to existing COD models. Experimental results show that ERRNet outperforms existing cutting-edge baselines on three COD datasets and five medical image segmentation datasets. Especially, compared with the existing top-1 model SINet, ERRNet significantly improves the performance by ∼6% (mean E-measure) with notably high speed (79.3 FPS), showing that ERRNet could be a general and robust solution for the COD task.}
}
@article{ATAKY2022108382,
title = {A novel bio-inspired texture descriptor based on biodiversity and taxonomic measures},
journal = {Pattern Recognition},
volume = {123},
pages = {108382},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108382},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005628},
author = {Steve Tsham Mpinda Ataky and Alessandro {Lameiras Koerich}},
keywords = {Pattern recognition, Texture characterization and classification, Species richness, Taxonomic distinctiveness, Phylogenetic indices, Species abundance},
abstract = {Texture can be defined as the change of image intensity that forms repetitive patterns resulting from the physical properties of an object’s roughness or differences in a reflection on the surface. Considering that texture forms a system of patterns in a non-deterministic way, biodiversity concepts can help its characterization from an image. This paper proposes a novel approach to quantify such a complex system of diverse patterns through species diversity, richness, and taxonomic distinctiveness. The proposed approach considers each image channel as a species ecosystem and computes species diversity and richness as well as taxonomic measures to describe the texture. Furthermore, the proposed approach takes advantage of ecological patterns’ invariance characteristics to build a permutation, rotation, and translation invariant descriptor. Experimental results on three datasets of natural texture images and two datasets of histopathological images have shown that the proposed texture descriptor has advantages over several texture descriptors and deep methods.}
}
@article{FANG2022108398,
title = {Real masks and spoof faces: On the masked face presentation attack detection},
journal = {Pattern Recognition},
volume = {123},
pages = {108398},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108398},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005744},
author = {Meiling Fang and Naser Damer and Florian Kirchbuchner and Arjan Kuijper},
keywords = {Face presentation attack detection, COVID-19, Masked face, Face recognition, Biometric security},
abstract = {Face masks have become one of the main methods for reducing the transmission of COVID-19. This makes face recognition (FR) a challenging task because masks hide several discriminative features of faces. Moreover, face presentation attack detection (PAD) is crucial to ensure the security of FR systems. In contrast to the growing number of masked FR studies, the impact of face masked attacks on PAD has not been explored. Therefore, we present novel attacks with real face masks placed on presentations and attacks with subjects wearing masks to reflect the current real-world situation. Furthermore, this study investigates the effect of masked attacks on PAD performance by using seven state-of-the-art PAD algorithms under different experimental settings. We also evaluate the vulnerability of FR systems to masked attacks. The experiments show that real masked attacks pose a serious threat to the operation and security of FR systems.}
}
@article{ZHU2022108422,
title = {Neighborhood linear discriminant analysis},
journal = {Pattern Recognition},
volume = {123},
pages = {108422},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108422},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005987},
author = {Fa Zhu and Junbin Gao and Jian Yang and Ning Ye},
keywords = {Linear discriminant analysis, Reverse nearest neighbors, Neighborhood linear discriminant analysis, Multimodal class},
abstract = {Linear Discriminant Analysis (LDA) assumes that all samples from the same class are independently and identically distributed (i.i.d.). LDA may fail in the cases where the assumption does not hold. Particularly when a class contains several clusters (or subclasses), LDA cannot correctly depict the internal structure as the scatter matrices that LDA relies on are defined at the class level. In order to mitigate the problem, this paper proposes a neighborhood linear discriminant analysis (nLDA) in which the scatter matrices are defined on a neighborhood consisting of reverse nearest neighbors. Thus, the new discriminator does not need an i.i.d. assumption. In addition, the neighborhood can be naturally regarded as the smallest subclass, for which it is easier to be obtained than subclass without resorting to any clustering algorithms. The projected directions are sought to make sure that the within-neighborhood scatter as small as possible and the between-neighborhood scatter as large as possible, simultaneously. The experimental results show that nLDA performs significantly better than previous discriminators, such as LDA, LFDA, ccLDA, LM-NNDA, and l2,1-RLDA.}
}
@article{YU2022108401,
title = {Co-attentive multi-task convolutional neural network for facial expression recognition},
journal = {Pattern Recognition},
volume = {123},
pages = {108401},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108401},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100577X},
author = {Wenmeng Yu and Hua Xu},
keywords = {Facial expression recognition, Facial landmarks detection, Multi-task learning},
abstract = {Previous research on Facial Expression Recognition (FER) assisted by facial landmarks mainly focused on single-task learning or hard-parameter sharing based multi-task learning. However, soft-parameter sharing based methods have not been explored in this area. Therefore, this paper adopts Facial Landmark Detection (FLD) as the auxiliary task and explores new multi-task learning strategies for FER. First, three classical multi-task structures, including Hard-Parameter Sharing (HPS), Cross-Stitch Network (CSN), and Partially Shared Multi-task Convolutional Neural Network (PS-MCNN), are used to verify the advantages of multi-task learning for FER. Then, we propose a new end-to-end Co-attentive Multi-task Convolutional Neural Network (CMCNN), which is composed of the Channel Co-Attention Module (CCAM) and the Spatial Co-Attention Module (SCAM). Functionally, the CCAM generates the channel co-attention scores by capturing the inter-dependencies of different channels between FER and FLD tasks. The SCAM combines the max- and average-pooling operations to formulate the spatial co-attention scores. Finally, we conduct extensive experiments on four widely used benchmark facial expression databases, including RAF, SFEW2, CK+, and Oulu-CASIA. Extensive experimental results show that our approach achieves better performance than single-task and multi-task baselines, fully validating multi-task learning’s effectiveness and generalizability11Codes and detailed instructions can be available at https://github.com/thuiar/cmcnn..}
}
@article{ADIYEKE2022108393,
title = {Semi-supervised extensions of multi-task tree ensembles},
journal = {Pattern Recognition},
volume = {123},
pages = {108393},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108393},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005549},
author = {Esra Adıyeke and Mustafa Gökçe Baydoğan},
keywords = {Semi-supervised learning, Multi-task learning, Multi-objective trees, Ensemble learning, Totally randomized trees},
abstract = {Scale inconsistency is a widely encountered issue in multi-output learning problems. Specifically, target sets with multiple real valued or a mixture of categorical and real valued variables require addressing the scale differences to obtain predictive models with sufficiently good performance. Data transformation techniques are often employed to solve that problem. However, these operations are susceptible to different shortcomings such as changing the statistical properties of the data and increase the computational burden. Scale differences also pose problem in semi-supervised learning (SSL) models as they require processing of unsupervised information where distance measures are commonly employed. Classical distance metrics can be criticized as they lose efficiency when variables exhibit type or scale differences, too. Besides, in higher dimensions distance metrics cause problems due to loss of discriminative power. This paper introduces alternative semi-supervised tree-based strategies that are robust to scale differences both in terms of feature and target variables. We propose use of a scale-invariant proximity measure by means of tree-based ensembles to preserve the original characteristics of the data. We update classical tree derivation procedure to a multi-criteria form to resolve scale inconsistencies. We define proximity based clustering indicators and extend the supervised model with unsupervised criteria. Our experiments show that proposed method significantly outperforms its benchmark learning model that is predictive clustering trees.}
}
@article{BUJACK2022108313,
title = {Systematic generation of moment invariant bases for 2D and 3D tensor fields},
journal = {Pattern Recognition},
volume = {123},
pages = {108313},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108313},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004933},
author = {Roxana Bujack and Xinhua Zhang and Tomáš Suk and David Rogers},
keywords = {Pattern detection, Rotation invariant, Moment invariants, Generator approach, Basis, Flexible, Vector, Tensor},
abstract = {Moment invariants have been successfully applied to pattern detection tasks in 2D and 3D scalar, vector, and matrix valued data. However so far no flexible basis of invariants exists, i.e., no set that is optimal in the sense that it is complete and independent for every input pattern. In this paper, we prove that a basis of moment invariants can be generated that consists of tensor contractions of not more than two different moment tensors each under the conjecture of the set of all possible tensor contractions to be complete. This result allows us to derive the first generator algorithm that produces flexible bases of moment invariants with respect to orthogonal transformations by selecting a single non-zero moment to pair with all others in these two-factor products. Since at least one non-zero moment can be found in every non-zero pattern, this approach always generates a complete set of descriptors.}
}
@article{WANG2022108362,
title = {Cross-domain structure preserving projection for heterogeneous domain adaptation},
journal = {Pattern Recognition},
volume = {123},
pages = {108362},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108362},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005422},
author = {Qian Wang and Toby P. Breckon},
keywords = {Heterogeneous domain adaptation, Cross-domain projection, Image classification, Text classification},
abstract = {Heterogeneous Domain Adaptation (HDA) addresses the transfer learning problems where data from the source and target domains are of different modalities (e.g., texts and images) or feature dimensions (e.g., features extracted with different methods). It is useful for multi-modal data analysis. Traditional domain adaptation algorithms assume that the representations of source and target samples reside in the same feature space, hence are likely to fail in solving the heterogeneous domain adaptation problem. Contemporary state-of-the-art HDA approaches are usually composed of complex optimization objectives for favourable performance and are therefore computationally expensive and less generalizable. To address these issues, we propose a novel Cross-Domain Structure Preserving Projection (CDSPP) algorithm for HDA. As an extension of the classic LPP to heterogeneous domains, CDSPP aims to learn domain-specific projections to map sample features from source and target domains into a common subspace such that the class consistency is preserved and data distributions are sufficiently aligned. CDSPP is simple and has deterministic solutions by solving a generalized eigenvalue problem. It is naturally suitable for supervised HDA but has also been extended for semi-supervised HDA where the unlabelled target domain samples are available. Extensive experiments have been conducted on commonly used benchmark datasets (i.e. Office-Caltech, Multilingual Reuters Collection, NUS-WIDE-ImageNet) for HDA as well as the Office-Home dataset firstly introduced for HDA by ourselves due to its significantly larger number of classes than the existing ones (65 vs 10, 6 and 8). The experimental results of both supervised and semi-supervised HDA demonstrate the superior performance of our proposed method against contemporary state-of-the-art methods.}
}
@article{CAI2022108386,
title = {Unsupervised deep clustering via contractive feature representation and focal loss},
journal = {Pattern Recognition},
volume = {123},
pages = {108386},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108386},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005665},
author = {Jinyu Cai and Shiping Wang and Chaoyang Xu and Wenzhong Guo},
keywords = {Unsupervised learning, Clustering, Contractive feature representation, Focal loss, Auto-encoder},
abstract = {Deep clustering aims to promote clustering tasks by combining deep learning and clustering together to learn the clustering-oriented representation, and many approaches have shown their validity. However, the feature learning modules in existing methods hardly learn a discriminative representation. In addition, the label assignment mechanism becomes inefficient when dealing with some hard samples. To address these issues, a new joint optimization clustering framework is proposed through introducing the contractive representation in feature learning and utilizing focal loss in the clustering layer. The contractive penalty term added in feature learning would cause the local feature space contraction, resulting in learning more discriminative features. To our certain knowledge, this is also the first work to utilize the focal loss to improve the label assignment in deep clustering method. Moreover, the construction of the joint optimization framework enables the proposed method to learn feature representation and label assignment simultaneously in an end-to-end way. Finally, we comprehensively compare with some state-of-the-art clustering approaches on several clustering tasks to demonstrate the effectiveness of the proposed method.}
}
@article{WANG2022108578,
title = {Decomposing generation networks with structure prediction for recipe generation},
journal = {Pattern Recognition},
volume = {126},
pages = {108578},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108578},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000590},
author = {Hao Wang and Guosheng Lin and Steven C.H. Hoi and Chunyan Miao},
keywords = {Text generation, Vision-and-language},
abstract = {Recipe generation from food images and ingredients is a challenging task, which requires the interpretation of the information from another modality. Different from the image captioning task, where the captions usually have one sentence, cooking instructions contain multiple sentences and have obvious structures. To help the model capture the recipe structure and avoid missing some cooking details, we propose a novel framework: Decomposing Generation Networks (DGN) with structure prediction, to get more structured and complete recipe generation outputs. Specifically, we split each cooking instruction into several phases, and assign different sub-generators to each phase. Our approach includes two novel ideas: (i) learning the recipe structures with the global structure prediction component and (ii) producing recipe phases in the sub-generator output component based on the predicted structure. Extensive experiments on the challenging large-scale Recipe1M dataset validate the effectiveness of our proposed model, which improves the performance over the state-of-the-art results.}
}
@article{KHAMEKHEMJEMNI2022108370,
title = {Enhance to read better: A Multi-Task Adversarial Network for Handwritten Document Image Enhancement},
journal = {Pattern Recognition},
volume = {123},
pages = {108370},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108370},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005501},
author = {Sana {Khamekhem Jemni} and Mohamed Ali Souibgui and Yousri Kessentini and Alicia Fornés},
keywords = {Handwritten document image binarization, Document enhancement, Handwriting text recognition, Generative adversarial networks, Recurrent neural networks},
abstract = {Handwritten document images can be highly affected by degradation for different reasons: Paper ageing, daily-life scenarios (wrinkles, dust, etc.), bad scanning process and so on. These artifacts raise many readability issues for current Handwritten Text Recognition (HTR) algorithms and severely devalue their efficiency. In this paper, we propose an end to end architecture based on Generative Adversarial Networks (GANs) to recover the degraded documents into a clean and readable form. Unlike the most well-known document binarization methods, which try to improve the visual quality of the degraded document, the proposed architecture integrates a handwritten text recognizer that promotes the generated document image to be more readable. To the best of our knowledge, this is the first work to use the text information while binarizing handwritten documents. Extensive experiments conducted on degraded Arabic and Latin handwritten documents demonstrate the usefulness of integrating the recognizer within the GAN architecture, which improves both the visual quality and the readability of the degraded document images. Moreover, we outperform the state of the art in H-DIBCO challenges, after fine tuning our pre-trained model with synthetically degraded Latin handwritten images, on this task.}
}
@article{ZOU2022108402,
title = {AdaNFF: A new method for adaptive nonnegative multi-feature fusion to scene classification},
journal = {Pattern Recognition},
volume = {123},
pages = {108402},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108402},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005781},
author = {Zhiyuan Zou and Weibin Liu and Weiwei Xing},
keywords = {Scene classification, Adaptive feature fusion, Nonnegative matrix factorization, Feature fusion boosting},
abstract = {Scene classification is an important basis for many modern intelligent applications, however the performance of pattern recognition or deep learning-based methods are still not sufficient since complicated structure and context of scene images. In this paper, we propose a novel fusion framework of adaptive nonnegative feature fusion (AdaNFF) for scene classification. The AdaNFF integrates nonnegative matrix factorization, adaptive feature fusion and feature fusion boosting into an end-to-end process. Firstly, feature fusion is known as a general strategy to strengthen weak features, and we observe that pixel values and most hand-craft features of the scene image are naturally nonnegative. Therefore we are motivated to build a fusion method based on nonnegative matrix factorization, which can preserve features nonnegative properties and improve their representation performance. Secondly, with the results of fused single or multiple features fusion, we develop an adaptive feature fusion and boosting algorithm to improve the efficiency of image features. Finally, a normalized l2-norm classifier and a deep-learning like multilayer perceptron (MLP) classifier are trained to predict label of scene image. Under this framework, there are two versions of the proposed feature fusion method for nonnegative single-feature fusion and multi-feature fusion. All methods were validated on scene classification benchmarks. Experiment results suggest that the proposed methods can deal with multi-class scene problems and achieve remarkable classification performance.}
}
@article{ZHAO2022108346,
title = {Low-rank inter-class sparsity based semi-flexible target least squares regression for feature representation},
journal = {Pattern Recognition},
volume = {123},
pages = {108346},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108346},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005264},
author = {Shuping Zhao and Jigang Wu and Bob Zhang and Lunke Fei},
keywords = {Least squares regression, Low-rank inter-class sparsity, Feature representation, Image classification},
abstract = {Least squares regression (LSR) is an important machine learning method for feature extraction, feature selection, and image classification. For the training samples, there are correlations among samples from the same class. Therefore, many LSR-based methods utilize this property to pursue discriminative representation. However, if the training samples contain noise or outliers, it will be hard to obtain the exact inter-class correlation. To address this problem, in this paper, a novel LSR-based method is proposed, named low-rank inter-class sparsity based semi-flexible target least squares regression (LIS_StLSR). Firstly, the low-rank representation method is utilized to achieve the intrinsic characteristics of the training samples. Afterwards, the low-rank inter-class sparsity constraint is used to force the projected data to have an exact common sparsity structure in each class, which will be robust to noise and outliers in the training samples. This step can also reduce margins of samples from the same class and enlarge margins of samples from different classes to make the projection matrix discriminative. The low-rank representation and the discriminative projection matrix are jointly learned such that they can be boosted mutually. Moreover, a semi-flexible regression target matrix is introduced to measure the regression error more accurately, thus the regression performance can be enhanced to improve the classification accuracy. Experiments are implemented on the different databases of Yale B, AR, LFW, CASIA NIR-VIS, 15-Scene SPF, COIL-20, and Caltech 101, illustrating that the proposed LIS_StLSR outperforms many state-of-the-art methods.}
}
@article{DONG2022108558,
title = {A hierarchical receptive network oriented to target recognition in SAR images},
journal = {Pattern Recognition},
volume = {126},
pages = {108558},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108558},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000395},
author = {Ganggang Dong and Hongwei Liu},
keywords = {Target recognition, Deep learning, Knowledge, Hierarchical receptive, SAR Image},
abstract = {The recent years have witnessed a resurgence on neural network. Many functional layers are stacked hierarchically to learn the high-level representations. Yet the large album of radar image with label information are scarce. The fitting power of deep architectures are therefore limited. Additionally, the coherent imaging mechanism inevitably produce many speckles. They are with the statistical specificity of multiplicative noise, and hence make the image interpretation difficult. To solve the problems, this paper presents a new hierarchical receptive neural network. A signal-wise receptive module is first built by a family of delicate convolutional filters, with which the empirical features and knowledge are encoded. The receptive features are further refined in a patch-wise receptive unit, where some convolutional blocks are configured sequentially. The refined representations are finally used to make the inference. Multiple comparative studies are performed to demonstrate the advantage of proposed strategy.}
}
@article{DELEARDE2022108410,
title = {Description and recognition of complex spatial configurations of object pairs with Force Banner 2D features},
journal = {Pattern Recognition},
volume = {123},
pages = {108410},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108410},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005860},
author = {Robin Deléarde and Camille Kurtz and Laurent Wendling},
keywords = {Image analysis, Spatial relations, Relative position, Features and descriptors, Force histogram, Scene understanding},
abstract = {A major challenge in scene understanding is the handling of spatial relations between objects or object parts. Several descriptors dedicated to this task already exist, such as the force histogram which is a typical example of relative position descriptor. By computing the interaction between two objects for a given force in all the directions, it gives a good overview of the configuration, and it has useful properties that can make it invariant to the 2D viewpoint. Considering that using complementary forces (negative for repulsion, positive for attraction) should improve the description of complex spatial configurations, we propose to extend the force histogram to a panel of forces so as to make it a more complete descriptor. This gives a 2D descriptor that we called “(discrete) Force Banner” and which can be used as input of a classical Convolutional Neural Network (CNN), benefiting from their powerful performances, and reduced into more compact spatial features to use them in another system. As an illustration of its ability to describe spatial configurations, we used it to solve a classification problem aiming to discriminate simple spatial relations, but with variable configuration complexities. Experimental results obtained on datasets of synthetic and natural images with various shapes highlight the interest of this approach, in particular for complex spatial configurations.}
}
@article{SHI2022108566,
title = {ASMFS: Adaptive-similarity-based multi-modality feature selection for classification of Alzheimer's disease},
journal = {Pattern Recognition},
volume = {126},
pages = {108566},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108566},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000474},
author = {Yuang Shi and Chen Zu and Mei Hong and Luping Zhou and Lei Wang and Xi Wu and Jiliu Zhou and Daoqiang Zhang and Yan Wang},
keywords = {Multi-modality, Similarity learning, Feature selection, Alzheimer's disease},
abstract = {Multimodal classification methods using different modalities have great advantages over traditional single-modality-based ones for the diagnosis of Alzheimer's disease (AD) and its prodromal stage mild cognitive impairment (MCI). With the increasing amount of high-dimensional heterogeneous data to be processed, multi-modality feature selection has become a crucial research direction for AD classification. However, traditional methods usually depict the data structure using pre-defined similarity matrix as a priori, which is difficult to precisely measure the intrinsic relationship across different modalities in high-dimensional space. In this paper, we propose a novel multimodal feature selection method called Adaptive-Similarity-based Multi-modality Feature Selection (ASMFS) which performs adaptive similarity learning and feature selection simultaneously. Specifically, a similarity matrix is learned by jointly considering different modalities and at the same time, an efficient feature selection is conducted by imposing group sparsity-inducing l2,1-norm constraint. Evaluated on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database with baseline MRI and FDG-PET imaging data collected from 51 AD, 43 MCI converters (MCI-C), 56 MCI non-converters (MCI-NC) and 52 normal controls (NC), we demonstrate the effectiveness and superiority of our proposed method against other state-of-the-art approaches for multi-modality classification of AD/MCI.}
}
@article{WU2022108550,
title = {Adaptive-order proximity learning for graph-based clustering},
journal = {Pattern Recognition},
volume = {126},
pages = {108550},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108550},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000310},
author = {Danyang Wu and Wei Chang and Jitao Lu and Feiping Nie and Rong Wang and Xuelong Li},
keywords = {Graph-based clustering, Structured proximity matrix learning, High-order proximity, Adaptive learning},
abstract = {Recently, structured proximity matrix learning, which aims to learn a structured proximity matrix with explicit clustering structures from the first-order proximity matrix, has become the mainstream of graph-based clustering. However, the first-order proximity matrix always lacks several must-links compared to the groundtruth in real-world data, which results in a mismatched problem and affects the clustering performance. To alleviate this problem, this work introduces the high-order proximity to structured proximity matrix learning, and explores a novel framework named Adaptive-Order Proximity Learning (AOPL) to learn a consensus structured proximity matrix from the proximities of multiple orders. To be specific, AOPL selects the appropriate orders first, then assigns weights to these selected orders adaptively. In this way, a consensus structured proximity matrix is learned from the proximity matrices of appropriate orders. Based on AOPL framework, two practical models with different properties are derived, namely AOPL-Root and AOPL-Log. Besides, AOPL and the derived models are regarded as the same optimization problem subjected to some slightly different constraints. An efficient algorithm is proposed to solve them and the corresponding theoretical analyses are provided. Extensive experiments on several real-world datasets demonstrate superb performance of our model.}
}
@article{ZHANG2022108522,
title = {Learning upper patch attention using dual-branch training strategy for masked face recognition},
journal = {Pattern Recognition},
volume = {126},
pages = {108522},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108522},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000036},
author = {Yuxuan Zhang and Xin Wang and M. Saad Shakeel and Hao Wan and Wenxiong Kang},
keywords = {Masked face recognition, Mask-occlusion, Attention module, Dual-branch training strategy},
abstract = {In the context of pandemic, COVID-19, recognition of masked face images is a challenging problem, as most of the facial components become invisible. By utilizing prior information that mask-occlusion is located in the lower half of the face, we propose a dual-branch training strategy to guide the model to focus on the upper half of the face to extract robust features for Masked face recognition (MFR). During training, the features learned at the intermediate layers of the global branch are fed to our proposed attention module, named Upper Patch Attention (UPA), which acts as a local branch. Both branches are jointly optimized to enhance the feature extraction from non-occluded regions. We also propose a self-attention module, which integrates into the backbone network to enhance the interaction between the channels and spatial locations in the learning process. Extensive experiments on synthetic and real-masked face datasets demonstrate the effectiveness of our method.}
}
@article{YANG2022108546,
title = {QuadNet: Quadruplet loss for multi-view learning in baggage re-identification},
journal = {Pattern Recognition},
volume = {126},
pages = {108546},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108546},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000279},
author = {Hao Yang and Xiuxiu Chu and Li Zhang and Yunda Sun and Dong Li and Stephen J. Maybank},
keywords = {Baggage re-identification, Multi-view learning, Quadruplet loss, View-aware features},
abstract = {Recently, baggage re-identification (ReID) has become an attractive topic in computer vision because it plays an important role in intelligent surveillance. However, the wide variations in different views of baggage items degrade baggage ReID performance. In this paper, a novel QuadNet is proposed to solve the multi-view problem in baggage ReID at three levels. At the sample level, we propose a multi-view sampling strategy which samples hard examples from multiple identities in multiple views. The sampled baggage items are used to construct quadruplets. At the feature level, view-aware attentional local features are extracted from discriminative regions in each view. These local features are fused with global features to obtain better representations of the quadruplets. At the loss level, a multi-view quadruplet loss operating on the representations of quadruplets is proposed to reduce the intra-class distances caused by view variations and increase the inter-class distances of baggage images captured in the same view. A random local blur data augmentation is proposed to handle the motion blur which is often found in baggage images. The multi-task learning of materials is introduced to obtain discriminative features based on the materials of baggage surfaces. Extensive experiments on three ReID datasets, MVB, Market-1501 and VeRi-776, indicate the remarkable effectiveness and good generalization of the QuadNet model. It has achieved the state-of-the-art performance on the three datasets.}
}
@article{DU2022108582,
title = {Class-attribute inconsistency learning for novelty detection},
journal = {Pattern Recognition},
volume = {126},
pages = {108582},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108582},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000632},
author = {Shuaiyuan Du and Chaoyi Hong and Yinpeng Chen and Zhiguo Cao and Ziming Zhang},
keywords = {Novelty detection, Class-attribute inconsistency, Class-level similarity, Attribute-level similarity},
abstract = {In this paper, we address the problem of novelty detection whose goal is to recognize instances from unseen classes during testing. Our key idea is to leverage the inconsistency between class similarity and (latent) attribute similarity. We are motivated by the observation that a novel class may holistically appear like a certain known class (class-level reference) but often exhibits unique properties similar to others (attribute-level references). That is, the related class- and attribute-level references are often inconsistent for a novel class. A new two-stage Class-Attribute Inconsistency Learning network (CAILNet) is proposed to explore class-attribute inconsistency for novelty detection. Stage one aims to learn both class and attribute features based on the class labels and fake attribute labels, and stage two aims to search for the corresponding references and make fine-grained comparisons for final novelty decision. Empirically we conduct comprehensive experiments on three benchmark datasets, and demonstrate state-of-the-art performance.}
}
@article{JOHANSSON2022108554,
title = {Rule extraction with guarantees from regression models},
journal = {Pattern Recognition},
volume = {126},
pages = {108554},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108554},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000358},
author = {Ulf Johansson and Cecilia Sönströd and Tuwe Löfström and Henrik Boström},
keywords = {Rule extraction, Interpretability, Conformal prediction, Explainable AI, Predictive regression},
abstract = {Tools for understanding and explaining complex predictive models are critical for user acceptance and trust. One such tool is rule extraction, i.e., approximating opaque models with less powerful but interpretable models. Pedagogical (or black-box) rule extraction, where the interpretable model is induced using the original training instances, but with the predictions from the opaque model as targets, has many advantages compared to the decompositional (white-box) approach. Most importantly, pedagogical methods are agnostic to the kind of opaque model used, and any learning algorithm producing interpretable models can be employed for the learning step. The pedagogical approach has, however, one main problem, clearly limiting its utility. Specifically, while the extracted models are trained to mimic the opaque, there are absolutely no guarantees that this will transfer to novel data. This potentially low test set fidelity must be considered a severe drawback, in particular when the extracted models are used for explanation and analysis. In this paper, a novel approach, solving the problem with test set fidelity by utilizing the conformal prediction framework, is suggested for extracting interpretable regression models from opaque models. The extracted models are standard regression trees, but augmented with valid prediction intervals in the leaves. Depending on the exact setup, the use of conformal prediction guarantees that either the test set fidelity or the test set accuracy will be equal to a preset confidence level, in the long run. In the extensive empirical investigation, using 20 publicly available data sets, the validity of the extracted models is demonstrated. In addition, it is shown how normalization can be used to provide individualized prediction intervals, thus providing highly informative extracted models.}
}
@article{ZHANG2022108590,
title = {Deep knowledge integration of heterogeneous features for domain adaptive SAR target recognition},
journal = {Pattern Recognition},
volume = {126},
pages = {108590},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108590},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000711},
author = {Yukun Zhang and Xiansheng Guo and Lin Li and Nirwan Ansari},
keywords = {Heterogeneous features, Knowledge integration, Domain adaptation, Online learning, Synthetic aperture radar},
abstract = {How to integrate various heterogeneous features for better recognition performance is increasingly critical for automatic target recognition. Existing integration methods present the following drawbacks: (1) most feature integration methods ignore the information, both common and discriminate knowledge, among different types of features; (2) most decision integration methods ignore the fact that different knowledge contributes differently; (3) the feature weights of integration model learned in the source domain cannot perform well in the target domain. To tackle these problems, we propose a deep Knowledge Integration framework by combining heterogeneous features for Domain Adaptive synthetic aperture radar (SAR) target recognition (KIDA). In the training phase, we implement deep knowledge integration at both feature and decision levels. At the feature level, to exploit the common and discriminative knowledge, multiple heterogeneous features are projected from the feature space into a unified label space by exploring the shared and specific structures simultaneously. The shared structure integrates common information in different features, while the specific structure reserves discriminative information of each type of feature. At the decision level, to reveal the relative importance of different knowledge, a decision integration strategy with feature weights is adopted in the label space. In the online testing phase, to improve the generalization of the model in dynamical environments, we employ online learning with sequential target domain knowledge to update the feature weights, thus achieving domain adaptation. Extensive experiments on different datasets validate the effectiveness and advantages of the proposed KIDA, especially in noisy environments.}
}
@article{KAMAL2022108562,
title = {Super-encoder with cooperative autoencoder networks},
journal = {Pattern Recognition},
volume = {126},
pages = {108562},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108562},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000437},
author = {Imam Mustafa Kamal and Hyerim Bae},
keywords = {Autoencoder, Dimensionality reduction, Feature extraction, Pattern recognition, Cooperative neural networks},
abstract = {Dimensionality reduction plays a crucial role in classification, object detection, and pattern recognition tasks. Its main objective is to decrease the dimension of the original data while retaining the most distinctive information. With the emergence of deep learning, an autoencoder has become a state-of-the-art non-linear dimensionality-reduction method. Nonetheless, as the existing autoencoder models are devised to follow the data distribution and employ similarity techniques, preserving distinctive information can be problematic. To tackle this issue, we propose super-encoder (SE) networks trained in a supervised and cooperative manner. The SE consists of an encoder, separator, and decoder networks. The encoder combined with separator networks are dedicated to generating separable latent representation based on the label, and the decoder network should be able to reconstruct it to the original data simultaneously. Herein, we introduce a novel cooperative learning mechanism with a new loss function; therefore, the encoder, separator, and decoder networks can cooperate to achieve these objectives. Extensive experiments using benchmark datasets were conducted. The results indicated that the SE is more effective in extracting separable latent code than the existing supervised and unsupervised dimensionality-reduction models. Furthermore, as a generator, it can obtain highly competitive realistic images.}
}
@article{YUAN2022108541,
title = {Discriminative feature selection with directional outliers correcting for data classification},
journal = {Pattern Recognition},
volume = {126},
pages = {108541},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108541},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200022X},
author = {Lixin Yuan and Guoqiang Yang and Qian Xu and Tong Lu},
keywords = {Feature selection, Directional outlier, Redundant features, Deviation, Supervised method},
abstract = {With the rapid development of multimedia technologies (e.g. deep learning), Feature Selection (FS) is now playing a critical role in acquiring discriminative features from massive data. Traditional FS methods score feature importance and select the top best features by treating all instances equally; Hence, valuable instances like directional outliers (DOs), which are specific outliers closer to other class centres than to their owns, seldom receive particular attention during feature selection. Based on our observation, DOs derive from “misclassified instances” which lead to misclassification. In this paper, we present a novel supervised feature selection method entitled Feature Selection via Directional Outliers Correcting (FSDOC), for accurate data classification. The proposed FSDOC includes an optimization algorithm to capture DOs, and two correcting algorithms to reasonably capture redundant features by correcting DOs with intraclass deviation minimization and interclass relative distance maximization. We give theoretical guarantees and adequate analysis on all algorithms to show the effectiveness of FSDOC. Extensive experiments on fifteen public datasets, and two case studies of deep features and very-high dimensional Fisher Vector selection, demonstrate the superior performance of FSDOC.}
}
@article{HUANG2022108580,
title = {Deep face recognition for dim images},
journal = {Pattern Recognition},
volume = {126},
pages = {108580},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108580},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000619},
author = {Yu-Hsuan Huang and Homer H. Chen},
keywords = {Face recognition, Dim image, Rank-1 identification accuracy, Two-branch network, Convolutional neural network},
abstract = {The performance of many state-of-the-art deep face recognition models deteriorates significantly for images captured under low illumination, mainly because the features of dim probe face images cannot match well with those of normal-illumination gallery images. The issue cannot be satisfactorily addressed by enhancing the illumination of face images and performing face recognition on the resulted images alone. We propose a novel deep face recognition framework that consists of a feature restoration network, a feature extraction network, and an embedding matching module. The feature restoration network adopts a two-branch structure based on the convolutional neural network to generate a feature image from the raw image and the illumination-enhanced image. The feature extraction network encodes the feature image into an embedding, which is then used by the embedding matching module for face verification and identification. The overall verification accuracy is improved from 1.1% to 6.7% when tested on the Specs on Faces (SoF) dataset. For face identification, the rank-1 identification accuracy is improved by 2.8%.}
}
@article{AMEN2022108404,
title = {Big data directed acyclic graph model for real-time COVID-19 twitter stream detection},
journal = {Pattern Recognition},
volume = {123},
pages = {108404},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108404},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100580X},
author = {Bakhtiar Amen and Syahirul Faiz and Thanh-Toan Do},
keywords = {Anomaly detection, Big data, COVID-19, Directed acyclic graph, Event stream},
abstract = {Every day, large-scale data are continuously generated on social media as streams, such as Twitter, which inform us about all events around the world in real-time. Notably, Twitter is one of the effective platforms to update countries leaders and scientists during the coronavirus (COVID-19) pandemic. Other people have also used this platform to post their concerns about the spread of this virus and a rapid increase of death cases globally. The aim of this work is to detect anomalous events associated with COVID-19 from Twitter. To this end, we propose a distributed Directed Acyclic Graph topology framework to aggregate and process large-scale real-time tweets related to COVID-19. The core of our system is a novel lightweight algorithm that can automatically detect anomaly events. In addition, our system can also identify, cluster, and visualize important keywords in tweets. On 18 August 2020, our model detected the highest anomaly since many tweets mentioned the casualties’ updates and the debates on the pandemic that day. We obtained the three most commonly listed terms on Twitter: “covid”, “death”, and “Trump” (21,566, 11,779, and 4761 occurrences, respectively), with the highest TF-IDF score for these terms: “people” (0.63637), “school” (0.5921407) and “virus” (0.57385). From our clustering result, the word “death”, “corona”, and “case” are grouped into one cluster, where the word “pandemic”, “school”, and “president” are grouped as another cluster. These terms were located near each other on vector space so that they were clustered, indicating people’s most concerned topics on Twitter.}
}
@article{NIU2022108396,
title = {Defect attention template generation cycleGAN for weakly supervised surface defect segmentation},
journal = {Pattern Recognition},
volume = {123},
pages = {108396},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108396},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005616},
author = {Shuanlong Niu and Bin Li and Xinggang Wang and Songping He and Yaru Peng},
keywords = {Weakly supervised learning, Defect detection, Image segmentation, Generative adversarial network (GAN), Attention model},
abstract = {Surface defect segmentation is very important for the quality inspection of industrial production and is an important pattern recognition problem. Although deep learning (DL) has achieved remarkable results in surface defect segmentation, most of these results have been obtained by using massive images with pixel-level annotations, which are difficult to obtain at industrial sites. This paper proposes a weakly supervised defect segmentation method based on the dynamic templates generated by an improved cycle-consistent generative adversarial network (CycleGAN) trained by image-level annotations. To generate better templates for defects with weak signals, we propose a defect attention module by applying the defect residual for the discriminator to strengthen the elimination of defect regions and suppress changes in the background. A defect cycle-consistent loss is designed by adding structural similarity (SSIM) to the original L1 loss to include the grayscale and structural features; the proposed loss can better model the inner structure of defects. After obtaining the defect-free template, a defect segmentation map can easily be obtained through a simple image comparison and threshold segmentation. Experiments show that the proposed method is both efficient and effective, significantly outperforms other weakly supervised methods, and achieves performance that is comparable or even superior to that of supervised methods on three industrial datasets (intersection over union (IoU) on the DAGM 2007, KSD and CCSD datasets of 78.28%, 59.43%,and 68.83%, respectively). The proposed method can also be employed as a semiautomatic annotation tool combined with active learning.}
}
@article{BENYOUNES2022108421,
title = {Driving behavior explanation with multi-level fusion},
journal = {Pattern Recognition},
volume = {123},
pages = {108421},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108421},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005975},
author = {Hédi Ben-Younes and Éloi Zablocki and Patrick Pérez and Matthieu Cord},
keywords = {Explainable self-driving, Multi-level fusion, Cause classification, Natural language explanations, HDD, BDD-X},
abstract = {In this era of active development of autonomous vehicles, it becomes crucial to provide driving systems with the capacity to explain their decisions. In this work, we focus on generating high-level driving explanations as the vehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep architecture which explains the behavior of a trajectory prediction model. Supervised by annotations of human driving decisions justifications, BEEF learns to fuse features from multiple levels. Leveraging recent advances in the multi-modal fusion literature, BEEF is carefully designed to model the correlations between high-level decisions features and mid-level perceptual features. The flexibility and efficiency of our approach are validated with extensive experiments on the HDD and BDD-X datasets.}
}
@article{ZHANG2022108365,
title = {A polarization fusion network with geometric feature embedding for SAR ship classification},
journal = {Pattern Recognition},
volume = {123},
pages = {108365},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108365},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005458},
author = {Tianwen Zhang and Xiaoling Zhang},
keywords = {Synthetic aperture radar (SAR), Ship classification, Convolutional neural network, Polarization fusion (PF), Geometric feature embedding (GFE)},
abstract = {Current synthetic aperture radar (SAR) ship classifiers using convolutional neural networks (CNNs) offer state-of-the-art performance. Yet, they still have two defects potentially hindering accuracy progress – polarization insufficient utilization and traditional feature abandonment. Therefore, we propose a polarization fusion network with geometric feature embedding (PFGFE-Net) to solve them. PFGFE-Net achieves the polarization fusion (PF) from the input data, feature-level, and decision-level. Moreover, the geometric feature embedding (GFE) enriches expert experience. Results on OpenSARShip reveal PFGFE-Net's excellent performance.}
}
@article{LI2022108405,
title = {A robust and efficient fingerprint image restoration method based on a phase-field model},
journal = {Pattern Recognition},
volume = {123},
pages = {108405},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108405},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005811},
author = {Yibao Li and Qing Xia and Chaeyoung Lee and Sangkwon Kim and Junseok Kim},
keywords = {Fingerprint restoration, Diblock copolymer, Nonlocal Cahn–Hilliard equation},
abstract = {In this study, we present a robust and efficient fingerprint image restoration algorithm using the nonlocal Cahn–Hilliard (CH) equation, which was proposed for modeling the microphase separation of diblock copolymers. We take a small local region embedding the damaged domain and solve the nonlocal CH equation to restore the fingerprint image. A Gauss–Seidel type iterative method, which is efficient and simple to implement, is used. The proposed method has the advantage in that the pixel values in the damaged fingerprint domain can be obtained using the image information from the outside of the damaged fingerprint region. Fingerprint restoration based on adjacent pixel information can ensure the accuracy of the fingerprint information with a low computational cost. Computational experiments demonstrated the superior performance of the proposed fingerprint restoration algorithm.}
}
@article{WANG2022108373,
title = {A coarse-to-fine approach for dynamic-to-static image translation},
journal = {Pattern Recognition},
volume = {123},
pages = {108373},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108373},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005537},
author = {Teng Wang and Lin Wu and Changyin Sun},
keywords = {Dynamic-to-static image translation, Shadow detection, Attention mechanism, Visual place recognition},
abstract = {Dynamic-to-static image translation aims to convert the dynamic scene into static so that dynamic elements are eliminated from the image. Recent works typically see the problem as an image-to-image translation task, and perform the learned feature mapping over the whole dynamic image to synthesize the static image, which leads to unnecessary detail loss in original static regions. To that end, we delicately formulate it as an image inpainting-like problem to fill the missing static pixels in dynamic regions while retaining original static regions. We achieve this by proposing a coarse-to-fine framework. At coarse stage, we utilize a simple encoder-decoder network to rough out the static image. Using the coarse predicted image, we explicitly infer a more accurate dynamic mask to identify both dynamic objects and their shadows, so that the task could be effectively converted to an image inpainting problem. At fine stage, we recover the missing static pixels in the estimated dynamic regions on the basis of their coarse predictions. We enhance the coarse predicted contents by proposing a mutual texture-structure attention module, which enables the dynamic regions to borrow textures and structures separately from distant locations based on contextual similarity. Several losses are combined as the training objective function to generate excellent results with global consistency and fine details. Qualitative and quantitative experiments verify the superiority of our method in restoring high-quality static contents over state-of-the-art models. In addition, we evaluate the usefulness of the recovered static images by using them as query images to improve visual place recognition in dynamic scenes.}
}
@article{HUANG2022108352,
title = {Unified curiosity-Driven learning with smoothed intrinsic reward estimation},
journal = {Pattern Recognition},
volume = {123},
pages = {108352},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108352},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100532X},
author = {Fuxian Huang and Weichao Li and Jiabao Cui and Yongjian Fu and Xi Li},
keywords = {Reinforcement learning, Unified curiosity-driven exploration, Robust intrinsic reward, Task-relevant feature},
abstract = {In reinforcement learning (RL), the intrinsic reward estimation is necessary for policy learning when the extrinsic reward is sparse or absent. To this end, Unified Curiosity-driven Learning with Smoothed intrinsic reward Estimation (UCLSE) is proposed to address the sparse extrinsic reward problem from the perspective of completeness of intrinsic reward estimation. We further propose state distribution-aware weighting method and policy-aware weighting method to dynamically unify two mainstream intrinsic reward estimation methods. In this way, the agent can explore the environment more effectively and efficiently. Under this framework, we propose to employ an attention module to extract task-relevant features for a more precise estimation of intrinsic reward. Moreover, we propose to improve the robustness of policy learning by smoothing the intrinsic reward with a batch of transitions close to the current transition. Extensive experimental results on Atari games demonstrate that our method outperforms the state-of-the-art approaches in terms of both score and training efficiency.}
}
@article{ZHANG2022108385,
title = {Multi-scale signed recurrence plot based time series classification using inception architectural networks},
journal = {Pattern Recognition},
volume = {123},
pages = {108385},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108385},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005653},
author = {Ye Zhang and Yi Hou and Kewei OuYang and Shilin Zhou},
keywords = {Time series classification, Multi-scale, Signed, Recurrence plots, Inception network},
abstract = {Inspired by the great success of deep neural networks in image classification, recent works use Recurrence Plots (RP) to encode time series as images for classification. RP provide rich texture information and construct long-term time correlations, which are effective supplements to the networks. However, RP cannot handle the scale and length variability of sequences. Moreover, RP have serious tendency confusion problem. They cannot represent the upward and downward trends of sequences effectively. In addition to the defects of RP, existing time series classification (TSC) networks cannot adapt to the various scales of discriminative regions of time series effectively. To tackle these problems, this paper proposes a method, named MSRP-IFCN. It is composed of two submodules, the Multi-scale Signed RP (MSRP) and the Inception Fully Convolutional Network (IFCN). MSRP are proposed to handle the defects of RP. They comprise three components, namely the multi-scale RP, the asymmetric RP and the signed RP. We first use the multi-scale RP to enrich the scales of images. Then, the asymmetric RP are constructed to represent long sequences. Finally, the signed RP images are obtained by multiplying the designed sign masks to remove the tendency confusion. Besides, IFCN is proposed to enhance the existing TSC networks in multi-scale feature extraction. By introducing the modified Inception modules, IFCN obtains extensive receptive fields and better extracts multi-scale features from the MSRP images. Experimental results on 85 UCR datasets indicate the superior performance of MSRP-IFCN. The visualization results further demonstrate the effectiveness of our method.}
}
@article{ZHANG2022108394,
title = {Learning directly from synthetic point clouds for “in-the-wild” 3D face recognition},
journal = {Pattern Recognition},
volume = {123},
pages = {108394},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108394},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005732},
author = {Ziyu Zhang and Feipeng Da and Yi Yu},
keywords = {3D face recognition, Learning from synthetic, Curvature-aware point sampling, Transfer learning},
abstract = {Point clouds-based networks have achieved great attention in 3D object classification, segmentation, and indoor scene semantic parsing, but its application to 3D face recognition is still underdeveloped owing to two main reasons: lack of large-scale 3D facial data and absence of deep neural network that can directly extract discriminative face representations from point clouds. To address these two problems, a PointNet++ based network is proposed in this paper to extract face features directly from point clouds facial scans and a statistical 3D Morphable Model based 3D face synthesizing strategy is established to generate large-scale unreal facial scans to train the proposed network from scratch. A curvature-aware point sampling technique is proposed to hierarchically down-sample feature-sensitive points which are crucial to pass and aggregate discriminative facial features deeply. In addition, a novel 3D face transfer learning method is proposed to ease the domain discrepancy between synthetic and ‘in-the-wild’ faces. Experimental results on two public 3D face benchmarks show that the network trained only on synthesized data can also be well generalized to ‘in-the-wild’ 3D face recognition. Our method achieves the state-of-the-art results by achieving an overall rank-1 identification rate of 99.46% and 99.65% on FRGCv2 and Bosphorus, respectively. Further, we evaluate on a self-collected dataset to demonstrate the robustness and application potential of our method.}
}
@article{KALNISHKAN2022108557,
title = {Prediction with expert advice for a finite number of experts: A practical introduction},
journal = {Pattern Recognition},
volume = {126},
pages = {108557},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108557},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000383},
author = {Yuri Kalnishkan},
keywords = {Online learning, Prediction, Model selection},
abstract = {In this paper, prediction with expert advice is surveyed focusing on Vovk’s Aggregating Algorithm. The established theory as well as extensions developed in the recent decade are considered. The paper is aimed at practitioners and covers important application scenarios.}
}
@article{YAO2022108369,
title = {ADCNN: Towards learning adaptive dilation for convolutional neural networks},
journal = {Pattern Recognition},
volume = {123},
pages = {108369},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108369},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005495},
author = {Jie Yao and Dongdong Wang and Hao Hu and Weiwei Xing and Liqiang Wang},
keywords = {Adaptive dilated convolution, Representation learning, Image classification},
abstract = {Dilated convolution kernels are constrained by their shared dilation, keeping them from being aware of diverse spatial contents at different locations. We address such limitations by formulating the dilation as trainable weights with respect to individual positions. We propose Adaptive Dilation Convolutional Neural Networks (ADCNN), a light-weighted extension that allows convolutional kernels to adjust their dilation value based on different contents at the pixel level. Unlike previous content-adaptive models, ADCNN dynamically infers pixel-wise dilation via modeling feed-forward inter-patterns, which provides a new perspective for developing adaptive network structures other than sampling kernel spaces. Our evaluation results indicate ADCNNs can be easily integrated into various backbone networks and consistently outperform their regular counterparts on various visual tasks.}
}
@article{HUANG2022108353,
title = {Learning to select cuts for efficient mixed-integer programming},
journal = {Pattern Recognition},
volume = {123},
pages = {108353},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108353},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005331},
author = {Zeren Huang and Kerong Wang and Furui Liu and Hui-Ling Zhen and Weinan Zhang and Mingxuan Yuan and Jianye Hao and Yong Yu and Jun Wang},
keywords = {Mixed-Integer programming, Cutting plane, Multiple instance learning, Generalization ability},
abstract = {Cutting plane methods play a significant role in modern solvers for tackling mixed-integer programming (MIP) problems. Proper selection of cuts would remove infeasible solutions in the early stage, thus largely reducing the computational burden without hurting the solution accuracy. However, the major cut selection approaches heavily rely on heuristics, which strongly depend on the specific problem at hand and thus limit their generalization capability. In this paper, we propose a data-driven and generalizable cut selection approach, named Cut Ranking, in the settings of multiple instance learning. To measure the quality of the candidate cuts, a scoring function, which takes the instance-specific cut features as inputs, is trained and applied in cut ranking and selection. In order to evaluate our method, we conduct extensive experiments on both synthetic datasets and real-world datasets. Compared with commonly used heuristics for cut selection, the learning-based policy has shown to be more effective, and is capable of generalizing over multiple problems with different properties. Cut Ranking has been deployed in an industrial solver for large-scale MIPs. In the online A/B testing of the product planning problems with more than 107 variables and constraints daily, Cut Ranking has achieved the average speedup ratio of 12.42% over the production solver without any accuracy loss of solution.}
}
@article{ZHANG2022108565,
title = {Split, Embed and Merge: An accurate table structure recognizer},
journal = {Pattern Recognition},
volume = {126},
pages = {108565},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108565},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000462},
author = {Zhenrong Zhang and Jianshu Zhang and Jun Du and Fengren Wang},
keywords = {Table structure recognition, Self-regression, Attention mechanism, Encoder-decoder},
abstract = {Table structure recognition is an essential part for making machines understand tables. Its main task is to recognize the internal structure of a table. However, due to the complexity and diversity in their structure and style, it is very difficult to parse the tabular data into the structured format which machines can understand, especially for complex tables. In this paper, we introduce Split, Embed and Merge (SEM), an accurate table structure recognizer. SEM is mainly composed of three parts, splitter, embedder and merger. In the first stage, we apply the splitter to predict the potential regions of the table row/column separators, and obtain the fine grid structure of the table. In the second stage, by taking a full consideration of the textual information in the table, we fuse the output features for each table grid from both vision and text modalities. Moreover, we achieve a higher precision in our experiments through providing additional textual features. Finally, we process the merging of these basic table grids in a self-regression manner. The corresponding merging results are learned through the attention mechanism. In our experiments, SEM achieves an average F1-Measure of 97.11% on the SciTSR dataset which outperforms other methods by a large margin. We also won the first place of complex tables and third place of all tables in Task-B of ICDAR 2021 Competition on Scientific Literature Parsing. Extensive experiments on other publicly available datasets further demonstrate the effectiveness of our proposed approach.}
}
@article{YANG2022108377,
title = {Rain-component-aware capsule-GAN for single image de-raining},
journal = {Pattern Recognition},
volume = {123},
pages = {108377},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108377},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005574},
author = {Fei Yang and Jianfeng Ren and Zheng Lu and Jialu Zhang and Qian Zhang},
keywords = {De-raining, Capsule, Generative adversarial network, Rain-component-aware network},
abstract = {Images taken in the rain usually have poor visual quality, which may cause difficulties for vision-based analysis systems. The research aims to recover clean image content from a single rainy image by removing rain components without introducing any artifacts. Existing rain removal methods often model the rain component as noise, but it obviously has clear patterns instead of random noise. Motivated by this, we raise the idea to build modules to capture rain patterns for de-raining. A Rain-Component-Aware (RCA) network is proposed to capture the characteristics of the rain. We then integrate it into an image-conditioned generative adversarial network (image-cGAN) as a RCA loss to guide the generation of rainless images. This results in the proposed two-branch cGAN, where one branch aims at improving the image visual quality after de-raining, and the other aims at extracting rain patterns so that the rain could be effectively removed. To better capture the spatial relationship of different objects within an image, we incorporate the capsule structure in both generator and discriminator of cGAN, which further improves the quality of generated images. The proposed approach is hence named as RCA-cGAN. Benefited by the RCA loss based two-branch optimization and the capsule structure, RCA-cGAN achieves good de-raining effect. Extensive experimental results on several benchmark datasets show that the RCA network is effective to capture rain patterns and the proposed approach could produce much better de-raining images in terms of both subjective visual quality inspection and objective quantitative assessment.}
}
@article{YANG2022108545,
title = {Human-Centric Image Captioning},
journal = {Pattern Recognition},
volume = {126},
pages = {108545},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108545},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000267},
author = {Zuopeng Yang and Pengbo Wang and Tianshu Chu and Jie Yang},
keywords = {Human-centric, Image captioning, Feature hierarchization},
abstract = {In this paper, we propose a new topic, Human-Centric Captioning, to mainly describe the human behavior in an image. Human activities and relationships are the primary objectives of visual understanding in daily applications. However, existing image captioning systems cannot differently treat humans and other objects, which limits the ability to understand and describe diverse human activities. As the first explorer of this new task, we build a novel Human-Centric COCO dataset concentrating on humans. Accordingly, we propose a novel Human-Centric Captioning Model (HCCM) that focuses on human-centric feature hierarchization and sentence generation. Specifically, our model first utilizes human body part-level knowledge to hierarchize the image features and then applies a novel three-branch captioning model to process these hierarchical features independently to calibrate the descriptions of human actions. Comprehensive experiments demonstrate that our HCCM achieves the state-of-the-art performance with BLEU-4, CIDEr and SPICE scores of 41.5, 127.3, 23.5 respectively. Dataset and code are publicly available at https://github.com/JohnDreamer/HCCM/.}
}
@article{YU2022108581,
title = {Structure-aware conditional variational auto-encoder for constrained molecule optimization},
journal = {Pattern Recognition},
volume = {126},
pages = {108581},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108581},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000620},
author = {Junchi Yu and Tingyang Xu and Yu Rong and Junzhou Huang and Ran He},
keywords = {Molecule optimization, Conditional generation, Drug discovery},
abstract = {The goal of molecule optimization is to optimize molecular properties by modifying molecule structures. Conditional generative models provide a promising way to transfer the input molecules to the ones with better property. However, molecular properties are highly sensitive to small changes in molecular structures. This leads to an interesting thought that we can improve the property of molecules with limited modification in structure. In this paper, we propose a structure-aware conditional Variational Auto-Encoder, namely SCVAE, which exploits the topology of molecules as structure condition and optimizes the molecular properties with constrained structural modification. SCVAE leverages graph alignment of two-level molecule structures in an unsupervised manner to bind the structure conditions between two molecules. Then, this structure condition facilitates the molecule optimization with limited structural modification, namely, constrained molecule optimization, under a novel variational auto-encoder framework. Extensive experimental evaluations demonstrate that structure-aware CVAE generates new molecules with high similarity to the original ones and better molecular properties.}
}
@article{GIORGINIS2022108553,
title = {Fast data reduction by space partitioning via convex hull and MBR computation},
journal = {Pattern Recognition},
volume = {126},
pages = {108553},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108553},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000346},
author = {Thomas Giorginis and Stefanos Ougiaroglou and Georgios Evangelidis and Dimitris A. Dervos},
keywords = {Reduction by space partitioning, RSP3, Classification, Prototype generation, Big training data, Convex hull, Minimum bounding rectangle (MBR)},
abstract = {Large volumes of training data introduce high computational cost in instance-based classification. Data reduction algorithms select or generate a small (condensing) set of representative training prototypes from the available training data. The Reduction by Space Partitioning algorithm is one of the most well-known prototype generation algorithms that repetitively divides the original training data into subsets. This partitioning process needs to identify the diameter of each subset, i.e., its two farthest instances. This is a costly process since it requires the calculation of all distances between the instances in each subset. The paper introduces two new very fast variations that, instead of computing the actual diameter of a subset, choose a pair of distant-enough instances. The first variation uses instances belonging to an exact 3d convex hull of the subset, while the second one uses instances belonging to the minimum bounding rectangle of the subset. Our experimental study shows that the new variations vastly outperform the original algorithm without a penalty in classification accuracy and reduction rate.}
}
@article{SUN2022108577,
title = {Video super-resolution via mixed spatial-temporal convolution and selective fusion},
journal = {Pattern Recognition},
volume = {126},
pages = {108577},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108577},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000589},
author = {Wei Sun and Dong Gong and Javen Qinfeng Shi and Anton {van den Hengel} and Yanning Zhang},
keywords = {Video super-Resolution, Mixed spatial-Temporal convolution, Selective feature fusion},
abstract = {Video super-resolution aims to recover the high-resolution (HR) contents from the low-resolution (LR) observations relying on compositing the spatial-temporal information in the LR frames. It is crucial to model the spatial-temporal information jointly since the video sequences are three-dimensional spatial-temporal signals. Compared with explicitly estimating motions between the 2D frames, 3D convolutional neural networks (CNNs) have been shown its efficiency and effectiveness for video super-resolution (SR), as a natural way of spatial-temporal data modelling. Though promising, the performance of 3D CNNs is still far from satisfactory. The high computational and memory requirements limit the development of more advanced designs to extract and fuse the information from a larger spatial and temporal scale. We thus propose a Mixed Spatial-Temporal Convolution (MSTC) block that simultaneously extracts the spatial information and the supplemented temporal dependency among frames by jointly applying 2D and 3D convolution. To further fuse the learned features corresponding to different frames, we propose a novel similarity-based selective features strategy, unlike precious methods directly stacking the learned features. Additionally, an attention-based motion compensation module is applied to alleviate the influence of misalignment between frames. Experiments on three widely used benchmark datasets and real-world dataset show that, relying on superior feature extraction and fusion ability, the proposed network can outperform previous state-of-the-art methods, especially for recovering the confusing details.}
}
@article{GAMMERMAN2022108561,
title = {Special Issue on Conformal and Probabilistic Prediction with Applications: Preface},
journal = {Pattern Recognition},
volume = {126},
pages = {108561},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108561},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000425},
author = {Alexander Gammerman and Vladimir Vovk and Marco Cristani}
}
@article{LI2022108585,
title = {Enhanced nuclear norm based matrix regression for occluded face recognition},
journal = {Pattern Recognition},
volume = {126},
pages = {108585},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108585},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000668},
author = {Qin Li and Huihui He and Hong Lai and Tie Cai and Qianqian Wang and QuanXue Gao},
keywords = {Face recognition, Occluded image, Nuclear norm, Low-Rank},
abstract = {An effective approach for the task of face recognition is proposed in this paper, which formulates the problem as an enhanced nuclear norm based matrix regression model and explores the low-rank property of the reconstructed image. Previous works have already leveraged the nuclear norm to obtain a low-rank representation of the error image and get a promising recognition rate. Motivated by the low-rank property of the reconstructed image through theoretical observation, our model imposes the nuclear norm constraints not only on the representation residual but also on the reconstructed image. The proposed method preserves the 2D structural information of the error images and reconstructs images, which is significant for the face recognition tasks. To further improve the performance of the proposed model, we explore the impact of different regularization terms under various scenarios. Extensive experiments on several benchmark datasets show the efficacy of the proposed model especially in terms of robustness against contiguous occlusion and illumination changes, which achieves superior performance over the most competitive methods.}
}
@article{LU2022108569,
title = {Transferring discriminative knowledge via connective momentum clustering on person re-identification},
journal = {Pattern Recognition},
volume = {126},
pages = {108569},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108569},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000504},
author = {Yichen Lu and Weihong Deng},
keywords = {Person re-identification, Unsupervised domain adaptation, Graph convolutional networks, Momentum mechanism, Batch normalization},
abstract = {Unsupervised domain adaptation in person re-identification remains a challenge to learning discriminative representations due to the absence of labels in target domain. Clustering could provide pseudo-labels, but the limitation mainly comes from imperfect clustering and noisy pseudo-labels. To address this drawback, we propose Connective Momentum Clustering (CMC) framework to build a connection estimator via graph convolutional networks to transfer rich connection knowledge from the annotation space of source data to target domain. It estimates connections from context to reveal relationship between unlabeled data and helps to discover more reliable clusters. With momentum mechanism, stable pseudo-labels are updated iteratively with confidence and refined consistently to encourage more discriminative networks. Meanwhile, we notice that the huge domain gap between source and target domains results in severe pollution in BatchNorm layers. To tackle this problem, we normalize the data stream separately to decouple different distribution and further boost the performance in target domain. We adopt our CMC framework on mainstream tasks and achieves 80.2% mAP / 91.3% Rank-1 on Duke→Market task and 70.4% mAP / 82.4% Rank-1 on Market→Duke task.}
}
@article{YANG2022108549,
title = {SDUNet: Road extraction via spatial enhanced and densely connected UNet},
journal = {Pattern Recognition},
volume = {126},
pages = {108549},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108549},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000309},
author = {Mengxing Yang and Yuan Yuan and Ganchao Liu},
keywords = {Road extraction, Image segmentation, Remote sensing imagery, Spatial topology},
abstract = {Extracting road maps from high-resolution optical remote sensing images has received much attention recently, especially with the rapid development of deep learning methods. However, most of these CNN based approaches simply focused on multi-scale encoder architectures or multiple branches in neural networks, and ignored some inherent characteristics of the road surface. In this paper, we design a novel network for road extraction based on spatial enhanced and densely connected UNet, called SDUNet. SDUNet aggregates both the multi-level features and global prior information of road networks by combining the strengths of spatial CNN-based segmentation and densely connected blocks. To enhance the feature learning about prior information of road surface, a structure preserving model is designed to explore the continuous clues in the spatial level. Experimental results on two benchmark datasets show that the proposed method achieves the state-of-the-art performance, compared with previous approaches for road extraction. Code will be made available on https://github.com/MrStrangerYang/SDUNet.}
}
@article{WANG2022108579,
title = {Low-resolution human pose estimation},
journal = {Pattern Recognition},
volume = {126},
pages = {108579},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108579},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000607},
author = {Chen Wang and Feng Zhang and Xiatian Zhu and Shuzhi Sam Ge},
keywords = {Human pose estimation, Low resolution image, Heatmap learning, Offset learning, Quantization error},
abstract = {Human pose estimation has achieved significant progress on images with high imaging resolution. However, low-resolution imagery data bring nontrivial challenges which are still under-studied. To fill this gap, we start with investigating existing methods and reveal that the most dominant heatmap-based methods would suffer more severe model performance degradation from low-resolution, and offset learning is an effective strategy. Established on this observation, in this work we propose a novel Confidence-Aware Learning (CAL) method which further addresses two fundamental limitations of existing offset learning methods: inconsistent training and testing, decoupled heatmap and offset learning. Specifically, CAL selectively weighs the learning of heatmap and offset with respect to ground-truth and most confident prediction, whilst capturing the statistical importance of model output in mini-batch learning manner. Extensive experiments conducted on the COCO benchmark show that our method outperforms significantly the state-of-the-art methods for low-resolution human pose estimation.}
}
@article{CHEN2022108383,
title = {Majorities help minorities: Hierarchical structure guided transfer learning for few-shot fault recognition},
journal = {Pattern Recognition},
volume = {123},
pages = {108383},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108383},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100563X},
author = {Hao Chen and Ruonan Liu and Zongxia Xie and Qinghua Hu and Jianhua Dai and Junhai Zhai},
keywords = {Transfer learning, Fault recognition, Few-shot problem, Hierarchical category structure, Complex systems},
abstract = {To ensure the operational safety and reliability, fault recognition of complex systems is becoming an essential process in industrial systems. However, the existing recognition methods mainly focus on common faults with enough data, which ignore that many faults are lack of samples in engineering practice. Transfer learning can be helpful, but irrelevant knowledge transfer can cause performance degradation, especially in complex systems. To address the above problem, a hierarchy guided transfer learning framework (HGTL) is proposed in this paper for fault recognition with few-shot samples. Firstly, we fuse domain knowledge, label semantics and inter-class distance to calculate the affinity between categories, based on which a category hierarchical tree is constructed by hierarchical clustering. Then, guided by the hierarchical structure, the samples in most similar majority classes are selected from the source domain to pre-train the hierarchical feature learning network (HFN) and extract the transferable fault information. For the fault knowledge extracted from the child nodes of one parent node are similar and can be transferred with each other, so the trained HFN can extract better features of few samples classes with the help of the information from similar faults, and used to address few-shot fault recognition problems. Finally, a dataset of a nuclear power system with 65 categories and the widely used Tennessee Eastman dataset are analyzed respectively via the proposed method, as well as state-of-the-art recognition methods for comparison. The experimental results demonstrate the effectiveness and superiority of the proposed method in fault recognition with few-shot problem.}
}
@article{WANG2022108416,
title = {Query Pixel Guided Stroke Extraction with Model-Based Matching for Offline Handwritten Chinese Characters},
journal = {Pattern Recognition},
volume = {123},
pages = {108416},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108416},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005926},
author = {Tie-Qiang Wang and Xiaoyi Jiang and Cheng-Lin Liu},
keywords = {Stroke extraction, Conditional fully convolutional network, PathNet, Stroke matching, Tree search},
abstract = {Stroke extraction and matching are critical for structural interpretation based applications of handwritten Chinese characters, such as Chinese character education and calligraphy analysis. Stroke extraction from offline handwritten Chinese characters is difficult because of the missing of temporal information, the multi-stroke structures and the distortion of handwritten shapes. In this paper, we propose a comprehensive scheme for solving the stroke extraction problem for handwritten Chinese characters. The method consists of three main steps: (1) fully convolutional network (FCN) based skeletonization; (2) query pixel guided stroke extraction; (3) model-based stroke matching. Specifically, based on a recently proposed architecture of FCN, both the stroke skeletons and cross regions are firstly extracted from the character image by the proposed SkeNet and CrossNet, respectively. Stroke extraction is solved by simulating the human perception that once given a certain pixel from non-cross region of a stroke, the whole stroke containing the pixel can be traced. To realize this idea, we formulate stroke extraction as a problem of pairing and connecting skeleton-wise stroke segments which are adjacent to the same cross region, where the pairing consistency between stroke segments is measured using a PathNet [1]. To reduce the ambiguity of stroke extraction, the extracted candidate strokes are matched with a character model consisting of standard strokes by tree search to identify the correct strokes. For verifying the effectiveness of the proposed method, we train and test our models on character images with stroke segmentation annotations generated from the online handwriting datasets CASIA-OLHWDB and ICDAR13-Online, as well as a dataset of Regularly-Written online handwritten characters (RW-OLHWDB). The experimental results demonstrate the effectiveness of the proposed method and provide several benchmarks. Particularly, the precisions of stroke extraction for ICDAR13-Online and RW-OLHWDB are 89.0% and 94.9%, respectively.}
}
@article{SU2022108372,
title = {DLA-Net: Learning dual local attention features for semantic segmentation of large-scale building facade point clouds},
journal = {Pattern Recognition},
volume = {123},
pages = {108372},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108372},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005525},
author = {Yanfei Su and Weiquan Liu and Zhimin Yuan and Ming Cheng and Zhihong Zhang and Xuelun Shen and Cheng Wang},
keywords = {Semantic segmentation, Building facade, Self-attention, Attentive pooling, DLA-Net},
abstract = {The semantic segmentation of building facades is critical for various construction applications, such as urban building reconstruction and damage assessments. As there is a lack of 3D point cloud datasets related to fine-grained building facades, in this work we construct the first large-scale point cloud benchmark dataset for building facade semantic segmentation. In terms of the characteristics of building facade dataset, the existing methods of semantic segmentation cannot fully mine the local neighborhood information of point clouds; therefore, we propose an attention module that learns Dual Local Attention features, called DLA in this paper. The proposed DLA module consists of two blocks, a self-attention block and an attentive pooling block, which both embed an enhanced position encoding block. The DLA module can be easily embedded into various network architectures for point cloud segmentation, naturally resulting in a new 3D semantic segmentation network with an encoder-decoder architecture; we called this network the DLA-Net. Extensive experimental results on our constructed building facade dataset demonstrate that the proposed DLA-Net achieves better performance than the state-of-the-art methods for semantic segmentation.}
}
@article{PRIJATELJ2022108395,
title = {A Bayesian evaluation framework for subjectively annotated visual recognition tasks},
journal = {Pattern Recognition},
volume = {123},
pages = {108395},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108395},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005604},
author = {Derek S. Prijatelj and Mel McCurrie and Samuel E. Anthony and Walter J. Scheirer},
keywords = {Uncertainty estimation, Epistemic uncertainty, Supervised learning, Bayesian inference, Bayesian modeling},
abstract = {An interesting development in automatic visual recognition has been the emergence of tasks where it is not possible to assign objective labels to images, yet still feasible to collect annotations that reflect human judgements about them. Machine learning-based predictors for these tasks rely on supervised training that models the behavior of the annotators, i.e., what would the average person’s judgement be for an image? A key open question for this type of work, especially for applications where inconsistency with human behavior can lead to ethical lapses, is how to evaluate the epistemic uncertainty of trained predictors, i.e., the uncertainty that comes from the predictor’s model. We propose a Bayesian framework for evaluating black box predictors in this regime, agnostic to the predictor’s internal structure. The framework specifies how to estimate the epistemic uncertainty that comes from the predictor with respect to human labels by approximating a conditional distribution and producing a credible interval for the predictions and their measures of performance. The framework is successfully applied to four image classification tasks that use subjective human judgements: facial beauty assessment, social attribute assignment, apparent age estimation, and ambiguous scene labeling.}
}
@article{HUANG2022108384,
title = {Multi-level adversarial network for domain adaptive semantic segmentation},
journal = {Pattern Recognition},
volume = {123},
pages = {108384},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108384},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005641},
author = {Jiaxing Huang and Dayan Guan and Aoran Xiao and Shijian Lu},
keywords = {Unsupervised domain adaptation, Semantic segmentation, Adversarial learning, Self training},
abstract = {Recent progresses in domain adaptive semantic segmentation demonstrate the effectiveness of adversarial learning (AL) in unsupervised domain adaptation. However, most adversarial learning based methods align source and target distributions at a global image level but neglect the inconsistency around local image regions. This paper presents a novel multi-level adversarial network (MLAN) that aims to address inter-domain inconsistency at both global image level and local region level optimally. MLAN has two novel designs, namely, region-level adversarial learning (RL-AL) and co-regularized adversarial learning (CR-AL). Specifically, RL-AL models prototypical regional context-relations explicitly in the feature space of a labelled source domain and transfers them to an unlabelled target domain via adversarial learning. CR-AL fuses region-level AL and image-level AL optimally via mutual regularization. In addition, we design a multi-level consistency map that can guide domain adaptation in both input space (i.e., image-to-image translation) and output space (i.e., self-training) effectively. Extensive experiments show that MLAN outperforms the state-of-the-art with a large margin consistently across multiple datasets.}
}
@article{HORANYI2022108485,
title = {Repurposing existing deep networks for caption and aesthetic-guided image cropping},
journal = {Pattern Recognition},
volume = {126},
pages = {108485},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108485},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006610},
author = {Nora Horanyi and Kedi Xia and Kwang Moo Yi and Abhishake Kumar Bojja and Aleš Leonardis and Hyung Jin Chang},
keywords = {Image cropping, Aesthetics, Deep network re-purposing, Image captioning},
abstract = {We propose a novel optimization framework that crops a given image based on user description and aesthetics. Unlike existing image cropping methods, where one typically trains a deep network to regress to crop parameters or cropping actions, we propose to directly optimize for the cropping parameters by repurposing pre-trained networks on image captioning and aesthetic tasks, without any fine-tuning, thereby avoiding training a separate network. Specifically, we search for the best crop parameters that minimize a combined loss of the initial objectives of these networks. To make the optimization stable, we propose three strategies: (i) multi-scale bilinear sampling, (ii) annealing the scale of the crop region, therefore effectively reducing the parameter space, (iii) aggregation of multiple optimization results. Through various quantitative and qualitative evaluations, we show that our framework can produce crops that are well-aligned to intended user descriptions and aesthetically pleasing.}
}
@article{ZHANG2022108412,
title = {Incomplete multiview nonnegative representation learning with multiple graphs},
journal = {Pattern Recognition},
volume = {123},
pages = {108412},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108412},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005884},
author = {Nan Zhang and Shiliang Sun},
keywords = {Multiview clustering, Graph learning, Incomplete multiview clustering, Nonnegative matrix factorization},
abstract = {Multiview clustering has become an important research topic during the past decade. However, partial views of many data instances are missing in some realistic multiview learning scenarios. To handle this problem, we develop an effective incomplete multiview nonnegative representation learning (IMNRL) framework, which is suitable for incomplete multiview clustering in various situations. The IMNRL framework performs matrix factorization on multiple incomplete graphs and decomposes these incomplete graphs into a consensus nonnegative representation and view-specific spectral representations, which integrates the advantages of multiview nonnegative representation learning and graph learning. The proposed framework has the following merits: (1) it learns a consensus nonnegative embedding and view-specific embeddings simultaneously; (2) the nonnegative embedding satisfies the neighbor constraint on each incomplete view, which directly reveals the multiview clustering results. Experimental results show that the proposed framework outperforms other state-of-the-art incomplete multiview clustering algorithms.}
}
@article{CHALAVADI2022108548,
title = {mSODANet: A network for multi-scale object detection in aerial images using hierarchical dilated convolutions},
journal = {Pattern Recognition},
volume = {126},
pages = {108548},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108548},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000292},
author = {Vishnu Chalavadi and Prudviraj Jeripothula and Rajeshreddy Datla and Sobhan Babu Ch and Krishna Mohan C},
keywords = {Multi-scale object detection, Contextual features, Dilated convolutions, Aerial images},
abstract = {The object detection in aerial images is one of the most commonly used tasks in the wide-range of computer vision applications. However, the object detection is more challenging due to the following issues: (a) the pixel occupancy vary among the different scales of objects, (b) the distribution of objects is not uniform in aerial images, (c) the appearance of an object varies with different view-points and illumination conditions, and (d) the number of objects, even though they belong to same type, vary across the images. To address these issues, we propose a novel network for multi-scale object detection in aerial images using hierarchical dilated convolutions, called as mSODANet. In particular, we probe hierarchical dilated network using parallel dilated convolutions to learn the contextual information of different types of objects at multiple scales and multiple field-of-views. The introduced hierarchical dilated network captures the visual information of aerial image more effectively and enhances the detection capability of the model. Further, the extensive experiments conducted on three challenging publicly available datasets, i.e., Visdrone2019, DOTA (OBB & HBB), NWPU VHR-10, demonstrate the effectiveness of the proposed mSODANet and achieve the state-of-the-art performance on all three datasets.}
}
@article{LV2022108364,
title = {Semi-supervised Active Salient Object Detection},
journal = {Pattern Recognition},
volume = {123},
pages = {108364},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108364},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005446},
author = {Yunqiu Lv and Bowen Liu and Jing Zhang and Yuchao Dai and Aixuan Li and Tong Zhang},
keywords = {Salient object detection, Annotation-efficient Learning, Active learning, Variational Auto-Encoder},
abstract = {In this paper, we propose a novel semi-supervised active salient object detection (SOD) method that actively acquires a small subset of the most discriminative and representative samples for labeling. Two main contributions have been made to prevent the method from being overwhelmed by labeling similar distributed samples. First, we design a saliency encoder-decoder with adversarial discriminator to generate a confidence map, representing the network uncertainty on the current prediction. Then, we select the least confident (discriminative) samples from the unlabeled pool to form the “candidate labeled pool”. Second, we train a Variational Auto-Encoder (VAE) to select and add the most representative data from the “candidate labeled pool” into the labeled pool by comparing their corresponding features in the latent space. Within our framework, these two networks are optimized conditioned on the states of each other progressively. Experimental results on six benchmarking SOD datasets demonstrate that our annotation-efficient learning based salient object detection method, reaching to 14% labeling budget, can be on par with the state-of-the-art fully-supervised deep SOD models. The source code is publicly available via our project page: https://github.com/JingZhang617/Semi-sup-active-self-sup-Learning.}
}
@article{JIA2022108556,
title = {Learning multi-scale synergic discriminative features for prostate image segmentation},
journal = {Pattern Recognition},
volume = {126},
pages = {108556},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108556},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000371},
author = {Haozhe Jia and Weidong Cai and Heng Huang and Yong Xia},
keywords = {Prostate segmentation, Intra-class consistency, Inter-class discrimination, Synergic multi-task loss},
abstract = {Although deep convolutional neural networks (DCNNs) have been proposed for prostate MR image segmentation, the effectiveness of these methods is often limited by inadequate semantic discrimination and spatial context modeling. To address these issues, we propose a Multi-scale Synergic Discriminative Network (MSD-Net), which includes a shared encoder, a segmentation decoder, and a boundary detection decoder. We further design the cascaded pyramid convolutional block and residual refinement block, and incorporate them and the channel attention block into MSD-Net to exploit the multi-scale spatial contextual information and semantically consistent features of the gland. We also fuse the features from two decoders to boost the segmentation performance, and introduce the synergic multi-task loss to impose the consistence constraint on the joint segmentation and boundary detection. We evaluated MSD-Net against several prostate segmentation methods on three public datasets and achieved an improved accuracy. Our results indicate that the proposed MSD-Net outperforms existing methods with setting the new state-of-the-art for prostate segmentation in magnetic resonance images.}
}
@article{YANG2022108368,
title = {Discrete embedding for attributed graphs},
journal = {Pattern Recognition},
volume = {123},
pages = {108368},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108368},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005483},
author = {Hong Yang and Ling Chen and Shirui Pan and Haishuai Wang and Peng Zhang},
keywords = {Attributed graphs, Graph embedding, Weisfeiler-Lehman graph kernels, Learning to hash, Low-bit quantization},
abstract = {Attributed graphs refer to graphs where both node links and node attributes are observable for analysis. Attributed graph embedding enables joint representation learning of node links and node attributes. Different from classical graph embedding methods such as Deepwalk and node2vec that first project node links into low-dimensional vectors which are then linearly concatenated with node attribute vectors as node representation, attributed graph embedding fully explores data dependence between node links and attributes by either using node attributes as class labels to supervise structure learning from node links, or reversely using node links to supervise the learning from node attributes. However, existing attributed graph embedding models are designed in continuous Euclidean spaces which often introduce data redundancy and impose challenges to storage and computation costs. In this paper, we study a new problem of discrete embedding for attributed graphs that can learn succinct node representations. Specifically, we present a Binarized Attributed Network Embedding model (BANE for short) to learn binary node representation by factorizing a Weisfeiler-Lehman proximity matrix under the constraint of binary node representation. Furthermore, based on BANE, we propose a new Low-bit Quantization for Attributed Network Representation learning model (LQANR for short) to learn even more compact node representation of bit-width values. Theoretical analysis and empirical studies on real-world datasets show that the new discrete embedding models outperform benchmark methods.}
}
@article{TURSUN2022108528,
title = {An efficient framework for zero-shot sketch-based image retrieval},
journal = {Pattern Recognition},
volume = {126},
pages = {108528},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108528},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000097},
author = {Osman Tursun and Simon Denman and Sridha Sridharan and Ethan Goan and Clinton Fookes},
keywords = {Sketch-based image retrieval, Zero-shot learning, Knowledge distillation, Similarity learning},
abstract = {Zero-shot sketch-based image retrieval (ZS-SBIR) has recently attracted the attention of the computer vision community due to its real-world applications, and the more realistic and challenging setting that it presents over SBIR. ZS-SBIR inherits the main challenges of multiple computer vision problems including content-based Image Retrieval (CBIR), zero-shot learning and domain adaptation. The majority of previous studies using deep neural networks have achieved improved results by either projecting sketch and images into a common low-dimensional space, or transferring knowledge from seen to unseen classes. However, those approaches are trained with complex frameworks composed of multiple deep convolutional neural networks (CNNs) and are dependent on category-level word labels. This increases the requirements for training resources and datasets. In comparison, we propose a simple and efficient framework that does not require high computational training resources, and learns the semantic embedding space from a vision model rather than a language model, as is done by related studies. Furthermore, at training and inference stages our method only uses a single CNN. In this work, a pre-trained ImageNet CNN (i.e., ResNet50) is fine-tuned with three proposed learning objects: domain-balanced quadruplet loss, semantic classification loss, and semantic knowledge preservation loss. The domain-balanced quadruplet and semantic classification losses are introduced to learn discriminative, semantic and domain invariant features by considering ZS-SBIR as an object detection and verification problem. To preserve semantic knowledge learned with ImageNet and exploit it for unseen categories, the semantic knowledge preservation loss is proposed. To reduce computational cost and increase the accuracy of the semantic knowledge distillation process, ground-truth semantic knowledge is prepared in a class-oriented fashion prior to training. Extensive experiments are conducted on three challenging ZS-SBIR datasets: Sketchy Extended, TU-Berlin Extended and QuickDraw Extended. The proposed method achieves state-of-the-art results, and outperforms the majority of related works by a substantial margin.}
}
@article{WANG2022108564,
title = {Geometric imbalanced deep learning with feature scaling and boundary sample mining},
journal = {Pattern Recognition},
volume = {126},
pages = {108564},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108564},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000450},
author = {Zhe Wang and Qida Dong and Wei Guo and Dongdong Li and Jing Zhang and Wenli Du},
keywords = {Imbalance problem, Image classification, Geometric information, Boundary samples mining, Feature scaling},
abstract = {Data imbalance is a significant factor affecting classification performance in computer vision. In particular, data imbalance is harmful to classification learning and representation learning. To address this issue, this paper proposes a geometric deep learning framework combined with Feature Scaling Module (FSM) and Boundary Samples Mining Module (BSMM). Considering the geometric information in sample distributions of training samples, FSM is proposed to scale the features by hypersphere radius of each class, which improves the representation ability of minority classes. Meanwhile, it is noteworthy that the relationships and information between samples are essential for classification. Therefore, BSMM is proposed to mine the boundary samples by Gabriel Graph that takes the relationships into account. Finally, a loss scheduler is designed to adjust the training process of these two modules. With the scheduler, the model first learns representation and then focuses more on minority classes gradually. Extensive experiments on three benchmark datasets demonstrate the advantages of the proposed learning framework over the state-of-the-art models for solving the imbalance problem.}
}
@article{HUANG2022108376,
title = {Robust multi-feature collective non-negative matrix factorization for ECG biometrics},
journal = {Pattern Recognition},
volume = {123},
pages = {108376},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108376},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005562},
author = {Yuwen Huang and Gongping Yang and Kuikui Wang and Haiying Liu and Yilong Yin},
keywords = {ECG biometrics, Collective non-negative matrix factorization, Multiple features, Local binary pattern, Label information},
abstract = {The field of electrocardiogram (ECG) biometrics has received considerable attention in recent years. Although some promising methods have been proposed, it is challenging to design a robust and precise method to improve the recognition performance of ECG signals with noise and sample variation. While the advantage of improved local binary pattern (LBP) for establishing identities has been widely recognized, extracting the latent semantics from multiple LBP features has attracted little attention. We propose a robust multi-feature collective non-negative matrix factorization (RMCNMF) model to handle noise and sample variation in ECG Biometrics. We extract multiple LBP histograms as feature descriptors from segmented ECG signals, and propose a multi-feature learning framework that learns unified representations in the shared latent semantic space via collective non-negative matrix factorization. To further enhance the discrimination of learned representations, we integrate label information and multiple norms in the proposed model, which not only preserves intra- and inter-subject similarities but also mitigates the influence of noise and sample variation. RMCNMF can be solved by an efficient iteration method, for which we provide a convergence analysis in detail. Extensive experiments on four ECG databases show that it performs competitively with state-of-the-art methods.}
}
@article{JIANG2022108588,
title = {Robust image matching via local graph structure consensus},
journal = {Pattern Recognition},
volume = {126},
pages = {108588},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108588},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000693},
author = {Xingyu Jiang and Yifan Xia and Xiao-Ping Zhang and Jiayi Ma},
keywords = {Image matching, Feature matching, Mismatch removal, Outlier, Image registration},
abstract = {Image matching plays a vital role in many computer vision tasks, and this paper focuses on the mismatch removal problem of feature-based matching. We formulate the problem into a general yet effective optimization framework based on graph matching by combining integer quadratic programming with a compensation term for discouraging matches, termed as Local Graph Structure Consensus (LGSC). Considering the local area similarity of those potential true matches, we design a local graph structure for preserving geometric topology, which contains a local indicator vector and a local affinity vector for each correspondence. The local indicator vector is utilized for edge construction, while the local affinity vector represents the match correctness of the nodes and edges between two graphs. In particular, the ranking shift with scale and rotation invariance is exploited to represent the node affinity. Ultimately, we derive a closed-form solution with linearithmic time and linear space complexity. Moreover, a multi-scale and iterative graph construction strategy is proposed to promote the performance of our method in terms of robustness and effectiveness. Extensive experiments on various real image datasets demonstrate that our LGSC can achieve superior performance over current state-of-the-art approaches.}
}
@article{VOVK2022108536,
title = {Universal predictive systems},
journal = {Pattern Recognition},
volume = {126},
pages = {108536},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108536},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000176},
author = {Vladimir Vovk},
keywords = {Conformal prediction, Predictive distribution, Probabilistic calibration, Universal consistency},
abstract = {This paper describes probability forecasting systems that are universal, or universally consistent, in the sense of being consistent under any data-generating distribution, assuming that the observations are produced independently in the IID fashion. The notion of universal consistency is asymptotic and does not imply any small-sample guarantees of validity. On the other hand, the method of conformal prediction has been recently adapted to producing predictive distributions that satisfy a natural property of small-sample validity, namely they are automatically probabilistically calibrated. The main result of the paper is the existence of universal conformal predictive systems, which output predictive distributions that are both probabilistically calibrated and universally consistent.}
}
@article{2022108633,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {126},
pages = {108633},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(22)00114-5},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001145}
}
@article{QIN2022108544,
title = {D2T: A Framework For transferring detection to tracking},
journal = {Pattern Recognition},
volume = {126},
pages = {108544},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108544},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000255},
author = {Huai Qin and Changqian Yu and Changxin Gao and Nong Sang},
keywords = {Object tracking, Object detection, Transferring detection to tracking},
abstract = {Object detection methods draw increasing attention in deep learning based visual tracking algorithms due to their robust discrimination and powerful regression ability. To further explore the potential of object detection methods in the visual tracking task, there are two gaps that need to be bridged. The first is the difference in object definition. Object detection is class-specific while visual tracking is class-agnostic. Moreover, visual tracking needs to differentiate the target from intra-class distractors. The second is the difference in temporal dimension. Different from object detection which processes still-image, visual tracking concentrates on objects which vary continuously with time. In this paper, we propose a Detection to Tracking (D2T) framework to address the above issues and effectively transfer existing advanced detection methods to visual tracking task. Specifically, to bridge the gap of object definition, we propose a general-to-specific network that separates learning general object features and instance-level features. To make full use of the contextual information while adapting to the appearance variation of targets, we propose a temporal strategy combining short-term constraint and long-term updating. To the best of our knowledge, our D2T framework is the first universal framework which directly transfers deep learning based object detectors to visual tracking task. It provides a novel solution to visual object tracking, and it achieves superior performance in several public datasets.}
}
@article{YIN2022108568,
title = {Unsupervised person re-identification via simultaneous clustering and mask prediction},
journal = {Pattern Recognition},
volume = {126},
pages = {108568},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108568},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000498},
author = {Junhui Yin and Siqing Zhang and Jiyang Xie and Zhanyu Ma and Jun Guo},
keywords = {Person re-identification, Domain adaptation, Unsupervised clustering, Mask prediction, Semantic cluster},
abstract = {Extracting meaningful representation is a key challenge for person re-identification (re-ID) task, especially in the absence of ground truth labels. However, existing unsupervised approaches simply utilize pseudo labels generated from clustering to supervise re-ID model and thus have not yet fully explored the semantic information existing in data itself. This also limits the representation capabilities of learned models. To address the above problem, we propose mask prediction (MaskPre) as a pretext task for unsupervised re-ID, such that the clustering network can capture more semantic information and separate the images into semantic clusters automatically. Specifically, MaskPre masks region-level features with dynamic dropblock layer to generate differently masked views of a single image. To predict the masked regions and bridge the domain gap across views, we design mask prediction head and moving-average model to learn visual consistency from still image and temporal consistency during training process. Meanwhile, we optimize the model by grouping the two masked views into the same cluster, thus enhancing the consistency across views. Experimental results on three public benchmark datasets show that our proposed method outperforms the existing state-of-the-art approaches.}
}
@article{XIA2022108552,
title = {CSCNet: Contextual semantic consistency network for trajectory prediction in crowded spaces},
journal = {Pattern Recognition},
volume = {126},
pages = {108552},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108552},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000334},
author = {Beihao Xia and Conghao Wong and Qinmu Peng and Wei Yuan and Xinge You},
keywords = {Trajectory prediction, The context-aware transfer, The conditional context loss},
abstract = {Trajectory prediction aims to predict the movement trend of the agents like pedestrians, bikers, vehicles. It is helpful to analyze and understand human activities in crowded spaces and widely applied in many areas such as surveillance video analysis and autonomous driving systems. Thanks to the success of deep learning, trajectory prediction has made significant progress. The current methods are dedicated to studying the agents’ future trajectories under the social interaction and the sceneries’ physical constraints. Moreover, how to deal with these factors still catches researchers’ attention. However, they ignore the Semantic Shift Phenomenon when modeling these interactions in various prediction sceneries. There exist several kinds of semantic deviations inner or between social and physical interactions, which we call the “Gap”. In this paper, we propose a Contextual Semantic Consistency Network (CSCNet) to predict agents’ future activities with powerful and efficient context constraints. We utilize a well-designed context-aware transfer to obtain the intermediate representations from the scene images and trajectories. Then we eliminate the differences between social and physical interactions by aligning activity semantics and scene semantics to cross the Gap. Experiments demonstrate that CSCNet performs better than most of the current methods quantitatively and qualitatively.}
}
@article{FU2022108576,
title = {An attention-enhanced cross-task network to analyse lung nodule attributes in CT images},
journal = {Pattern Recognition},
volume = {126},
pages = {108576},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108576},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000577},
author = {Xiaohang Fu and Lei Bi and Ashnil Kumar and Michael Fulham and Jinman Kim},
keywords = {Deep learning, Lung nodule analysis, Multi-task, Computed tomography (CT), Attention},
abstract = {Accurate characterization of visual attributes such as spiculation, lobulation, and calcification of lung nodules in computed tomography (CT) images is critical in cancer management. The characterization of these attributes is often subjective, which may lead to high inter- and intra-observer variability. Furthermore, lung nodules are often heterogeneous in the cross-sectional image slices of a 3D volume. Current state-of-the-art methods that score multiple attributes rely on deep learning-based multi-task learning (MTL) schemes. These methods, however, extract shared visual features across attributes and then examine each attribute without explicitly leveraging their inherent intercorrelations. Furthermore, current methods treat each slice with equal importance without considering their relevance or heterogeneity, which limits performance. In this study, we address these challenges with a new convolutional neural network (CNN)-based MTL model that incorporates multiple attention-based learning modules to simultaneously score 9 visual attributes of lung nodules in CT image volumes. Our model processes entire nodule volumes of arbitrary depth and uses a slice attention module to filter out irrelevant slices. We also introduce cross-attribute and attribute specialization attention modules that learn an optimal amalgamation of meaningful representations to leverage relationships between attributes. We demonstrate that our model outperforms previous state-of-the-art methods at scoring attributes using the well-known public LIDC-IDRI dataset of pulmonary nodules from over 1,000 patients. Our model also performs competitively when repurposed for benign-malignant classification. Our attention modules provide easy-to-interpret weights that offer insights into the predictions of the model.}
}
@article{PANG2022108555,
title = {Hierarchical electricity time series prediction with cluster analysis and sparse penalty},
journal = {Pattern Recognition},
volume = {126},
pages = {108555},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108555},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200036X},
author = {Yue Pang and Xiangdong Zhou and Junqi Zhang and Quan Sun and Jianbin Zheng},
keywords = {Hierarchical time series forecasting, Data mining, Machine learning},
abstract = {In big data applications, hierarchical time series prediction is an important element of decision-making and concerns the inherent aggregation consistency, which is maintained by reconciliation methods. The paper proposes a novel multiple alternative clustering time series analysis based hierarchical electricity time series prediction method. Instead of adhering the aggregation consistency passively, we first exploit time series mining to construct a hierarchy, and then apply an optimal reconciliation method to improve the prediction accuracy. In particular, k-means clustering method is employed to cluster time series for many times with different k so as to make a large number of time series clusters (patterns), and then the clusters (patterns) based hierarchies are constructed respectively. With the large number of clusters hierarchies and the original geographical hierarchy, an optimal aggregation consistency reconciliation based prediction approach is proposed. Furthermore, the sparse penalty is adapted in our method for “ideal” clusters selection to improve the prediction performance. Compared with the state-of-the-art methods on real-life datasets, our method achieves the improvement of 11.13% and 24.07% accurate one-step ahead forecasts on electricity load and solar power data respectively.}
}
@article{JI2022108560,
title = {A globally convergent approximate Newton method for non-convex sparse learning},
journal = {Pattern Recognition},
volume = {126},
pages = {108560},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108560},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000413},
author = {Fanfan Ji and Hui Shuai and Xiao-Tong Yuan},
keywords = {Sparse learning, Newton-type method, Linear models, Quadratic approximation, Iterative hard thresholding},
abstract = {Newton-type greedy pursuit methods have been shown to work favorably for cardinality-constrained sparse learning problems. The appealing sparsity recovery performance of the existing Newton-type greedy pursuit methods, however, is typically guaranteed within a local neighborhood around the target solution. To address this limitation, we present in this paper a novel approximate Newton pursuit method for sparse learning with linear models. The computation procedure of our method iterates between constructing an inexact Newton-type quadratic majorization to the global empirical risk and solving the quadratic approximation via iterative hard thresholding. Provable global guarantees on mean squared prediction error, which is less understood for prior methods, are provided for our method. Numerical evidence is provided to show the advantages of our approach over the prior methods.}
}
@article{LIU2022108584,
title = {MoRE: Multi-output residual embedding for multi-label classification},
journal = {Pattern Recognition},
volume = {126},
pages = {108584},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108584},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000656},
author = {Siyu Liu and Xuehua Song and Zhongchen Ma and Ernest Domanaanmwi Ganaa and XiangJun Shen},
keywords = {Distance metric, Low-rank structure, Residual embedding},
abstract = {Multi-label classification (MLC) is one of the challenging tasks in computer vision, where it confronts high dimensional problem both in output label and input feature spaces. This paper proposed solving MLC through multi-output residual embedding (MoRE), which learns appropriate distance metric by analyzing the residuals between input and output spaces. Unlike traditional MLC paradigms that learn relationships between label space and feature space, our proposed approach further learns a low-rank structure in residuals between input and output spaces. And it encodes such residual projection to achieve dimension reduction in label space, enhancing the performance of the proposed algorithm in processing high dimensional MLC task. Furthermore, considering the label correlations between instances and its neighbors, multiple residuals of instances neighbors are also incorporated into the proposed model to further learn more appropriate distance metric in the same way. Overall, with residual embedding learning from instances and their neighbors, the obtained metric can learn a more appropriate low-rank structure in label space to handle high dimensional problem in MLC. Experimental results on several data sets, such as Cal500, Corel5k, Bibtex, Delicious, Tmc2007, 20ng, Mirflickr and Rcv1s1, demonstrate the excellent predictive performance of MoRE among STOA methods, such as LMMO-kNN, M3MDC, KRAM, SEEM, CPLST, CSSP, FaIE.}
}
@article{WANG2022108589,
title = {Class-specific discriminative metric learning for scene recognition},
journal = {Pattern Recognition},
volume = {126},
pages = {108589},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108589},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200070X},
author = {Chen Wang and Guohua Peng and Bernard {De Baets}},
keywords = {Class-specific distance metrics, Discriminative metric learning, Scene recognition},
abstract = {Metric learning aims to learn an appropriate distance metric for a given machine learning task. Despite its impressive performance in the field of image recognition, it may still not be discriminative enough for scene recognition because of the high within-class diversity and high between-class similarity of scene images. In this paper, we propose a novel class-specific discriminative metric learning method (CSDML) to alleviate these problems. More specifically, we learn a distinctive linear transformation for each class (or, equivalently, a Mahalanobis distance metric for each class), which allows to project the samples of that class into a corresponding low-dimensional discriminative space. The overall aim is to simultaneously minimize the Euclidean distances between the projections of samples of the same class (or, equivalently, the Mahalanobis distances between these samples) and maximize the Euclidean distances between the projections of samples of different classes. Additionally, we incorporate least squares regression into the optimization problem, rendering class-specific metric learning more flexible and better suited to tackle scene recognition. Experimental results on four benchmark scene datasets demonstrate that the proposed method outperforms most of the state-of-the-art approaches.}
}
@article{YIN2022108371,
title = {Incomplete multi-view clustering with cosine similarity},
journal = {Pattern Recognition},
volume = {123},
pages = {108371},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108371},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005513},
author = {Jun Yin and Shiliang Sun},
keywords = {Multi-view learning, Missing view, Cosine similarity, Gradient descent, Matrix factorization},
abstract = {Incomplete multi-view clustering partitions multi-view data suffering from missing views, for which matrix factorization approaches seek the latent representation of incomplete multi-view data and constitute one effective category of methods. To exploit data properties further, manifold structure preserving is also incorporated into matrix factorization. However, previous methods optimized the data similarity matrix in the manifold structure preserving term as an unknown variable, which is not guaranteed to faithfully represent the similarities of the original multi-view data and also increases the computational difficulty. To overcome these drawbacks, in this paper, we propose Incomplete Multi-view Clustering with Cosine Similarity (IMCCS). In IMCCS, we directly calculate the cosine similarity in the original multi-view space to strengthen the ability of preserving the manifold structure of the original multi-view data. There is no need to introduce the additional variable. The manifold structure preserving term with cosine similarity and the matrix factorization term are integrated into a unified objective function. An iterative algorithm with gradient descent is designed to solve this objective. Extensive experiments on multi-view datasets show that IMCCS outperforms state-of-the-art incomplete multi-view clustering methods.}
}
@article{TAI2022108399,
title = {Kernelized Supervised Laplacian Eigenmap for Visualization and Classification of Multi-Label Data},
journal = {Pattern Recognition},
volume = {123},
pages = {108399},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108399},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005756},
author = {Mariko Tai and Mineichi Kudo and Akira Tanaka and Hideyuki Imai and Keigo Kimura},
keywords = {Supervised Laplacian eigenmaps, Out-of-sample problem, Multi-label problems, Kernel trick, Separability-guided feature extraction},
abstract = {We had previously proposed a supervised Laplacian eigenmap for visualization (SLE-ML) that can handle multi-label data. In addition, SLE-ML can control the trade-off between the class separability and local structure by a single trade-off parameter. However, SLE-ML cannot transform new data, that is, it has the “out-of-sample” problem. In this paper, we show that this problem is solvable, that is, it is possible to simulate the same transformation perfectly using a set of linear sums of reproducing kernels (KSLE-ML) with a nonsingular Gram matrix. We experimentally showed that the difference between training and testing is not large; thus, a high separability of classes in a low-dimensional space is realizable with KSLE-ML by assigning an appropriate value to the trade-off parameter. This offers the possibility of separability-guided feature extraction for classification. In addition, to optimize the performance of KSLE-ML, we conducted both kernel selection and parameter selection. As a result, it is shown that parameter selection is more important than kernel selection. We experimentally demonstrated the advantage of using KSLE-ML for visualization and for feature extraction compared with a few typical algorithms.}
}
@article{OBESO2022108411,
title = {Visual vs internal attention mechanisms in deep neural networks for image classification and object detection},
journal = {Pattern Recognition},
volume = {123},
pages = {108411},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108411},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005872},
author = {Abraham Montoya Obeso and Jenny Benois-Pineau and Mireya Saraí {García Vázquez} and Alejandro Álvaro Ramírez Acosta},
keywords = {Deep learning, Image classification, Object detection, Visual attention, Saliency maps},
abstract = {The so-called “attention mechanisms” in Deep Neural Networks (DNNs) denote an automatic adaptation of DNNs to capture representative features given a specific classification task and related data. Such attention mechanisms perform both globally by reinforcing feature channels and locally by stressing features in each feature map. Channel and feature importance are learnt in the global end-to-end DNNs training process. In this paper, we present a study and propose a method with a different approach, adding supplementary visual data next to training images. We use human visual attention maps obtained independently with psycho-visual experiments, both in task-driven or in free viewing conditions, or powerful models for prediction of visual attention maps. We add visual attention maps as new data alongside images, thus introducing human visual attention into the DNNs training and compare it with both global and local automatic attention mechanisms. Experimental results show that known attention mechanisms in DNNs work pretty much as human visual attention, but still the proposed approach allows a faster convergence and better performance in image classification tasks.}
}
@article{QIN2022108547,
title = {Multi-level augmented inpainting network using spatial similarity},
journal = {Pattern Recognition},
volume = {126},
pages = {108547},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108547},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000280},
author = {Jia Qin and Huihui Bai and Yao Zhao},
keywords = {Image inpainting, Spatial information, Spatial similarity, Pyramid reconstruction structure},
abstract = {Recently, multi-scale neural networks have shown promising improvements in image inpainting. However, most of them adopt the progressive way, in which the errors on lower scales may be propagated on higher scales. Addressing this issue, we propose a multi-level augmented inpainting network (MLA-Net) to rationally harmonize the inter- and intra-level contexts. Here, a pyramid reconstruction structure (PRS) with three parallel levels is designed to establish the inter-level relationship, which can boost the representation of the features by integrating the texture details into semantics. Then, we propose a novel spatial similarity based attention mechanism (SSA) to ensure the intra-level local continuity between the holes and related available patches. In SSA, in order to focus on the important textures and structures rather than calculating each pixel of the feature equally, a spatial map is utilized to highlight the corresponding spatial locations during the similarity computation. The experiments are evaluated on multiple challenging datasets, which demonstrate that MLA-Net can generate accurate results with better visual quality compared with the state-of-the-art methods. For the 256×256 Places2 dataset, PSNR increases 1.02 dB, while FID decreases 0.075. For the 256×256 CelebA-HQ dataset, there are 0.22 dB and 0.613 improvements in PSNR and FID.}
}
@article{ZHANG2022108419,
title = {Orthogonal least squares based fast feature selection for linear classification},
journal = {Pattern Recognition},
volume = {123},
pages = {108419},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108419},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005951},
author = {Sikai Zhang and Zi-Qiang Lang},
keywords = {Feature selection, Orthogonal least squares, Canonical correlation analysis, Linear discriminant analysis, Multi-label, Multivariate time series, Feature interaction},
abstract = {An Orthogonal Least Squares (OLS) based feature selection method is proposed for both binomial and multinomial classification. The novel Squared Orthogonal Correlation Coefficient (SOCC) is defined based on Error Reduction Ratio (ERR) in OLS and used as the feature ranking criterion. The equivalence between the canonical correlation coefficient, Fisher’s criterion, and the sum of the SOCCs is revealed, which unveils the statistical implication of ERR in OLS for the first time. It is also shown that the OLS based feature selection method has speed advantages when applied for greedy search. The proposed method is comprehensively compared with the mutual information based feature selection methods and the embedded methods using both synthetic and real world datasets. The results show that the proposed method is always in the top 5 among the 12 candidate methods. Besides, the proposed method can be directly applied to continuous features without discretisation, which is another significant advantage over mutual information based methods.}
}
@article{2022108459,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {123},
pages = {108459},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(21)00635-X},
url = {https://www.sciencedirect.com/science/article/pii/S003132032100635X}
}
@article{LIU2022108375,
title = {GL-GAN: Adaptive global and local bilevel optimization for generative adversarial network},
journal = {Pattern Recognition},
volume = {123},
pages = {108375},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108375},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005550},
author = {Ying Liu and Heng Fan and Xiaohui Yuan and Jinhai Xiang},
keywords = {Generative adversarial networks (GAN), Global and local bilevel optimization, Ada-OP, Image generation},
abstract = {Although Generative Adversarial Networks (GAN) have shown remarkable performance in image generation, there exist some challenges in instability and convergence speed. During the training, the results of some models display the imbalances of quality within a generated image, in which some defective parts appear compared with other regions. Different from general single global optimization methods, we introduce an adaptive global and local bilevel optimization model (GL-GAN). The model achieves the generation of high-resolution images in a complementary and promoting way, where global optimization is to optimize the whole images and local is only to optimize the low-quality areas. Based on DCGAN, GL-GAN is able to effectively avoid the nature of imbalance by local bilevel optimization, which is accomplished by first locating low-quality areas and then optimizing them. Moreover, through feature map cues from discriminator output, we propose the adaptive local and global optimization method (Ada-OP) for interactive optimization and observe that it boosts the convergence speed. Compared with the current GAN methods, our model has shown impressive performance on CelebA, Oxford Flowers, CelebA-HQ and LSUN datasets.}
}
@article{LIU2022108403,
title = {Fitbeat: COVID-19 estimation based on wristband heart rate using a contrastive convolutional auto-encoder},
journal = {Pattern Recognition},
volume = {123},
pages = {108403},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108403},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005793},
author = {Shuo Liu and Jing Han and Estela Laporta Puyal and Spyridon Kontaxis and Shaoxiong Sun and Patrick Locatelli and Judith Dineley and Florian B. Pokorny and Gloria Dalla Costa and Letizia Leocani and Ana Isabel Guerrero and Carlos Nos and Ana Zabalza and Per Soelberg Sørensen and Mathias Buron and Melinda Magyari and Yatharth Ranjan and Zulqarnain Rashid and Pauline Conde and Callum Stewart and Amos A Folarin and Richard JB Dobson and Raquel Bailón and Srinivasan Vairavan and Nicholas Cummins and Vaibhav A Narayan and Matthew Hotopf and Giancarlo Comi and Björn Schuller and RADAR-CNS Consortium},
keywords = {COVID-19, Respiratory tract infection, Anomaly detection, Contrastive learning, Convolutional auto-encoder},
abstract = {This study proposes a contrastive convolutional auto-encoder (contrastive CAE), a combined architecture of an auto-encoder and contrastive loss, to identify individuals with suspected COVID-19 infection using heart-rate data from participants with multiple sclerosis (MS) in the ongoing RADAR-CNS mHealth research project. Heart-rate data was remotely collected using a Fitbit wristband. COVID-19 infection was either confirmed through a positive swab test, or inferred through a self-reported set of recognised symptoms of the virus. The contrastive CAE outperforms a conventional convolutional neural network (CNN), a long short-term memory (LSTM) model, and a convolutional auto-encoder without contrastive loss (CAE). On a test set of 19 participants with MS with reported symptoms of COVID-19, each one paired with a participant with MS with no COVID-19 symptoms, the contrastive CAE achieves an unweighted average recall of 95.3%, a sensitivity of 100% and a specificity of 90.6%, an area under the receiver operating characteristic curve (AUC-ROC) of 0.944, indicating a maximum successful detection of symptoms in the given heart rate measurement period, whilst at the same time keeping a low false alarm rate.}
}
@article{GUO2022108448,
title = {Differentiable neural architecture learning for efficient neural networks},
journal = {Pattern Recognition},
volume = {126},
pages = {108448},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108448},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321006245},
author = {Qingbei Guo and Xiao-Jun Wu and Josef Kittler and Zhiquan Feng},
keywords = {Deep neural network, Convolutional neural network, Neural architecture search, Automated machine learning},
abstract = {Efficient neural networks has received ever-increasing attention with the evolution of convolutional neural networks (CNNs), especially involving their deployment on embedded and mobile platforms. One of the biggest problems to obtaining such efficient neural networks is efficiency, even recent differentiable neural architecture search (DNAS) requires to sample a small number of candidate neural architectures for the selection of the optimal neural architecture. To address this computational efficiency issue, we introduce a novel architecture parameterization based on scaled sigmoid function, and propose a general Differentiable Neural Architecture Learning (DNAL) method to obtain efficient neural networks without the need to evaluate candidate neural networks. Specifically, for stochastic supernets as well as conventional CNNs, we build a new channel-wise module layer with the architecture components controlled by a scaled sigmoid function. We train these neural network models from scratch. The network optimization is decoupled into the weight optimization and the architecture optimization, which avoids the interaction between the two types of parameters and alleviates the vanishing gradient problem. We address the non-convex optimization problem of efficient neural networks by the continuous scaled sigmoid method instead of the common softmax method. Extensive experiments demonstrate our DNAL method delivers superior performance in terms of efficiency, and adapts to conventional CNNs (e.g., VGG16 and ResNet50), lightweight CNNs (e.g., MobileNetV2) and stochastic supernets (e.g., ProxylessNAS). The optimal neural networks learned by DNAL surpass those produced by the state-of-the-art methods on the benchmark CIFAR-10 and ImageNet-1K dataset in accuracy, model size and computational complexity. Our source code is available at https://github.com/QingbeiGuo/DNAL.git.}
}
@article{CHEN2022108567,
title = {Deep attention aware feature learning for person re-Identification},
journal = {Pattern Recognition},
volume = {126},
pages = {108567},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108567},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000486},
author = {Yifan Chen and Han Wang and Xiaolu Sun and Bin Fan and Chu Tang and Hui Zeng},
keywords = {Person re-identification, Attention learning, Multi-task learning},
abstract = {Visual attention has proven to be effective in improving the performance of person re-identification. Most existing methods apply visual attention heuristically by learning an additional attention map to re-weight the feature maps for person re-identification, however, this kind of methods inevitably increase the model complexity and inference time. In this paper, we propose to incorporate the ability of predicting attention maps as additional objectives in a person ReID network without changing the original structure, thus maintain the same inference time and model size. Two kinds of attention maps have been considered to make the learned feature maps being aware of the person and related body parts respectively. Globally, a holistic attention branch (HAB) is proposed to make the feature maps obtained by backbone could focus on persons so as to alleviate the influence of background. Locally, a partial attention branch (PAB) is proposed to make the extracted features can be decoupled into several groups that are separately responsible for different body parts, thus increasing the robustness to pose variation and partial occlusion. These two kinds of attentions are universal and can be incorporated into existing ReID networks. We have tested its performance on two typical networks (TriNet [1] and Bag of Tricks [2]) and observed significant performance improvement on five widely used datasets.}
}
@article{FERNANDO2022108551,
title = {Split ‘n’ merge net: A dynamic masking network for multi-task attention},
journal = {Pattern Recognition},
volume = {126},
pages = {108551},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108551},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000322},
author = {Tharindu Fernando and Sridha Sridharan and Simon Denman and Clinton Fookes},
keywords = {Multi-task learning, Attention, Cuffless blood pressure measurement, Biomedical signal processing, Deep learning, Emotion recognition},
abstract = {In this paper we propose a novel Multi-Task Learning (MTL) framework, Split ‘n’ Merge Net. We draw the inspiration from the multi-head attention formulation of Transformers and propose a novel, simple and interpretable pathway to process information captured and exploited by multiple tasks. In particular, we propose a novel splitting network design, which is empowered with multi-head attention, and generates dynamic masks to filter task specific information and task agnostic shared factors from the input. To drive this generation, and to avoid the oversharing of information between the tasks, we propose a novel formulation of the mutual information loss which encourages the generated split embeddings to be distinct as possible. A unique merging network is also introduced to fuse the task specific, and shared information and generate an augmented embedding for the individual downstream tasks in the MTL pipeline. We evaluate the proposed Split ‘n’ Merge Network on two distinct MTL tasks where we achieve state-of-the-art results for both. Our primary, ablation and interpretation evaluations indicate the robustness and flexibility of the propose approach and demonstrates its applicability to numerous, diverse real-world MTL applications.}
}
@article{PIAO2022108523,
title = {AccLoc: Anchor-Free and two-stage detector for accurate object localization},
journal = {Pattern Recognition},
volume = {126},
pages = {108523},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108523},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000048},
author = {Zhengquan Piao and Junbo Wang and Linbo Tang and Baojun Zhao and Wenzheng Wang},
keywords = {Object detection, Accurate localization, Anchor-free, NMS-free, Two-stage},
abstract = {Current anchor-free object detectors have obtained detection performances comparable to those of anchor-based object detectors while avoiding the weaknesses of anchor designs. However, two challenges limit the localization performance. First, such anchor-free detectors have one stage that predicts the classification and localization results directly. A large regression space reduces the localization performance of such methods. Second, most of the existing detectors extract features which are ineffective for accurate localization. In this paper, for the first challenge, we propose two-stage networks to predict regression results stage by stage, thereby reducing the scope of the prediction space. For the second challenge, we design two novel modules with the aim of extracting effective features for accurate localization. Experimental results validate that each module in our approach is effective and validate that our approach has better object localization performance than previous related and advanced methods.}
}
@article{ZHOU2022108367,
title = {A unified deep sparse graph attention network for scene graph generation},
journal = {Pattern Recognition},
volume = {123},
pages = {108367},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108367},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005471},
author = {Hao Zhou and Yazhou Yang and Tingjin Luo and Jun Zhang and Shuohao Li},
keywords = {Scene graph generation, Statistical co-occurrence knowledge, Relationship measurement network, Graph attention network, Sparse graph},
abstract = {Scene graph generation (SGG) plays an important role in deep understanding of the visual scene. Despite the empirical success of traditional methods in many applications, they still have several challenges in the high computational complexity of dense graph and the inaccurate pruning of sparse graph. To tackle these problems, we propose a novel deep sparse graph attention network to mine the rich contextual clues and simultaneously preserve the statistical co-occurrence knowledge of SGG. Specifically, our Relationship Measurement Network (RelMN) is adapted to first classify all object pairs in dense graph as the foreground and background categories to filter the false relationships and then construct a sparse graph efficiently. Meanwhile, we design a novel feature aggregation and update method via graphical message passing to jointly learn the node and edge features for object recognition and relationship classification in the graph attention network. Extensive experimental results on the large scale VG and VRD datasets demonstrate our proposed method outperforms several state-of-the-art approaches.}
}
@article{ALI2022108559,
title = {Energy minimization for image focus volume in shape from focus},
journal = {Pattern Recognition},
volume = {126},
pages = {108559},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108559},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000401},
author = {Usman Ali and Muhammad Tariq Mahmood},
keywords = {Shape from focus, Energy minimization, Focus volume optimization},
abstract = {In shape from focus (SFF) methods, the quality of depth map is mainly dependent on the accuracy level of image focus volume. Most of the SFF techniques optimize focus volume without incorporating any prior or additional structural information about the scene and thus resultant depth maps are deteriorated. We mitigate this deficiency by proposing to optimize focus volume through energy minimization. The proposed energy function contains smoothness and structural similarity along with data term. Smoothness constraint enforces spatial coherence while structural similarity constraint tries to preserve structures which are consistent with image sequence. This results in an optimized focus volume that imitates the underlying scene accurately. For the implementation of our 3D objective function, we employ an efficient technique that decomposes the problem into a sequence of 1D simple sub-problems. Experiments conducted on synthetic and real image sequences from a variety of datasets demonstrate that the proposed method optimizes the focus volume effectively and thus provides improved depth maps.}
}
@article{HUANG2022108583,
title = {Bag dissimilarity regularized multi-instance learning},
journal = {Pattern Recognition},
volume = {126},
pages = {108583},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108583},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000644},
author = {Shiluo Huang and Zheng Liu and Wei Jin and Ying Mu},
keywords = {Multi-instance learning (MIL), Dissimilarity regularization, Fisher score},
abstract = {Multi-instance learning (MIL) is able to cope with the weakly supervised problems where the training data is represented by labeled bags consisting of multiple unlabeled instances. Due to its practical significance, MIL has recently drawn increasing attention. Introducing bag representations is an attractive way to learn MIL data. However, it is difficult for the existing MIL methods to utilize both implicit and explicit bag representations simultaneously. In this paper, we propose a bag dissimilarity regularized (BDR) framework that incorporates multiple bag representations regardless of explicitness or implicitness. Here, the implicit bag representations are incorporated into a regularization term that contains the intrinsic geometric information provided by the bag dissimilarities. The regularization term can be added to the objective function of supervised classifiers. An effective method for explicit bag embedding is also proposed, which exploits the Fisher score derived from factor analysis. Finally, we propose two specific BDR methods based on support vector machine and broad learning system. The proposed BDR methods are evaluated on 14 datasets, and have achieved competitive results with limited computation consumption. We also discuss the effectiveness and the characteristics of BDR framework.}
}
@article{LUO2022108586,
title = {Meta-seg: A survey of meta-learning for image segmentation},
journal = {Pattern Recognition},
volume = {126},
pages = {108586},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108586},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200067X},
author = {Shuai Luo and Yujie Li and Pengxiang Gao and Yichuan Wang and Seiichi Serikawa},
keywords = {Deep learning, Image segmentation, Meta-learning, Computer vision},
abstract = {A well-performed deep learning model in image segmentation relies on a large number of labeled data. However, it is hard to obtain sufficient high-quality raw data in industrial applications. Meta-learning, one of the most promising research areas, is recognized as a powerful tool for approaching image segmentation. To this end, this paper reviews the state-of-the-art image segmentation methods based on meta-learning. We firstly introduce the background of the image segmentation, including the methods and metrics of image segmentation. Second, we review the timeline of meta-learning and give a more comprehensive definition of meta-learning. The differences between meta-learning and other similar methods are compared comprehensively. Then, we categorize the existing meta-learning methods into model-based, optimization-based, and metric-based. For each categorization, the popular used meta-learning models are discussed in image segmentation. Next, we conduct comprehensive computational experiments to compare these models on two pubic datasets: ISIC-2018 and Covid-19. Finally, the future trends of meta-learning in image segmentation are highlighted.}
}
@article{CHEN2022108539,
title = {CSDA-Net: Seeking reliable correspondences by channel-Spatial difference augment network},
journal = {Pattern Recognition},
volume = {126},
pages = {108539},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108539},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000206},
author = {Shunxing Chen and Linxin Zheng and Guobao Xiao and Zhen Zhong and Jiayi Ma},
keywords = {Feature matching, Deep learning, Outlier rejection, Attention mechanism},
abstract = {Establishing reliable correspondences is a fundamental task in computer vision, and it requires rich contextual information. In this paper, we propose a Channel-Spatial Difference Augment Network (CSDA-Net), by selectively aggregating information from spatial and channel aspects, to seek reliable correspondences for feature matching. Specifically, we firstly introduce the spatial and channel attention mechanism to construct a simple yet effective block for discriminately extracting the global context. After that, we design a Overlay Attention block by further exploiting the spatial and channel attention mechanism with different squeeze operations, to gather more comprehensive contextual information. Finally, the proposed CSDA-Net is able to achieve feature maps with a strong representative ability for feature matching due to the integration of the two novel blocks. Extensive experiments on outlier rejection and relative pose estimation have shown better performance improvements of our CSDA-Net over current state-of-the-art methods on both outdoor and indoor datasets.}
}
@article{SUN2022108563,
title = {GAN for vision, KG for relation: A two-stage network for zero-shot action recognition},
journal = {Pattern Recognition},
volume = {126},
pages = {108563},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108563},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000449},
author = {Bin Sun and Dehui Kong and Shaofan Wang and Jinghua Li and Baocai Yin and Xiaonan Luo},
keywords = {Action recognition, Zero-shot learning, Generative adversarial networks, Graph convolution network},
abstract = {Zero-shot action recognition can recognize samples of unseen classes that are unavailable in training by exploring common latent semantic representation in samples. However, most methods neglected the connotative relation and extensional relation between the action classes, which leads to the poor generalization ability of the zero-shot learning. Furthermore, the learned classifier inclines to predict the samples of seen class, which leads to poor classification performance. To solve the above problems, we propose a two-stage deep neural network for zero-shot action recognition, which consists of a feature generation sub-network serving as the sampling stage and a graph attention sub-network serving as the classification stage. In the sampling stage, we utilize generative adversarial networks (GAN) trained by action features and word vectors of seen classes to synthesize the action features of unseen classes, which can balance the training sample data of seen classes and unseen classes. In the classification stage, we construct a knowledge graph (KG) based on the relationship between word vectors of action classes and related objects, and propose a graph convolution network (GCN) based on attention mechanism, which dynamically updates the relationship between action classes and objects, and enhances the generalization ability of zero-shot learning. In both stages, we all use word vectors as bridges for feature generation and classifier generalization from seen classes to unseen classes. We compare our method with state-of-the-art methods on UCF101 and HMDB51 datasets. Experimental results show that our proposed method improves the classification performance of the trained classifier and achieves higher accuracy.}
}