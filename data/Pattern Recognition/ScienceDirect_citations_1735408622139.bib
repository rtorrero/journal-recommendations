@article{2023109706,
title = {Editorial Board},
journal = {Pattern Recognition},
volume = {141},
pages = {109706},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(23)00404-1},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004041}
}
@article{WEN2023109606,
title = {Multi-level progressive transfer learning for cervical cancer dose prediction},
journal = {Pattern Recognition},
volume = {141},
pages = {109606},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109606},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003072},
author = {Lu Wen and Jianghong Xiao and Jie Zeng and Chen Zu and Xi Wu and Jiliu Zhou and Xingchen Peng and Yan Wang},
keywords = {Radiation therapy, Dose prediction, Transfer learning, Deep neural network},
abstract = {Recently, deep learning has accomplished the automation of radiation therapy planning, enhancing its quality and efficiency. However, such progress comes at the cost of a large amount of clinical data. For some low-incidence cancers, i.e., cervical cancer, with limited available data, current data-hungry deep models fail to achieve satisfactory performance. To address this, in this paper, considering that cervical cancer and rectum cancer share the same scanning area and organs at risk (OARs), we resort to transfer learning to transfer the knowledge acquired from rectum cancer (source domain) to cervical cancer (target domain) to perform dose map prediction task. To overcome the possible negative transferring problem, we design a two-phase paradigm to progressively transfer knowledge. In the first phase, we aggregate the data of the two domains by linear interpolation and pre-train an aggregated network with the aggregated data to perceive the target dose distribution beforehand. In the second phase, we elaborately design two modules, i.e., a Feature-level Transfer (FT) Module, and an Image-level Transfer (IT) Module, to selectively transfer knowledge in multi-level. Specifically, the FT module aims to preserve those filters that are more helpful while the IT module tries to highlight those samples with more target-specific knowledge. Extensive experiments proclaim the exemplary performance of our proposed method compared with other state-of-the-art methods.}
}
@article{WANG2023109634,
title = {Hypercomplex context guided interaction modeling for scene graph generation},
journal = {Pattern Recognition},
volume = {141},
pages = {109634},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109634},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003357},
author = {Zheng Wang and Xing Xu and Yadan Luo and Guoqing Wang and Yang Yang},
keywords = {Scene graph generation, Context guidence, Interaction modeling, Hypercomplex embedding},
abstract = {Intuitively, humans can consciously and subjectively attend to the interactions between objects, and thus infer reasonable visual relations. However, mainstream approaches of Scene Graph Generation (SGG) strive to alleviate the long-tailed distribution problem with various complicated re-weighting strategies, where a simple concatenation of the refined object features is treated as the final representation of visual relations. In spite of their remarkable progress, such an operation overlooks the importance of interaction on relation recognition. To tackle the problem, this work devises a hyperComplex- Context guided Interaction Modeling (CCIM for short) plug-in, which can be successfully assimilated by the existing methods for performance improvement. Specifically, we first extract the contextual relation feature determined by the constraint relation≈union(head,tail)−headobject−tailobject. Then, we encode the features of relations and objects into hypercomplex space, with three imaginary components, to learn more expressive representations for SGG. Next, guided by the context, we can capture the interaction between a head or tail object and their relation through the Hamilton product. We further reinforce the interaction between enhanced hypercomplex-valued representations of the two entities with Quaternion inner product. At last, the concatenation of all components from the learned hypercomplex feature is adopted as our final relation representation. Extensive experiments on the popular benchmark Visual Genome in various existing approaches demonstrate the effectiveness and generalization of our proposed model-agnostic method under comprehensive evaluation metrics.}
}
@article{WANG2023109626,
title = {RPI-CapsuleGAN: Predicting RNA-protein interactions through an interpretable generative adversarial capsule network},
journal = {Pattern Recognition},
volume = {141},
pages = {109626},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109626},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003278},
author = {Yifei Wang and Xue Wang and Cheng Chen and Hongli Gao and Adil Salhi and Xin Gao and Bin Yu},
keywords = {RNA-protein interactions, elastic net, multi-information fusion, generative adversarial capsule network, interpretable, convolutional block attention module},
abstract = {ABSTRACT
RNA-protein interactions (RPI) play a crucial regulatory role in cellular physiological processes. The study and prediction of RPIs can be insightful for exploring disease mechanisms and drug target design. Traditional RPI prediction methods relied mainly on tedious and expensive biological experiments, and there is an increasing interest in developing more cost-effective computational methods to predict RPIs. This work proposes an interpretable RPI-CapsuleGAN method for RPI prediction based on a generative adversarial capsule network with a convolutional block attention module. First, RPI-CapsuleGAN extracts and fuses multiple features to characterize RNA and protein sequences. Subsequently, the elastic net feature selection method is used to retain features that are highly informative to RPI prediction. Finally, we introduce a convolutional attention mechanism into the generative adversarial capsule network for the first time in order to construct the RPI prediction framework, which is shown to improve the model feature learning of interpretable and expression ability, and effectively solves the problem of the disappearance of the model spatial structure hierarchy. Based on a five-fold cross-validation test, the prediction accuracy of the RPI-CapsuleGAN method reaches 97.1%, 88.8%, 92.5%, 97.3%, and 87.8% for datasets RPI488, RPI369, RPI2241, RPI1807, and RPI1446. The RPI-CapsuleGAN method has higher accuracy than state-of-the-art RPI prediction methods that use the same datasets. In the test dataset NPInter227 constructed in this paper, five groups of test sets are composed of positive samples and five groups of negative samples, the prediction accuracy reaches 97.38%, 96.48%, 97.38%, 97.81%, and 97.15%, respectively, outperforming other mainstream deep learning algorithms. In addition, RPI-CapsuleGAN obtained better results for the prediction of independent test datasets. Extensive experiments detailed here show that RPI-CapsuleGAN can provide an efficient, accurate, and stable method for RPI prediction.}
}
@article{ZHOU2023109602,
title = {SurroundNet: Towards effective low-light image enhancement},
journal = {Pattern Recognition},
volume = {141},
pages = {109602},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109602},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003035},
author = {Fei Zhou and Xin Sun and Junyu Dong and Xiao Xiang Zhu},
keywords = {Image processing, Image enhancement, Convolution Neural Network, Surround function, Lightweight},
abstract = {Although Convolution Neural Networks (CNNs) have made substantial progress in the low-light image enhancement task, one critical problem of CNNs is the paradox of model complexity and performance. This paper presents a novel SurroundNet that only involves less than 150K parameters (about 80–98 percent size reduction compared to SOTAs) and achieves very competitive performance. The proposed network comprises several Adaptive Retinex Blocks (ARBlock), which can be viewed as a novel extension of Single Scale Retinex in feature space. The core of our ARBlock is an efficient illumination estimation function called Adaptive Surround Function (ASF). It can be regarded as a general form of surround functions and be implemented by convolution layers. In addition, we also introduce a Low-Exposure Denoiser (LED) to smooth the low-light image before the enhancement. We evaluate the proposed method on two real-world low-light datasets. Experimental results demonstrate the superiority of our submitted SurroundNet in both performance and network parameters against State-of-the-Art low-light image enhancement methods. The code is available at https://github.com/ouc-ocean-group/SurroundNet.}
}
@article{BLANCOMALLO2023109646,
title = {Do all roads lead to Rome? Studying distance measures in the context of machine learning},
journal = {Pattern Recognition},
volume = {141},
pages = {109646},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109646},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003473},
author = {Eva Blanco-Mallo and Laura Morán-Fernández and Beatriz Remeseiro and Verónica Bolón-Canedo},
keywords = {Distance measures, Similarity measures, Classification, Clustering, Machine learning},
abstract = {Many machine learning and data mining tasks are based on distance measures, so a large amount of literature addresses this aspect somehow. Due to the broad scope of the topic, this paper aims to provide an overview of the use of these measures in the most common machine learning problems, pointing out those aspects to consider to choose the most appropriate measure for a particular task. For this purpose, the most recent works addressing the subject were reviewed and seven of the most commonly used measures were analyzed, investigating in detail their main properties and applications. Different experiments were carried out to study their relationships and compare their performance. The degradation of the results in the presence of noise was also considered, as well as the execution time required by each measure.}
}
@article{LIU2023109593,
title = {FontTransformer: Few-shot high-resolution Chinese glyph image synthesis via stacked transformers},
journal = {Pattern Recognition},
volume = {141},
pages = {109593},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109593},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002947},
author = {Yitian Liu and Zhouhui Lian},
keywords = {Font generation, Style transfer, Transformers},
abstract = {Automatic generation of high-quality Chinese fonts from a few online training samples is a challenging task, especially when the amount of samples is very small. Existing few-shot font generation methods can only synthesize low-resolution glyph images that often possess incorrect topological structures or/and incomplete strokes. To address the problem, this paper proposes FontTransformer, a novel few-shot learning model, for high-resolution Chinese glyph image synthesis by using stacked Transformers. The key idea is to apply the parallel Transformer to avoid the accumulation of prediction errors and utilize the serial Transformer to enhance the quality of synthesized strokes. Meanwhile, we also design a novel encoding scheme to feed more glyph information and prior knowledge to our model, which further enables the generation of high-resolution and visually-pleasing glyph images. Both qualitative and quantitative experimental results demonstrate the superiority of our method compared to other existing approaches in few-shot Chinese font synthesis task.}
}
@article{LYU2023109661,
title = {Process-Oriented heterogeneous graph learning in GNN-Based ICS anomalous pattern recognition},
journal = {Pattern Recognition},
volume = {141},
pages = {109661},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109661},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300362X},
author = {Shuaiyi L(y)u and Kai Wang and Liren Zhang and Bailing Wang},
keywords = {Fine-Grained anomaly recognition, Process-Oriented associativity, Heterogeneous graph learning, Industrial control systems},
abstract = {Over the past few years, massive penetrations targeting an Industrial Control System (ICS) network intend to compromise its core industrial processes. So far, numerous advanced methods have been proposed to detect anomalous patterns in the numeric data streams with respect to the heterogeneous field devices involved in the industrial processes. These methods, despite reporting decent results, usually conduct system-wise detection instead of fine-grained anomalous pattern recognition at the device level. Furthermore, lacking explicit consideration of the exclusive process-related features with respect to each differentiated device, the fitness of their application in specified industrial processes is undermined. To tackle these issues, a GNN-based Attributed Heterogeneous Graph Analyzer (the AHGA) is designed to perform device-wise anomalous pattern detection via in-depth process-oriented associativity learning. The AHGA’s framework is constructed with four building blocks: a graph processor, a feature analyzer, a link inference decoder, and an anomaly detector. Its performance is assessed and compared against multiple link inference and anomaly detection baselines over 2 popular ICS datasets (SWaT and WADI). Comparative results demonstrate the AHGA’s reliability in capturing sophisticated process-oriented relations among heterogeneous devices as well as its effectiveness in boosting the performance of anomalous pattern recognition at device-level granularity.}
}
@article{MA2023109585,
title = {Crowd counting from single images using recursive multi-pathway zooming and foreground enhancement},
journal = {Pattern Recognition},
volume = {141},
pages = {109585},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109585},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002868},
author = {Junjie Ma and Yaping Dai and Zhiyang Jia and Fuchun Sun and Yap-Peng Tan and Jun Liu},
keywords = {Crowd counting, Density estimation, Multi-Pathway zooming, Foreground enhancement},
abstract = {Crowd counting is a challenging task due to many challenges such as scale variations and noisy background. To handle these challenges, we propose a novel framework named Multi-Pathway Zooming Network (MZNet) in this paper. The proposed framework recursively optimizes multi-scale features using multiple zooming pathways and progressively enhances the foreground information to improve crowd counting performance. Each zooming pathway comprises two zooming directions, zooming in and zooming out. Convolutional features at different resolutions are propagated to optimize the context information at each specific level. By sequentially integrating and interacting multi-observation information, the optimized features are powerful in handling the scale variation issue, and thus the crowd counting performance can be enhanced. To address the noisy background in many scenarios, we also introduce a new scheme to enhance the foreground information by incorporating a masked input image into the network, which is formed by a mask that element-wise multiplies with the original image. Finally, the context information, incorporated with an output density map, is recursively finetuned in our network to boost the counting performance. Extensive experiments evaluated on challenging benchmark datasets show competitive performances for both crowded and sparse scenarios.}
}
@article{WANG2023109562,
title = {Coloring anime line art videos with transformation region enhancement network},
journal = {Pattern Recognition},
volume = {141},
pages = {109562},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109562},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002625},
author = {Ning Wang and Muyao Niu and Zhi Dou and Zhihui Wang and Zhiyong Wang and Zhaoyan Ming and Bin Liu and Haojie Li},
keywords = {, Anime video, Synthesis, GAN},
abstract = {Automatic colorization of anime line art videos aims to produce color frames given line art frames and reference color images, which is challenging due to various motions and geometric transformations across frame sequences. Existing methods usually utilize the feature maps of reference images directly and treat all the regions in an image equally. However, this may overlook the details of the regions undergoing geometric transformations. To emphasize the regions with significant transformations between the reference and target frames, we propose a Transformation Region Enhancement Network (TRE-Net) to exploit useful reference information and enhance the colorization of key transformation regions with Region Localization Module (RLM) and Feature Enhancement Module (FEM). Specifically, we propose Multi-scale Euclidean Distance Difference (Multi-scale EDD) Maps in RLM which effectively locate geometric transformation regions by contrasting the Euclidean Distance Maps of two line arts and aggregating representations at multiple scales of the network. In addition, FEM is devised to enhance feature learning in the regions with geometric transformation and to ensure proper color alignment. FEM learns locally enhanced features through an attention-gating operation at a low computational cost. With the well-represented key geometric transformation regions, our method exploits the multi-scale reference information well for color alignment, thus produces perceptually pleasing frames. Comprehensive experimental results show that our proposed method is superior to existing methods in terms of the overall quality of colorized anime line art videos.}
}
@article{XU2023109650,
title = {Tensor train factorization under noisy and incomplete data with automatic rank estimation},
journal = {Pattern Recognition},
volume = {141},
pages = {109650},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109650},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003515},
author = {Le Xu and Lei Cheng and Ngai Wong and Yik-Chung Wu},
keywords = {Bayesian inference, Tensor completion, Tensor train},
abstract = {As a powerful tool in analyzing multi-dimensional data, tensor train (TT) decomposition shows superior performance compared to other tensor decomposition formats. Existing TT decomposition methods, however, either easily overfit with noise, or require substantial fine-tuning to strike a balance between recovery accuracy and model complexity. To avoid the above shortcomings, this paper treats the TT decomposition in a fully Bayesian perspective, which includes automatic TT rank determination and noise power estimation. Theoretical justification on adopting the Gaussian-product-Gamma priors for inducing sparsity on the slices of the TT cores is provided, thus allowing the model complexity to be automatically determined even when the observed tensor data is noisy and contains many missing values. Furthermore, using the variational inference framework, an effective learning algorithm on the probabilistic model parameters is derived. Simulations on synthetic data demonstrate that the proposed algorithm accurately recovers the underlying TT structure from incomplete noisy observations. Further experiments on image and video data also show its superior performance to other existing TT decomposition algorithms.}
}
@article{SEPULVEDA2023109642,
title = {Generalization of deep learning models for natural gas indication in 2D seismic data},
journal = {Pattern Recognition},
volume = {141},
pages = {109642},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109642},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003436},
author = {Luis Fernando Marin Sepulveda and Marcelo Gattass and Aristofanes Correa Silva and Roberto Quevedo and Diogo Michelon and Carlos Siedschlag and Roberto Ribeiro},
keywords = {Autoencoder, Generalizability, Dataset training recommendation, 2D Seismic onshore data, Deep learning, Clustering},
abstract = {Methods based on Machine Learning and Deep Learning are increasingly popular to help interpret large volumes of data that belong to various areas and seek to fulfill multiple tasks. One of these areas studies seismic data in the search for hydrocarbon reserves, for which Deep Learning models are trained, showing acceptable results for low study data. However, these models present generalization problems. Their performance tends to decrease when used on seismic data from new exploration. This tendency is particularly true for 2D data, which have many features. This work presents a method to improve the generalization of the Deep Learning model for the indication of natural gas in 2D seismic data based on the recommendation of training data and hyperparameter operations of the model. The tests used a database of the Parnaíba basin in northeast Brazil. Experiments showed an increase in the correct indication of natural gas that varies according to the metric 8%≤Recall≤37%, with a fluctuation in the increase of false positives of −2%≤Precision≤13%. It is an improvement in the generalization of the Deep Learning model of up to 11% according to the F1 score metric or up to 10% according to the IoU metric.}
}
@article{LYU2023109581,
title = {Corrigendum to “FETNet: Feature Erasing and Transferring Network for Scene Text Removal”: Pattern Recognition Volume 140 (2023) 109531},
journal = {Pattern Recognition},
volume = {141},
pages = {109581},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109581},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002820},
author = {Guangtao Lyu and Kun Liu and Anna Zhu and Seiichi Uchida and Brian Kenji Iwana}
}
@article{ZHANG2023109667,
title = {Slide deep reinforcement learning networks: Application for left ventricle segmentation},
journal = {Pattern Recognition},
volume = {141},
pages = {109667},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109667},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003680},
author = {Wanjun Zhang and Xiaohua Ding and Yang Liu and Baojun Qiao},
keywords = {Slide deep reinforcement learning, Pixelwise image segmentation, Left ventricle, Reinforcement learning state, Reinforcement learning action},
abstract = {Automatic segmentation of the left ventricle (LV) in four-chamber view images is critical for computer-aided cardiac disease diagnosis. The complex structure of the cardiac image and the encoder-decoder networks may cause coarse segmentation results. High-accuracy LV segmentation is still a challenge with existing automatic LV segmentation methods. In this paper, we propose a slide deep reinforcement learning segmentation network for pixelwise LV segmentation. The main architecture of the slide reinforcement learning networks consists of a slider item combined state, a group of morphology transforming actions and an agent network. The specifically designed reinforcement learning state comprises an image item and a slider item, which contains both original image information and network act information. The reinforcement learning actions proposed in this paper enable accurate and fast formulation of the binary segment result for each frame by controlling the length and location of the slider. Additionally, the confidence branch proposed in our experiment provides a continuous frame series environment, and the identification algorithm avoids losing the segmentation target. The segmentation result reveals that the proposed method outperforms FCN, SegNet, U-Net and TransUnet. The IoU improved by 23.01%, 15.4%, 11.24% and 6.9%. Additionally, we demonstrate how the proposed method can be used as a semisupervised method, which is more convenient for the image annotation process.}
}
@article{SOUTOARIAS2023109607,
title = {AIDA: Analytic isolation and distance-based anomaly detection algorithm},
journal = {Pattern Recognition},
volume = {141},
pages = {109607},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109607},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003084},
author = {Luis Antonio {Souto Arias} and Cornelis W. Oosterlee and Pasquale Cirillo},
keywords = {Outlier detection, Anomaly explanation, Isolation, Distance, Ensemble methods},
abstract = {Many unsupervised anomaly detection algorithms rely on the concept of nearest neighbours to compute the anomaly scores. Such algorithms are popular because there are no assumptions about the data, making them a robust choice for unstructured datasets. However, the number (k) of nearest neighbours, which critically affects the model performance, cannot be tuned in an unsupervised setting. Hence, we propose the new and parameter-free Analytic Isolation and Distance-based Anomaly (AIDA) detection algorithm, that combines the metrics of distance with isolation. Based on AIDA, we also introduce the Tempered Isolation-based eXplanation (TIX) algorithm, which identifies the most relevant features characterizing an outlier, even in large multi-dimensional datasets, improving the overall explainability of the detection mechanism. Both AIDA and TIX are thoroughly tested and compared with state-of-the-art alternatives, proving to be useful additions to the existing set of tools in anomaly detection.}
}
@article{AHMAD2023109635,
title = {Robust federated learning under statistical heterogeneity via Hessian spectral decomposition},
journal = {Pattern Recognition},
volume = {141},
pages = {109635},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109635},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003369},
author = {Adnan Ahmad and Wei Luo and Antonio Robles-Kelly},
keywords = {Federated learning, Hessian, Non-IID data},
abstract = {Federated Learning (FL) is a collaborative machine learning paradigm in which a global model is learned via aggregating local ones. Although statistical heterogeneity of the local training data is necessary for the generalisability of the global model, it also introduces local model “drift” that slows down the convergence. Thus, how to optimally aggregate local models in FL remains an open problem. Recognising that training data lends varying evidential credence to different parts of a local model, we propose a novel approach to exploit such evidential asymmetry in FL aggregation in not independent and identically distributed (non-IID) data by applying a unique weight coefficient to each of the local parameter updates. To this end, we measure the parameter-level evidential credence making use of the eigenvalues of the Hessian of the local likelihood function, which are theoretically connected to the observed Fisher information. We employ these eigenpairs to propose a novel aggregation method, which we name FedHess. Our experiments show FedHess achieves smoother and faster convergence to a more accurate global model when compared with popular baselines such as Federated Average (FedAvg), FedProx, SCAFFOLD, Federated Curvature (FedCurv) and FedDF across different types of heterogeneous training data drawn from a number of benchmark datasets.}
}
@article{ZHANG2023109611,
title = {Adaptive fusion affinity graph with noise-free online low-rank representation for natural image segmentation},
journal = {Pattern Recognition},
volume = {141},
pages = {109611},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109611},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003126},
author = {Yang Zhang and Moyun Liu and Huiming Zhang and Guodong Sun and Jingwu He},
keywords = {Graph, Image segmentation, Sparse subspace clustering, Low-rank representation, Affinity propagation},
abstract = {Affinity graph-based segmentation methods have become a major trend in computer vision. The performance of these methods rely on the constructed affinity graph, with particular emphasis on the neighborhood topology and pairwise affinities among superpixels. However, these graph-based methods ignore the noisy data from images, that influence the accuracy of pairwise similarities. Multiscale combinatorial grouping and graph fusion also generate a higher computational complexity. In this paper, we propose an adaptive fusion affinity graph with noise-free low-rank representation in an online manner for natural image segmentation. An input image is first over-segmented into superpixels at different scales and then filtered by an improved kernel density estimation method. Moreover, we select global nodes of these superpixels on the basis of their subspace-preserving presentation, which reveals the feature distribution of superpixels exactly. To reduce time complexity while improving performance, a sparse representation of global nodes based on noise-free online low-rank representation is used to obtain a global graph at each scale. Experimental results on BSD300, BSD500, MSRC, SBD, and PASCAL VOC show the effectiveness of our method in comparison with the state-of-the-art approaches. The code is available at https://github.com/Yangzhangcst/AFA-graph.}
}
@article{THAKUR2023109603,
title = {Multi scale pixel attention and feature extraction based neural network for image denoising},
journal = {Pattern Recognition},
volume = {141},
pages = {109603},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109603},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003047},
author = {Ramesh Kumar Thakur and Suman Kumar Maji},
keywords = {Blind Gaussian noise removal, Deep convolutional residual network, Convolutional layer, Residual architecture, Dilated convolution, Skip connection},
abstract = {In this paper, we propose a blind Gaussian denoising network that utilize the features of the input image and its negative for generating denoised output of the same. The proposed network is a dual path model which employs a multi-scale pixel attention (MSPA) block on one path and a multi-scale feature extraction (MSFE) block on another. The concept of using the features of a negative image (that it highlights the low contrast region) in blind Gaussian denoising network is, to the best of our knowledge, a first such attempt. The proposed MSPA and MSFE blocks are designed to focus on the features of the image at multiple scales. The MSPA block focuses on the important features of the negative of the input image whereas the MSFE block focuses on extracting features of the input noisy image. The features of both the images are then combined and a final residual noise is obtained, subtracting which from the input noisy image produces the final denoised result. The proposed network is lightweight and fast, due to the low number of convolutional layers involved, and produces superior results (both quantitatively and qualitatively) when compared with various traditional and learning based blind Gaussian denoising techniques. The code of this paper can be downloaded from https://github.com/RTSIR/NIFBGDNet.}
}
@article{LI2023109631,
title = {Multi-hypothesis representation learning for transformer-based 3D human pose estimation},
journal = {Pattern Recognition},
volume = {141},
pages = {109631},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109631},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003321},
author = {Wenhao Li and Hong Liu and Hao Tang and Pichao Wang},
keywords = {3D Human pose estimation, Transformer, Multi-Hypothesis, Self-Hypothesis, Cross-Hypothesis},
abstract = {Despite significant progress, estimating 3D human poses from monocular videos remains a challenging task due to depth ambiguity and self-occlusion. Most existing works attempt to solve both issues by exploiting spatial and temporal relationships. However, those works ignore the fact that it is an inverse problem where multiple feasible solutions (i.e., hypotheses) exist. To relieve this limitation, we propose a Multi-Hypothesis Transformer that learns spatio-temporal representations of multiple plausible pose hypotheses. In order to effectively model multi-hypothesis dependencies and build strong relationships across hypothesis features, we introduce a one-to-many-to-one three-stage framework: (i) Generate multiple initial hypothesis representations; (ii) Model self-hypothesis communication, merge multiple hypotheses into a single converged representation and then partition it into several diverged hypotheses; (iii) Learn cross-hypothesis communication and aggregate the multi-hypothesis features to synthesize the final 3D pose. Through the above processes, the final representation is enhanced and the synthesized pose is much more accurate. Extensive experiments show that the proposed method achieves state-of-the-art results on two challenging datasets: Human3.6M and MPI-INF-3DHP. The code and models are available at https://github.com/Vegetebird/MHFormer.}
}
@article{RAKESH2023109623,
title = {An improved differential evolution algorithm for quantifying fraudulent transactions},
journal = {Pattern Recognition},
volume = {141},
pages = {109623},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109623},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003242},
author = {Deepak Kumar Rakesh and Prasanta K. Jana},
keywords = {Quantifying fraudulent transactions (QFT), Cost-based feature selection, Multiobjective optimization, Differential evolution},
abstract = {Identification of fraudulent credit card transactions is a complex problem mainly due to the following factors: 1) The relative behavior of customers and fraudsters may alter over time. 2) The ratio of legitimate to fraudulent transactions is highly imbalanced, and 3) Investigators examine a small segment of transactions in a reasonable time frame. Researchers have proposed various algorithms to identify potential fraud in a new incoming transaction. However, these approaches require significant human investigator effort and are sometimes misleading. To address this issue, this paper proposes an improved multiobjective differential evolution (DE) algorithm to estimate the distribution of fraudulent transactions in a set of new incoming transactions, referred to as quantifying fraudulent transactions. Our paper has three major novelties. First, we present the problem formulation of cost-based feature selection with maximum quantification ability. Second, we improve the DE by applying effective trial vector generation algorithms to the random control parameter settings to exploit the advantage of individual DE variants. Third, we develop the maximum-relevancy-minimum-redundancy-based Pareto refining operator to enhance the self-learning ability of individuals in Pareto solutions. We compare our approach against four other modifications of DE and five state-of-the-art evolutionary algorithms on real-time credit datasets in streaming and non-streaming frameworks using hyper-volume, two-set coverage, and spread performance metrics.}
}
@article{PAN2023109594,
title = {Few-shot classification with task-adaptive semantic feature learning},
journal = {Pattern Recognition},
volume = {141},
pages = {109594},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109594},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002959},
author = {Mei-Hong Pan and Hong-Yi Xin and Chun-Qiu Xia and Hong-Bin Shen},
keywords = {Few-shot learning, Multi-modality, Task-adaptive training, Semantic feature learner},
abstract = {Few-shot classification aims to learn a classifier that categorizes objects of unseen classes with limited samples. One general approach is to mine as much information as possible from limited samples. This can be achieved by incorporating data from multiple modalities. However, existing multi-modality methods only use additional modality in support samples while adhering to a single modal in query samples. Such approach could lead to information imbalance between support and query samples, which confounds model generalization from support to query samples. Towards this problem, we propose a task-adaptive semantic feature learning mechanism to incorporate semantic features for both support and query samples. The semantic feature learner is trained episodic-wisely by regressing from the feature vectors of the support samples. It is utilized to predict semantic features for the query samples. Such method maintains a consistent training scheme between support and query samples and enables direct model transfer from support to query data, which significantly improves model generalization. We conduct extensive experiments on four benchmarks in both inductive and transductive settings. Results show that the proposed TasNet outperforms state-of-the-art methods with an improvement of 1% to 5% in classification accuracy, demonstrating the superiority of our method. The exhaustive ablation studies further validate the effectiveness of our framework. The code is available at: https://github.com/pmhDL/TasNet}
}
@article{KIM2023109659,
title = {Improved robustness of vision transformers via prelayernorm in patch embedding},
journal = {Pattern Recognition},
volume = {141},
pages = {109659},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109659},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003606},
author = {Bum Jun Kim and Hyeyeon Choi and Hyeonah Jang and Dong Gu Lee and Wonseok Jeong and Sang Woo Kim},
keywords = {Vision transformer, Patch embedding, Contrast enhancement, Robustness, Layer normalization, Convolutional neural network, Deep learning},
abstract = {Vision Transformers (ViTs) have recently demonstrated state-of-the-art performance in various vision tasks, replacing convolutional neural networks (CNNs). However, because ViT has a different architectural design than CNN, it may behave differently. To investigate whether ViT has a different performance or robustness, we tested ViT and CNN under various imaging conditions in practical vision tasks. We confirmed that for most image transformations, ViT’s robustness was comparable or even better than that of CNN. However, for contrast enhancement, ViT performed particularly poorly. We show that this is because positional embedding in ViT’s patch embedding can work improperly when the color scale changes. We demonstrate that the use of PreLayerNorm, a modified patch embedding structure, ensures the consistent behavior of ViT. Results demonstrate that ViT with PreLayerNorm exhibited improved robustness in the contrast-varying environments.}
}
@article{MENG2023109630,
title = {SiamRank: A siamese based visual tracking network with ranking strategy},
journal = {Pattern Recognition},
volume = {141},
pages = {109630},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109630},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300331X},
author = {Feiyu Meng and Xiaomei Gong and Yi Zhang},
keywords = {Visual tracking, Siamese, Ranking, Classification, State estimation},
abstract = {Visual tracking is one of the most fundamental and active research topics in the field of computer vision with industrial applications. It has to solve 2 core problems, namely classification and state estimation. Most of the existing trackers utilize deep networks to extract the features of the object. Especially, Siamese based approaches have prevailed in tracking tasks, which generate labels for both positive and negative samples. However, these approaches introduce ambiguities and inaccurate semantic information at the same time, which may cause failure in classification. To address this problem, we present SiamRank by adding the sequential information of different samples in one image. We apply the proposed network to 2 backbones (AlexNet and GoogLeNet) to testify its general performance. Extensive experiments have been carried out on 7 popular benchmarks, including OTB100, LaSOT, GOT-10 K, TrackingNet, NFS, UAV123 and VOT2019, and our tracker achieves state-of-the-art results. Specifically, on both large-scale TrackingNet dataset and long-time LaSOT dataset, SiamRank surpasses the previous approaches with a relative gain of 10%, while running at 65 FPS.}
}
@article{ZHANG2023109671,
title = {Orthonormal product quantization network for scalable face image retrieval},
journal = {Pattern Recognition},
volume = {141},
pages = {109671},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109671},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003722},
author = {Ming Zhang and Xuefei Zhe and Hong Yan},
keywords = {Product quantization, Face image retrieval, Orthonormal codewords, Convolutional neural networks},
abstract = {Existing deep quantization methods provided an efficient solution for large-scale image retrieval. However, the significant intra-class variations, like pose, illumination, and expressions in face images, still pose a challenge. In light of this, face image retrieval requires sufficiently powerful learning metrics, which are absent in current deep quantization works. Moreover, to tackle the growing unseen identities in the query stage, face image retrieval drives more demands regarding model generalization and scalability than general image retrieval tasks. This paper integrates product quantization with orthonormal constraints into an end-to-end deep learning framework to effectively retrieve face images. Specifically, we propose a novel scheme that uses predefined orthonormal vectors as codewords to enhance the quantization informativeness and reduce codewords’ redundancy. A tailored loss function maximizes discriminability among identities in each quantization subspace for both the quantized and original features. An entropy-based regularization term is imposed to reduce the quantization error. Experiments are conducted on four commonly-used face datasets under both seen and unseen identity retrieval settings. Our method outperforms all the compared state-of-the-art under both settings. The proposed orthonormal codewords consistently boost both models’ standard retrieval performance and generalization ability, demonstrating the superiority of our method for scalable face image retrieval.}
}
@article{XIANG2023109608,
title = {Margin-aware rectified augmentation for long-tailed recognition},
journal = {Pattern Recognition},
volume = {141},
pages = {109608},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109608},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003096},
author = {Liuyu Xiang and Jungong Han and Guiguang Ding},
keywords = {Long-tailed recognition, Data augmentation, Mixup},
abstract = {The long-tailed data distribution is prevalent in real world and it poses great challenge on deep neural network training. In this paper, we propose Margin-aware Rectified Augmentation (MRA) to tackle this problem. Specifically, the MRA consists of two parts. From the data perspective, we analyze that data imbalance will cause the decision boundary be biased, and we propose a novel Margin-aware Rectified mixup (MR-mixup) that adaptively rectifies the biased decision boundary. Furthermore, from the model perspective, we analyze that the imbalance will also lead to consistent ‘gradient suppression’ on minority class logits. Then we propose Reweighted Mutual Learning (RML) that provides extra ‘soft target’ as supervision signal and augments the ‘encouraging gradients’ on the minority classes. We conduct extensive experiments on benchmark datasets CIFAR-LT, ImageNet-LT and iNaturalist18. The results demonstrate that the proposed MRA not only achieves state-of-the-art performance, but also yields a better-calibrated prediction.}
}
@article{LIU2023109636,
title = {BDNet: A BERT-based dual-path network for text-to-image cross-modal person re-identification},
journal = {Pattern Recognition},
volume = {141},
pages = {109636},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109636},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003370},
author = {Qiang Liu and Xiaohai He and Qizhi Teng and Linbo Qing and Honggang Chen},
keywords = {Person re-identification, Image-text retrieval, Cross-modality, Attention},
abstract = {Text-to-image person re-identification (TI-ReID) aims to provide a descriptive sentence to find a specific person in the gallery. The task is very challenging due to the huge feature differences between both image and text descriptions. Currently, most approaches use the idea of combining global and local features to get more fine-grained features. However, these methods usually acquire local features with the help of human pose or segmentation models, which makes it difficult to use in realistic scenarios due to the introduction of additional models or complex training evaluation strategies. To facilitate practical applications, we propose a BERT-based framework for dual-path TI-ReID. Without the help of additional models, our approach directly employs visual attention in the global feature extraction network to allow the network to adaptively learn to focus on salient local features in image and text descriptions, which enhances the network’s attention to local information through a visual attention mechanism, thus strengthening the global feature representation and effectively improving the global feature representation. In addition, to learn text and image modality invariant feature representations, we propose a convolutional shared network (CSN) to learn image and text features together. To optimize cross-modal feature distances more effectively, we propose a global hybrid modal triplet global metric loss. In addition to combining local metric learning and global metric learning, we also introduce the CMPM loss and CMPC loss to jointly optimize the proposed model. Extensive experiments on the CUHK-PEDES dataset show that the proposed method performs significantly better than the current research results, achieving a Rank-1/mAP accuracy of 66.27%/ 57.04%.}
}
@article{LIU2023109628,
title = {Deepfacelab: Integrated, flexible and extensible face-swapping framework},
journal = {Pattern Recognition},
volume = {141},
pages = {109628},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109628},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003291},
author = {Kunlin Liu and Ivan Perov and Daiheng Gao and Nikolay Chervoniy and Wenbo Zhou and Weiming Zhang},
keywords = {Face swapping, Practical machine learning, Open source},
abstract = {Face swapping has drawn a lot of attention for its compelling performance. However, current deepfake methods suffer the effects of obscure workflow and poor performance. To solve these problems, we present DeepFaceLab, the current dominant deepfake framework for practical face-swapping. It provides the necessary tools as well as an easy-to-use way to conduct high-quality face-swapping. It also offers a flexible and loose coupling structure for people who need to strengthen their pipeline with other features without writing complicated boilerplate code. We detail the principles that drive the implementation of DeepFaceLab and introduce its pipeline. DeepFaceLab could achieve cinema-level results with high fidelity as our supplemental video shows. We also demonstrate the advantage of our system by comparing our approach with other face-swapping methods. Deepfake defense not only requires the research of detection but also requires the efforts of generation methods. As for a popular and practical toolkit, we encourage users to promote harmless deepfake-entertainment content on social media, reminding the public of the existence of deepfake when they are looking for entertainment.}
}
@article{ZHANG2023109664,
title = {Faster OreFSDet: A lightweight and effective few-shot object detector for ore images},
journal = {Pattern Recognition},
volume = {141},
pages = {109664},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109664},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003655},
author = {Yang Zhang and Le Cheng and Yuting Peng and Chengming Xu and Yanwei Fu and Bo Wu and Guodong Sun},
keywords = {Ore images, Few-shot object detection, Real-time, Light-weight},
abstract = {For the ore particle size detection, obtaining a sizable amount of high-quality ore labeled data is time-consuming and expensive. General object detection methods often suffer from severe over-fitting with scarce labeled data. Despite their ability to eliminate over-fitting, existing few-shot object detectors encounter drawbacks such as slow detection speed and high memory requirements, making them difficult to implement in a real-world deployment scenario. To this end, we propose a lightweight and effective few-shot detector to achieve competitive performance with general object detection with only a few samples for ore images. First, the proposed support feature mining block characterizes the importance of location information in support features. Next, the relationship guidance block makes full use of support features to guide the generation of accurate candidate proposals. Finally, the dual-scale semantic aggregation module retrieves detailed features at different resolutions to contribute with the prediction process. Experimental results show that our method consistently exceeds the few-shot detectors with an excellent performance gap on all metrics. Moreover, our method achieves the smallest model size of 19 MB as well as being competitive at 50 FPS detection speed compared with general object detectors.}
}
@article{NAPOLES2023109640,
title = {Presumably correct decision sets},
journal = {Pattern Recognition},
volume = {141},
pages = {109640},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109640},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003412},
author = {Gonzalo Nápoles and Isel Grau and Agnieszka Jastrzębska and Yamisleydi Salgueiro},
keywords = {Data analysis, Granular computing, Decision sets, Rough sets},
abstract = {The paper presents the presumably correct decision sets as a tool to analyze uncertainty in the form of inconsistency in decision systems. As a first step, problem instances are gathered into three regions containing weak members, borderline members, and strong members. This is accomplished by using the membership degrees of instances to their neighborhoods while neglecting their actual labels. As a second step, we derive the presumably correct and incorrect sets by contrasting the decision classes determined by a neighborhood function with the actual decision classes. We extract these sets from either the regions containing strong members or the whole universe, which defines the strict and relaxed versions of our theoretical formalism. These sets allow isolating the instances difficult to handle by machine learning algorithms as they are responsible for inconsistent patterns. The simulations using synthetic and real-world datasets illustrate the advantages of our model compared to rough sets, which is deemed a solid state-of-the-art approach to cope with inconsistency. In particular, it is shown that we can increase the accuracy of selected classifiers up to 36% by weighting the presumably correct and incorrect instances during the training process.}
}
@article{YU2023109656,
title = {Smoothing group L1/2 regularized discriminative broad learning system for classification and regression},
journal = {Pattern Recognition},
volume = {141},
pages = {109656},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109656},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003576},
author = {Dengxiu Yu and Qian Kang and Junwei Jin and Zhen Wang and Xuelong Li},
keywords = {Broad learning system, Discriminative, Sparsity, Smoothing group  regularization, Optimization},
abstract = {This paper presents the framework of the smoothing group L1/2 regularized discriminative broad learning system for pattern classification and regression. The core idea is to improve the sparseness of the standard broad learning system and improve performance on recognition and generalization. First, the ε-dragging technique is introduced into the standard broad learning system to relax regression targets and enlarge distances between categories. Then, we integrate the group L1/2 regularization to optimize the network architecture to achieve sparsity. For the original group L1/2 regularization, the objective function is non-convex and non-smooth, which is hard for theoretical analysis. Therefore, we propose a simple and effective smoothing technique, i.e.,smoothing group L1/2 regularization, which can effectively eliminate the deficiency of the original group L1/2 regularization. As a result, the final weights projection matrix has a compact form and shows discriminative power capability. In addition, the alternating direction method of multipliers was adopted to optimize the algorithm. The simulation results show that the proposed algorithm has redundancy control capability and improved performance on recognition and generalization. The simulation results proves the efficiency of the theoretical analysis.}
}
@article{SHABANI2023109604,
title = {Augmented bilinear network for incremental multi-stock time-series classification},
journal = {Pattern Recognition},
volume = {141},
pages = {109604},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109604},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003059},
author = {Mostafa Shabani and Dat Thanh Tran and Juho Kanniainen and Alexandros Iosifidis},
keywords = {Deep learning, Low rank tensor decomposition, Limit order book data, Financial time-series analysis},
abstract = {Deep Learning models have become dominant in tackling financial time-series analysis problems, overturning conventional machine learning and statistical methods. Most often, a model trained for one market or security cannot be directly applied to another market or security due to differences inherent in the market conditions. In addition, as the market evolves over time, it is necessary to update the existing models or train new ones when new data is made available. This scenario, which is inherent in most financial forecasting applications, naturally raises the following research question: How to efficiently adapt a pre-trained model to a new set of data while retaining performance on the old data, especially when the old data is not accessible? In this paper, we propose a method to efficiently retain the knowledge available in a neural network pre-trained on a set of securities and adapt it to achieve high performance in new ones. In our method, the prior knowledge encoded in a pre-trained neural network is maintained by keeping existing connections fixed, and this knowledge is adjusted for the new securities by a set of augmented connections, which are optimized using the new data. The auxiliary connections are constrained to be of low rank. This not only allows us to rapidly optimize for the new task but also reduces the storage and run-time complexity during the deployment phase. The efficiency of our approach is empirically validated in the stock mid-price movement prediction problem using a large-scale limit order book dataset. Experimental results show that our approach enhances prediction performance as well as reduces the overall number of network parameters.}
}
@article{LIANG2023109632,
title = {Multi-view unsupervised feature selection with tensor robust principal component analysis and consensus graph learning},
journal = {Pattern Recognition},
volume = {141},
pages = {109632},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109632},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003333},
author = {Cheng Liang and Lianzhi Wang and Li Liu and Huaxiang Zhang and Fei Guo},
keywords = {Multi-view unsupervised feature selection, Low-rank tensor learning, Spectral embedding, Robust sparse regression model},
abstract = {Recently, multi-view unsupervised feature selection has attracted much attention due to its efficiency and better interpretability in processing high-dimensional multi-view datasets. Most existing methods rely on the constructed similarity matrices to obtain reliable pseudo labels to guide the feature selection. However, the considerable adverse noise in the raw data inevitably impedes the exploration of true underlying similarity structures. Besides, the inter-view correlations are often ignored during the common representation learning, which limits the effective fusion of the essential information from multiple views. To solve these issues, we design a novel robust multi-view unsupervised feature selection framework. Specifically, our method seeks a set of noise-free view-specific similarity matrices by leveraging tensor robust principal component analysis, where the high-order connections among different views are well exploited through the constructed low-rank tensor. Meanwhile, a high-quality consensus similarity matrix is adaptively learned from the view-specific representations within the same unified framework to capture the shared local structures. To enhance the discriminative ability of the feature selection matrix, we further impose a rank constraint on the consensus similarity matrix to obtain reliable pseudo cluster indicators. We present an efficient optimization algorithm ground on the alternating direction method of multipliers to solve the proposed model. Experimental results on six multi-view datasets confirm the superiority of our method.}
}
@article{BAYER2023109520,
title = {An incremental facility location clustering with a new hybrid constrained pseudometric},
journal = {Pattern Recognition},
volume = {141},
pages = {109520},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109520},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002200},
author = {Tomáš Bayer and Ivana Kolingerová and Markéta Potůčková and Miroslav Čábelka and Eva Štefanová},
keywords = {Facility location, Clusterization, Pseudometric, Detection, Simplification, Point cloud},
abstract = {The Euclidean metric, one of the classical similarity measures applied in clustering algorithms, has drawbacks when applied to spatial clustering. The resulting clusters are spherical and similarly sized, and the edges of objects are considerably smoothed. This paper proposes a novel hybrid constrained pseudometric formed by the linear combination of the Euclidean metric and a pseudometric plus penalty. The pseudometric is used in a new deterministic incremental heuristic facility location algorithm (IHFL). Our method generates larger, isotropic, and partially overlapping clusters of different sizes and spatial densities, better adapting to the surface complexity than the classical non-deterministic clustering. Cluster properties are used to derive new features for supervised/unsupervised learning. Possible applications are the classification of point clouds, their simplification, detection, filtering, and extraction of different structural patterns or sampled objects. Experiments were run on point clouds derived from laser scanning and images.}
}
@article{YANG2023109627,
title = {CPSS-FAT: A consistent positive sample selection for object detection with full adaptive threshold},
journal = {Pattern Recognition},
volume = {141},
pages = {109627},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109627},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300328X},
author = {Xiaobao Yang and Junsheng Wu and Lang He and Sugang Ma and Zhiqiang Hou and Wei Sun},
keywords = {Object detection, Label assignment, Location quality estimation, Adaptive threshold},
abstract = {Recent CNN-based methods for object detection largely focus on training the backbone of the object detector, neglecting the key part of selecting positive/negative samples. Instead, based on our analyses about the limitations and inconsistency of existing positive sample selection, we propose a novel Consistent Positive Sample Selection (CPSS) method to select the positive samples automatically, which joints Box-IoU and normalized central distance of object-anchor by mutual weighting. Meanwhile, we employ a consistent approach to location quality evaluation for suppressing the false-positive predicted box when inferencing. Furthermore, an auxiliary Full Adaptive Threshold (FAT) post-processing is also adopted according to the objects’ occlusion level to improve the recall ratio. We implement the proposed CPSS-FAT detector using MS COCO 2017 and CityScapes datasets, the comparing results indicate that our approaches are effective and robust to the different objects in an open world. Especially, we achieve 52.2% AP and 43.1% ARS, outperforming most existing detectors.}
}
@article{FENG2023109536,
title = {Community Channel-Net: Efficient channel-wise interactions via community graph topology},
journal = {Pattern Recognition},
volume = {141},
pages = {109536},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109536},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002364},
author = {Fan Feng and Qi Liu and Zhanglin Peng and Ruimao Zhang and Rosa H.M. Chan},
keywords = {Deep Neural Networks, Complex Networks, Representation Learning},
abstract = {The layer-wise structure of deep neural networks (DNNs) isolates the channel interactions in the same layer, which significantly impedes the efficient learning of DNNs. Several existing methods enable channel-wise information exchange via learning channel interdependence in a heuristic and empirical manner. Nevertheless, only informative channels are emphasized while other channels are suppressed in these approaches. This results in a low channel diversity, which impeds the generalization of DNNs. Our work aims to learn channel-wise interdependence and keep the channel diversity concurrently via designing optimal channel interaction patterns. We model the channel interaction pattern from a graph perspective, where the interactions can be regarded as information exchange on the channel graph. Based on this framework, we propose the Community Channel-Net (CC-Net), using a community-based graph topology for channel interaction. Each community contains channels with semantic commonalities, and the inter-community connections are activated among critical channels. With this structured and dynamic topology, the channels from the same community can learn channel interdependence, and those critical channels from distinct communities can gain more diverse features. CC-Net outperforms baselines on image classification tasks over various backbones with fewer computational costs.}
}
@article{LIAO2023109624,
title = {Tensor completion via convolutional sparse coding with small samples-based training},
journal = {Pattern Recognition},
volume = {141},
pages = {109624},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109624},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003254},
author = {Tianchi Liao and Zhebin Wu and Chuan Chen and Zibin Zheng and Xiongjun Zhang},
keywords = {Tensor completion, Convolutional sparse coding, High-pass filter, Inexact ADMM},
abstract = {Tensor data often suffer from missing value problems due to the complex high-dimensional structure while acquiring them. To complete the missing information, lots of Low-Rank Tensor Completion (LRTC) methods have been proposed, most of which depend on the low-rank property of tensor data. In this way, the low-rank component of the original data could be recovered roughly. However, the shortcoming is that the detailed information can not be fully restored, no matter the Sum of the Nuclear Norm (SNN) nor the Tensor Nuclear Norm (TNN) based methods. On the contrary, in the field of signal processing, Convolutional Sparse Coding (CSC) can provide a good representation of the high-frequency component of the image, which is generally associated with the detail component of the data. To this end, we propose two novel methods, LRTC-CSC-I and LRTC-CSC-II, which adopt CSC as a supplementary regularization for LRTC to capture the high-frequency components. Therefore, the LRTC-CSC methods can not only solve the missing value problem but also recover the details. Moreover, the regularizer CSC can be trained with small samples due to the sparsity characteristic. Extensive experiments show the effectiveness of LRTC-CSC methods, and quantitative evaluation indicates that the performance of our models are superior to state-of-the-art methods.}
}
@article{FOUCART2023109600,
title = {Evaluating participating methods in image analysis challenges: Lessons from MoNuSAC 2020},
journal = {Pattern Recognition},
volume = {141},
pages = {109600},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109600},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003011},
author = {Adrien Foucart and Olivier Debeir and Christine Decaestecker},
keywords = {Challenge, Competition, Digital pathology, Image analysis, Performance metrics},
abstract = {Biomedical image analysis competitions often rank the participants based on a single metric that combines assessments of different aspects of the task at hand. While this is useful for declaring a single winner for a competition, it makes it difficult to assess the strengths and weaknesses of participating algorithms. By involving multiple capabilities (detection, segmentation and classification) and releasing the prediction masks provided by several teams, the MoNuSAC 2020 challenge provides an interesting opportunity to look at what information may be lost by using entangled metrics. We analyse the challenge results based on the “Panoptic Quality” (PQ) used by the organizers, as well as on disentangled metrics that assess the detection, classification and segmentation abilities of the algorithms separately. We show that the PQ hides interesting aspects of the results, and that its sensitivity to small changes in the prediction masks makes it hard to interpret these results and to draw useful insights from them. Our results also demonstrate the necessity to have access, as much as possible, to the raw predictions provided by the participating teams so that challenge results can be more easily analysed and thus more useful to the research community.}
}
@article{FU2023109660,
title = {Multi-stage information diffusion for joint depth and surface normal estimation},
journal = {Pattern Recognition},
volume = {141},
pages = {109660},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109660},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003618},
author = {Zhiheng Fu and Siyu Hong and Mengyi Liu and Hamid Laga and Mohammed Bennamoun and Farid Boussaid and Yulan Guo},
keywords = {Depth estimation, Surface normal estimation, Multitask learning, Attention map, Multi-Stage information fusion},
abstract = {Depth and surface normal estimations are important for 3D geometric perception, which has numerous applications including autonomous vehicles and robots. In this paper, we propose a lightweight Multi-stage Information Diffusion Network (MIDNet) for the simultaneous prediction of depth and surface normals from a single RGB image. To obtain semantic and detail-preserving features, we adopt a high-resolution network as our backbone to learn multi-scale features, which are then fused into shared features for the two tasks. To mutually boost each task, a Cross-Correlation Attention Module (CCAM) is proposed to adaptively integrate information for the prediction of the two tasks in multiple stages, including feature-level information interaction and task-level information interaction. Ablation studies show that the proposed multi-stage information diffusion strategy can boost the performance gain for the two tasks at different levels. Compared to current state-of-the-art methods on the NYU Depth V2, Stanford 2D-3D-Semantic and KITTI datasets, our method achieves superior performance for both monocular depth and surface normal estimation.}
}
@article{YANG2023109564,
title = {DyGAT: Dynamic stroke classification of online handwritten documents and sketches},
journal = {Pattern Recognition},
volume = {141},
pages = {109564},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109564},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002649},
author = {Yu-Ting Yang and Yan-Ming Zhang and Xiao-Long Yun and Fei Yin and Cheng-Lin Liu},
keywords = {Stroke classification, Sketch semantic segmentation, Document layout analysis, Diagram recognition, Streaming recognition},
abstract = {Online handwriting is widely used in human-machine interface, education, office automation, and so on. Stroke classification for online handwritten documents and sketches aims to divide strokes into several semantic categories and is a necessary step for document recognition and understanding. Previous methods are essentially static in that they have to wait for the user to finish the whole sketch before making prediction. However, in practice, the more user-friendly way is to make real-time prediction as the user is writing. In this paper, we introduce Dynamic Graph ATtention network (DyGAT) to solve the dynamic stroke classification problem. The core of our method is to formalize a document/sketch into a multi-feature graph, in which nodes represent strokes, edges represent the relationships between strokes, and multiple nodes are applied to one stroke to control the information flow. The proposed method is general and is applicable to online handwritten data of many types. We conduct experiments on popular public datasets to perform sketch semantic segmentation, document layout analysis and diagram recognition, and experimental results show competitive performance. Particularly, the proposed method achieves stroke classification accuracies which are only slightly lower than those of static classification.}
}
@article{LI2023109652,
title = {Knowledge transduction for cross-domain few-shot learning},
journal = {Pattern Recognition},
volume = {141},
pages = {109652},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109652},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003539},
author = {Pengfang Li and Fang Liu and Licheng Jiao and Shuo Li and Lingling Li and Xu Liu and Xinyan Huang},
keywords = {Few-shot learning, Domain adaptation, Feature adaptation, Feature transduction, Feed-forward attention, Deep sparse representation},
abstract = {Cross-Domain Few-Shot Learning (CDFSL) aims to classify new categories from new domains with few samples. It confronts a greater domain shift than Few-Shot Learning (FSL). Based on the transfer learning framework, we propose a Knowledge Transduction method (KT) to alleviate domain shift and achieve few-shot recognition. First, a feature adaptation module based on feed-forward attention is constructed to learn domain-adapted features. The feature adaptation module weakens domain shift by transducing knowledge from an auxiliary dataset to the new dataset. Second, a feature transduction module based on deep sparse representation is developed to gather class semantics from limited support images. The feature transduction module transduces knowledge from support images to query images for few-shot recognition. In addition, a stochastic image augmentation method is proposed for FSL to train a more generalized model through consistency representation learning. Our method achieves competitive accuracy on four CDFSL datasets and four FSL datasets compared to state-of-the-art methods. The source code is available at https://github.com/XDUpfLi/KT.}
}
@article{LV2023109591,
title = {Learning cross-domain semantic-visual relationships for transductive zero-shot learning},
journal = {Pattern Recognition},
volume = {141},
pages = {109591},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109591},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002923},
author = {Fengmao Lv and Jianyang Zhang and Guowu Yang and Lei Feng and Yufeng Yu and Lixin Duan},
keywords = {Zero-shot learning, Transfer learning, Domain adaptation},
abstract = {Zero-Shot Learning (ZSL) learns models for recognizing new classes. One of the main challenges in ZSL is the domain discrepancy caused by the category inconsistency between training and testing data. Domain adaptation is the most intuitive way to address this challenge. However, existing domain adaptation techniques cannot be directly applied into ZSL due to the disjoint label space between source and target domains. This work proposes the Transferrable Semantic-Visual Relation (TSVR) approach towards transductive ZSL. TSVR redefines image recognition as predicting the similarity/dissimilarity labels for semantic-visual fusions consisting of class attributes and visual features. After the above transformation, the source and target domains can have the same label space, which hence enables to quantify domain discrepancy. For the redefined problem, the number of similar semantic-visual pairs is significantly smaller than that of dissimilar ones. To this end, we further propose to use Domain-Specific Batch Normalization to align the domain discrepancy.}
}
@article{WANG2023109586,
title = {GGD-GAN: Gradient-Guided dual-Branch adversarial networks for relic sketch generation},
journal = {Pattern Recognition},
volume = {141},
pages = {109586},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109586},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300287X},
author = {Jun Wang and Erlei Zhang and Shan Cui and Jiaxin Wang and Qunxi Zhang and Jianping Fan and Jinye Peng},
keywords = {Relics digital protection, Sketch generation, Image translation, Deep learning},
abstract = {Sketch reflects the main content and structure of the painted cultural relics, which help researcher understand the original drawing intention and painting skills. Although the existing automatic sketch extraction methods can improve efficiency, most of them are based on edge detection leading the limitations in incomplete details, serious disease noise and blurring. In this paper, a gradient guided dual-branch generative adversarial networks (GANs) is proposed for high-quality relic sketch generation. The sketch generation branch (SGB) and auxiliary gradient-image generation branch (GGB) were designed via two independent GANs for learning different and complement characteristics. We also designed a feature transmission module for transferring context features from SGB to GGB, and a fusion block to realize the gradient guidance from GGB to SGB, forcing SGB to pay more attention to shape, details, and noise suppression. Both branches not only learn different characteristics independently, but also affect and complement each other, which generates rich and clean high-quality sketches. In our experiments, we compared our method with eight state-of-the-art algorithms quantitatively and qualitatively, analysis the effects of the gradient guidance feature transfer, and the generalization of the proposed dual-branch GAN framework. Experiments show the proposed framework is promising in its ability to extract a clear, coherent, and complete sketch of painted cultural relics.}
}
@article{LIU2023109599,
title = {Local multi-scale feature aggregation network for real-time image dehazing},
journal = {Pattern Recognition},
volume = {141},
pages = {109599},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109599},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300300X},
author = {Yong Liu and Xiaorong Hou},
keywords = {Deep learning, Feature aggregation, Image restoration, Lightweight network, Multi-scale},
abstract = {Haze causes visual degradation and obscures image information, which gravely affects the reliability of computer vision tasks in real-time systems. Leveraging an enormous number of learning parameters as the restoration costs, learning-based methods have gained significant success, but they are runtime intensive or memory inefficient. In this paper, we propose a local multi-scale feature aggregation network, called LMFA-Net, which has a lightweight model structure and can be used for real-time dehazing. By learning the local mapping relationship between the clean value of a haze image at a certain point and its surrounding local region, LMFA-Net can directly restore the final haze-free image. In particular, we adopt a novel multi-scale feature extraction sub-network (M-Net) to extract features from different scales. As a lightweight network, LMFA-Net can achieve fast and efficient dehazing. Extensive experiments demonstrate that our proposed LMFA-Net surpasses previous state-of-the-art lightweight dehazing methods in both quantitatively and qualitatively.}
}
@article{KLONECKI2023109605,
title = {Cost-constrained feature selection in multilabel classification using an information-theoretic approach},
journal = {Pattern Recognition},
volume = {141},
pages = {109605},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109605},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003060},
author = {Tomasz Klonecki and Paweł Teisseyre and Jaesung Lee},
keywords = {Multilabel learning, Multilabel feature selection, Cost-constrained feature selection, Cost factor optimization, Mutual information},
abstract = {Feature selection is one of the key steps in building a predictive model in multi-label classification. However, most of the existing methods do not take into account information about the costs associated with considered features, such as the costs of performing diagnostic medical tests. We consider a problem of cost-constrained multilabel feature selection, which aims to select a feature subset relevant to multiple labels while satisfying a user-specific maximal admissible budget. This approach allows for building a model with high predictive power, for which the cost of making a prediction for a single instance does not exceed the user-specified budget. In this problem, the balance between the feature subset relevance and its cost should be considered concurrently, which is nontrivial in practice because their optimal balance is unknown. In this paper, we propose a novel criterion for cost-constrained multilabel feature selection that combines the relevance and cost of the candidate feature. The relevance measure is derived using the lower bound of the mutual information between the feature subset and label vector. Moreover, we propose an effective method for determining the cost-factor value that controls the trade-off between relevancy and cost. The experimental results on multilabel datasets with various characteristics demonstrate the superiority of the proposed method over conventional methods.}
}
@article{LIANG2023109609,
title = {L1-norm discriminant analysis via Bhattacharyya error bounds under Laplace distributions},
journal = {Pattern Recognition},
volume = {141},
pages = {109609},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109609},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003102},
author = {Zhizheng Liang and Lei Zhang},
keywords = {Laplace distributions, Bhattacharyya error bound, Discriminant criteria, Kernel functions, Data classification},
abstract = {L1-norm discriminant analysis has been proposed to enhance the robustness of classical LDA in the presence of outliers. This paper develops L1-norm discriminant analysis by exploring Bhattacharyya error bounds under Laplace distributions. Unlike some previous models, we assume that the samples of each class in the projected space follow Laplace distributions. It is the first time that the Bhattacharyya error bound is derived under Laplace distributions. It is interesting to note that this bound has a closed-form expression in a one-dimensional projected space. We relax the Bhattacharyya error bound to achieve another bound that can facilitate the design of a tractable model. Since the parameters of Laplace distributions are estimated by the maximum likelihood estimation, this yields the problem of estimating class centroids in our model. We employ a simple yet effective strategy to estimate class centroids in the original space. To address the small-sample-size problem, we transform the original model into a difference criterion by introducing additional parameters. We achieve an alternative representation of our model and design an effective algorithm to solve it. In addition, we also extend our model to its kernel version. The experiments on a series of data sets are done to demonstrate the effectiveness of our method in dealing with contaminated data.}
}
@article{SUN2023109625,
title = {Multi-view prototype-based disambiguation for partial label learning},
journal = {Pattern Recognition},
volume = {141},
pages = {109625},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109625},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003266},
author = {Shiding Sun and Xiaotong Yu and Yingjie Tian},
keywords = {Multi-view learning, Partial label learning, Weakly supervised learning},
abstract = {In this work, we study the multi-view partial label learning (MVPLL) problem, where each instance is depicted by different view features and associated with a set of candidate labels, among which a true label exists but is inaccessible in the training phase. Most existing PLL methods only consider single-view case, which learn view classifier independently and neglect the view correlations, thus can not be applied to solve MVPLL problem. Due to the non-deep framework, traditional MVPLL approach is weak in the representation ability, so its performance is still to be improved. To solve the MVPLL problem, a deep multi-view prototype-based disambiguation approach is proposed in this paper. Specifically, we innovatively employ the deep neural network for multi-view ambiguously-labeled image classification to enhance the representation ability, which makes use of the information fusion between multiple views. To improve the discriminative ability, we propose multi-view prototype-based label disambiguation algorithm. On theoretical aspect, an estimation error bound for view-risk estimator is established, which is shown to be larger than that for fuse-risk estimator. Experiments demonstrate the superiorities of our proposed method in terms of the prediction accuracy.}
}
@article{WANG2023109601,
title = {Low-rank kernel regression with preserved locality for multi-class analysis},
journal = {Pattern Recognition},
volume = {141},
pages = {109601},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109601},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003023},
author = {Yingxu Wang and Long Chen and Jin Zhou and Tianjun Li and Yufeng Yu},
keywords = {Kernel ridge regression, Low-rank learning, Locality preserving, Random feature space},
abstract = {Kernel ridge regression (KRR) is a kind of efficient supervised algorithm for multi-class analysis. However, limited by the implicit kernel space, current KRR methods have weak abilities to deal with redundant features and hidden local structures. Thus, they may get indifferent results when applied to analyze the data with complicated components. To overcome this weakness and obtain better multi-class regression performance, we propose a new method named low-rank kernel regression with preserved locality (RLRKRR). In this method, data are mapped into an explicit feature space by using the random Fourier feature technique to discover the non-linear relationship between data samples. In addition, during the training of the regression coefficient matrix, the low-rank components of this explicit feature space are simultaneously extracted for reducing the effect of the redundancy. Moreover, the graph regularization is performed on the extracted low-rank components to preserve local structures. Furthermore, the l2,p norm is imposed on the regression error term for relieving the impact of outliers. Based on these strategies, RLRKRR is capable to achieve rewarding results in complicated multi-class data analysis. In the comprehensive experiments conducted on various types of datasets, RLRKRR outperforms several state-of-the-art regression methods in terms of classification accuracy (CA).}
}
@article{CHOI2023109645,
title = {Semantic-guided de-attention with sharpened triplet marginal loss for visual place recognition},
journal = {Pattern Recognition},
volume = {141},
pages = {109645},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109645},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003461},
author = {Seung-Min Choi and Seung-Ik Lee and Jae-Yeong Lee and In So Kweon},
keywords = {Visual place recognition, Image retrieval, Triplet marginal loss, Attention, De-attention, Semantic guidance, Semantic segmentation},
abstract = {Thanks to Earth-level Street View images from Google Maps, a visual image geo-localization can estimate the coarse location of a query image with a visual place recognition process. However, this can get very challenging when non-static objects change with time, severely degrading image retrieval accuracy. We address the problem of city-scale visual place recognition in complex urban environments crowded with non-static clutters. To this end, we first analyze what clutters degrade similarity matching between the query and database images. Second, we design a self-supervised trainable de-attention module that prevents the network from focusing on non-static objects in an input image. In addition, we propose a novel triplet marginal loss called sharpened triplet marginal loss to make feature descriptors more discriminative. Lastly, due to the lack of geo-tagged public datasets with a high density of non-static objects, we propose a clutter augmentation method to evaluate our approach. The experimental results show that our model has notably improved over the existing attention methods in geo-localization tasks on the public benchmark datasets and on their augmented versions with high population and traffic. Our code is available at https://github.com/ccsmm78/deattention_with_stml_for_vpr.}
}
@article{HU2023109592,
title = {Holistic transformer: A joint neural network for trajectory prediction and decision-making of autonomous vehicles},
journal = {Pattern Recognition},
volume = {141},
pages = {109592},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109592},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002935},
author = {Hongyu Hu and Qi Wang and Zhengguang Zhang and Zhengyi Li and Zhenhai Gao},
keywords = {Autonomous vehicles, Decision-making, Holistic transformer, Multiple cues, Trajectory prediction},
abstract = {Trajectory prediction and behavioral decision-making are two important tasks for autonomous vehicles that require a good understanding of the environmental context. Notably, behavioral decisions are better made by referring to the outputs of trajectory predictions. However, most current solutions perform these tasks separately. Therefore, this paper proposes a new joint holistic transformer network that combines multiple cues to predict trajectories and make behavioral decisions simultaneously. To better explore the intrinsic relationships among cues, the network uses existing knowledge and adopts three kinds of attention mechanisms: the sparse multi-head type for reducing noise impact, feature selection sparse type for optimally using partial prior knowledge, and multi-head with sigmoid activation type for optimally using posteriori knowledge. Compared with other trajectory prediction models, the proposed model has a better comprehensive performance and good interpretability. Perceptual noise robustness experiments demonstrate that the proposed model has good noise robustness. Thus, simultaneous trajectory prediction and behavioral decision-making combining multiple cues are accomplished, which reduces computational costs and enhances semantic relationships between scenes and agents.}
}
@article{BENATO2023109649,
title = {Deep feature annotation by iterative meta-pseudo-labeling on 2D projections},
journal = {Pattern Recognition},
volume = {141},
pages = {109649},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109649},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003503},
author = {Bárbara C. Benato and Alexandru C. Telea and Alexandre X. Falcão},
keywords = {Pseudo-labeling, Deep feature annotation, Semi-supervised learning, Feature space projection, Data annotation},
abstract = {The absence of large annotated datasets to train deep neural networks (DNNs) is an issue since manual annotation is time-consuming, expensive, and error-prone. Semi-supervised learning techniques can address the problem propagating pseudo labels from supervised to unsupervised samples. However, they still require training and validation sets with many supervised samples. This work proposes a methodology, namely Deep Feature Annotation (DeepFA), that dismisses the validation set and uses very few supervised samples (e.g., 1% of the dataset). DeepFA modifies the feature spaces of a DNN along with meta-pseudo-labeling iterations in a 2D non-linear projection space using the most confidently labeled samples of an optimum-path forest semi-supervised classifier. We present a comprehensive study on DeepFA and a new variant that detects the best DNN model for generalization during the pseudo-labeling iterations. We evaluate components of DeepFA on eight datasets, finding the best DeepFA approach and showing that it outperforms self-pseudo-labeling.}
}
@article{ZHOU2023109665,
title = {Feature fusion and latent feature learning guided brain tumor segmentation and missing modality recovery network},
journal = {Pattern Recognition},
volume = {141},
pages = {109665},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109665},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323003667},
author = {Tongxue Zhou},
keywords = {Brain tumor segmentation, Multimodal feature fusion, Missing modalities, Spatial consistency, Latent feature learning},
abstract = {Accurate brain tumor segmentation is an essential step for clinical diagnosis and surgical treatment. Multimodal brain tumor segmentation strongly relies on an effective fusion method and an excellent segmentation network. However, it is common to have some missing MR modalities in clinical scenarios due to image corruption, acquisition protocol, scanner availability and scanning cost, which can heavily decrease the tumor segmentation accuracy, and also cause information loss for down-streaming disease analysis. To address this issue, I propose a novel multimodal feature fusion and latent feature learning guided deep neural network. On the one hand, the proposed network can help to segment brain tumors when one or more modalities are missing. On the other hand, it can retrieve the missing modalities to compensate for incomplete data. The proposed network consists of three key components. First, a Multimodal Feature Fusion Module (MFFM) is proposed to effectively fuse the complementary information from different modalities, consisting of a Cross-Modality Fusion Module (CMFM) and a Multi-Scale Fusion Module (MSFM). Second, a Spatial Consistency-based Latent Feature Learning Module (SC-LFLM) is presented to exploit multimodal latent correlation and extract the relevant features to benefit segmentation. Third, the Multi-Task Learning (MTL) paths are integrated to supervise the segmentation and recover the missing modalities. The proposed method is evaluated on BraTS 2018 dataset, and it can achieve superior segmentation results when one or more modalities are missing, compared with the state-of-the-art methods. Furthermore, the proposed modules can be easily adapted to other multimodal network architectures and research fields.}
}